#!/usr/bin/env python3
"""
üöÄ AI-POWERED FINANCE NER EXTRACTOR

–ó–∞–º–µ–Ω–∞ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–æ–≥–æ NER (Natasha) –Ω–∞ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω—É—é –ò–ò-—Å–∏—Å—Ç–µ–º—É –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å–æ–±—ã—Ç–∏–π.


- –ù–æ–≤–µ–π—à–∏–µ –º–æ–¥–µ–ª–∏ (GPT-5/GPT-4o-mini/Qwen3-4B)
- –ü–æ–Ω–∏–º–∞–Ω–∏–µ —Å–ª–æ–∂–Ω—ã—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –∫–æ–Ω—Ç–µ–∫—Å—Ç–æ–≤
- –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π

 
- Prompt Caching: —ç–∫–æ–Ω–æ–º–∏—è –¥–æ 90% –Ω–∞ —Ç–æ–∫–µ–Ω–∞—Ö
- Parallel Processing: –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
- Batch Processing: –º–∞—Å—Å–∏–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å—Å–∏–≤–æ–≤


- –§–∏–Ω–∞–Ω—Å–æ–≤—ã–π –≥–ª–æ—Å—Å–∞—Ä–∏–π: –∫–æ–º–ø–∞–Ω–∏–∏, –∏–Ω–¥–µ–∫—Å—ã, —Ç–µ—Ä–º–∏–Ω—ã
- CEG —Å–∏—Å—Ç–µ–º–∞: —Å–æ–±—ã—Ç–∏—è, —è–∫–æ—Ä–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è, —Ü–µ–ø–æ—á–∫–∏ –ø—Ä–∏—á–∏–Ω
- Russian Finance: –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤


- is_advertisement: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∫–ª–∞–º—ã
- content_types: —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ, –ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏–µ, —é—Ä–∏–¥–∏—á–µ—Å–∫–∏–µ, –ø—Ä–∏—Ä–æ–¥–Ω—ã–µ –±–µ–¥—Å—Ç–≤–∏—è  
- –°–æ–±—ã—Ç–∏—è: —Å–∞–Ω–∫—Ü–∏–∏, —Å—Ç–∞–≤–∫–∏ –¶–ë, —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∫–æ–º–ø–∞–Ω–∏–π
- –ë–µ–∑ —è–∫–æ—Ä–Ω–æ—Å—Ç–∏ –≤ NER - –≤—ã—á–∏—Å–ª—è–µ—Ç—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ

- –°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π —Å–∏—Å—Ç–µ–º–æ–π Natasha

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
1. –ë—ã—Å—Ç—Ä–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–µ–π
2. Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –º–∞—Å—Å–∏–≤–æ–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
3. –ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å CEG —Å–∏—Å—Ç–µ–º–æ–π –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –≤–ª–∏—è–Ω–∏—è
4. –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–∫–ª–∞–º–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
5. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ç–∏–ø–æ–≤ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
"""

import os
import json
import time
import asyncio
import logging
from typing import Dict, List, Optional, Any
from enum import Enum
from pydantic import BaseModel, Field, validator
import os
import json
from dotenv import load_dotenv
import httpx
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
load_dotenv()
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-5")
OPENAI_API_KEY = os.environ.get('API_KEY_2')

# ===== PYDANTIC –ú–û–î–ï–õ–ò =====
class Person(BaseModel):
    name: str = Field(description="–ò–º—è —á–µ–ª–æ–≤–µ–∫–∞")
    position: Optional[str] = Field(None, description="–î–æ–ª–∂–Ω–æ—Å—Ç—å")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")

class Company(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
    ticker: Optional[str] = Field(None, description="–¢–∏–∫–µ—Ä")
    sector: Optional[str] = Field(None, description="–°–µ–∫—Ç–æ—Ä")

class Market(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞")
    type: str = Field(description="–¢–∏–ø")
    value: Optional[float] = Field(None, description="–ó–Ω–∞—á–µ–Ω–∏–µ")
    change: Optional[str] = Field(None, description="–ò–∑–º–µ–Ω–µ–Ω–∏–µ")

class FinancialMetric(BaseModel):
    metric_type: str = Field(description="–¢–∏–ø –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è")
    value: str = Field(description="–ó–Ω–∞—á–µ–Ω–∏–µ")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")

class ExtractedEntities(BaseModel):
    publication_date: Optional[str] = Field(None, description="–î–∞—Ç–∞")
    people: List[Person] = Field(default_factory=list)
    companies: List[Company] = Field(default_factory=list)
    markets: List[Market] = Field(default_factory=list)
    financial_metrics: List[FinancialMetric] = Field(default_factory=list)
    
    # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è CEG —Å–∏—Å—Ç–µ–º—ã
    sector: Optional[str] = Field(None, description="–û—Ç—Ä–∞—Å–ª—å")
    country: Optional[str] = Field(None, description="–°—Ç—Ä–∞–Ω–∞") 
    event_types: List[str] = Field(default_factory=list, description="–¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π")
    event: Optional[str] = Field(None, description="–û–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è")
    
    # –§–∏–ª—å—Ç—Ä—ã –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∑–∞—Ü–∏—è
    is_advertisement: bool = Field(False, description="–†–µ–∫–ª–∞–º–∞?")
    content_types: List[str] = Field(default_factory=list, description="–¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
    
    # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ–±—Ä–∞–±–æ—Ç–∫–∏
    confidence_score: float = Field(0.8, description="–£–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å")
    language: str = Field("ru", description="–Ø–∑—ã–∫")
    requires_market_data: bool = Field(False, description="–ù—É–∂–Ω—ã —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ?")
    urgency_level: str = Field("normal", description="–°—Ä–æ—á–Ω–æ—Å—Ç—å")

class BatchExtractedEntities(BaseModel):
    """–ú–æ–¥–µ–ª—å –¥–ª—è batch-–æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    news_items: List[ExtractedEntities] = Field(default_factory=list)
    total_processed: int = 0
    batch_id: Optional[str] = None

# ===== –ì–õ–û–°–°–ê–†–ò–ô =====
class RussianFinanceGlossary:
    def __init__(self):
        self.companies = {
            "–ì–ê–ó–ü–†–û–ú": {"full_name": "–ü–ê–û –ì–∞–∑–ø—Ä–æ–º", "ticker": "GAZP", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–°–ë–ï–†–ë–ê–ù–ö": {"full_name": "–ü–ê–û –°–±–µ—Ä–±–∞–Ω–∫", "ticker": "SBER", "sector": "–§–∏–Ω–∞–Ω—Å—ã"},
            "–õ–£–ö–û–ô–õ": {"full_name": "–ü–ê–û –õ–£–ö–û–ô–õ", "ticker": "LKOH", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–ù–û–†–ù–ò–ö–ï–õ–¨": {"full_name": "–ì–ú–ö –ù–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å", "ticker": "GMKN", "sector": "–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è"},
        }
        
        self.event_types = {
            "sanctions": ["—Å–∞–Ω–∫—Ü–∏–∏", "—Å–∞–Ω–∫—Ü", "–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏", "–∑–∞–ø—Ä–µ—Ç"],
            "rate_hike": ["–∫–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞", "–ø–æ–≤—ã—Å–∏–ª —Å—Ç–∞–≤–∫—É", "—Ä–æ—Å—Ç —Å—Ç–∞–≤–∫–∏"],
            "earnings": ["–ø—Ä–∏–±—ã–ª—å", "–≤—ã—Ä—É—á–∫–∞", "–æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å", "—Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã"],
        }
        
        self.content_types = {
            "financial": ["—Ñ–∏–Ω–∞–Ω—Å—ã", "–±–∞–Ω–∫", "–∞–∫—Ü–∏–∏", "–≤–∞–ª—é—Ç", "–∫—É—Ä—Å"],
            "political": ["–ø–æ–ª–∏—Ç–∏–∫", "–ø—Ä–∞–≤–∏—Ç–µ–ª—å—Å—Ç–≤–æ", "–ø—Ä–µ–∑–∏–¥–µ–Ω—Ç", "–º–∏–Ω—Ç—Ä–∞", "–≤–ª–∞—Å—Ç—å"],
            "legal": ["–∑–∞–∫–æ–Ω", "—Å—É–¥", "–ø—Ä–∞–≤–æ", "–∏—Å–∫", "–≤–µ—Ä–¥–∏–∫—Ç"],
            "natural_disaster": ["–ø–æ–∂–∞—Ä", "–Ω–∞–≤–æ–¥–Ω–µ–Ω–∏–µ", "–∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ", "—Å—Ç–∏—Ö–∏–π–Ω–æ–µ"],
        }
        
        self.advertisement_markers = [
            "—Ä–µ–∫–ª–∞–º–∞", "–ø—Ä–æ–º–æ", "–∞–∫—Ü–∏—è", "—Å–∫–∏–¥–∫–∞", "–∫—É–ø–∏", 
            "–∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–æ–Ω–Ω", "–∫–æ–Ω—Å—É–ª—å—Ç–∞–Ω—Ç", "–±—Ä–æ–∫–µ—Ä", "–ø–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç", "@"
        ]
    
    def get_glossary_text(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π"""
        text = "# –§–ò–ù–ê–ù–°–û–í–´–ô –ì–õ–û–°–°–ê–†–ò–ô\n\n"
        
        text += "## –ö–æ–º–ø–∞–Ω–∏–∏:\n"
        for abbr, info in self.companies.items():
            text += f"- {abbr}: {info['full_name']} (–¢–∏–∫–µ—Ä: {info['ticker']})\n"
        
        text += "\n## –¢–∏–ø—ã —Å–æ–±—ã—Ç–∏–π:\n"
        for event_type, keywords in self.event_types.items():
            text += f"- {event_type}: {', '.join(keywords[:3])}\n"
        
        text += "\n## –¢–∏–ø—ã –∫–æ–Ω—Ç–µ–Ω—Ç–∞:\n"
        for content_type, keywords in self.content_types.items():
            text += f"- {content_type}: {', '.join(keywords[:3])}\n"
        
        return text

# ===== –û–°–ù–û–í–ù–û–ô –ù–ï–† =====
class CachedFinanceNERExtractor:
    """–ò–ò-–∏–∑–≤–ª–µ–∫–∞—Ç–µ–ª—å —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π prompt caching"""
    
    def __init__(self, api_key: str, model: str = "gpt-4o-mini", enable_caching: bool = True):
        self.api_key = api_key
        self.model = model
        self.enable_caching = enable_caching
        self.base_url = "https://api.openai.com/v1"
        self.glossary = RussianFinanceGlossary()
        
        # Prompt –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏
        self.base_instructions = """–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –∞–Ω–∞–ª–∏–∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π. –ò–∑–≤–ª–µ–∫–∏ JSON —Å –ø–æ–ª—è–º–∏:
          "publication_date": "–¥–∞—Ç–∞ –∏–ª–∏ null",
          "people": [{"name": "–§–ò–û", "position": "–¥–æ–ª–∂–Ω–æ—Å—Ç—å", "company": "–∫–æ–º–ø–∞–Ω–∏—è"}],
          "companies": [{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "ticker": "—Ç–∏–∫–µ—Ä", "sector": "–æ—Ç—Ä–∞—Å–ª—å"}],
          "markets": [{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "type": "—Ç–∏–ø", "value": —á–∏—Å–ª–æ, "change": "–∏–∑–º–µ–Ω–µ–Ω–∏–µ"}],
          "financial_metrics": [{"metric_type": "—Ç–∏–ø", "value": "—Å—Ç—Ä–æ–∫–∞", "company": "–∫–æ–º–ø–∞–Ω–∏—è"}],
          "sector": "–æ—Å–Ω–æ–≤–Ω–∞—è –æ—Ç—Ä–∞—Å–ª—å –∏–ª–∏ null",
          "country": "—Å—Ç—Ä–∞–Ω–∞ –∏–ª–∏ null",
          "event_types": ["—Ç–∏–ø—ã —Å–æ–±—ã—Ç–∏–π"],
          "event": "–æ–ø–∏—Å–∞–Ω–∏–µ —Å–æ–±—ã—Ç–∏—è –∏–ª–∏ null",
          "is_advertisement": "true –µ—Å–ª–∏ —Ä–µ–∫–ª–∞–º–∞",
          "content_types": ["financial", "political", "legal", "natural_disaster"], 
          "confidence_score": "—É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å 0-1",
          "language": "ru –∏–ª–∏ en",
          "requires_market_data": "true –µ—Å–ª–∏ –Ω—É–∂–Ω—ã —Ä—ã–Ω–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ",
          "urgency_level": "low/normal/high/critical"

–ü–†–ê–í–ò–õ–ê:
1. –ò—Å–ø–æ–ª—å–∑—É–π –≥–ª–æ—Å—Å–∞—Ä–∏–π
2. is_advertisement = true –µ—Å–ª–∏ —Å–æ–¥–µ—Ä–∂–∏—Ç —Ä–µ–∫–ª–∞–º—É
3. content_types: financial/–øolitical/legal/natural_disaster
4. –î–ª—è requires_market_data —Å—Ç–∞–≤—å true –µ—Å–ª–∏ –≤–ª–∏—è–µ—Ç –Ω–∞ —Ü–µ–Ω—ã
5. –Ø–∫–æ—Ä–Ω–æ—Å—Ç—å –ù–ï –æ–ø—Ä–µ–¥–µ–ª—è–π - –±—É–¥–µ—Ç –≤—ã—á–∏—Å–ª—è—Ç—å—Å—è –æ—Ç–¥–µ–ª—å–Ω–æ"""

        self.batch_instructions = """–û–±—Ä–∞–±–æ—Ç–∞–π –º–∞—Å—Å–∏–≤ –Ω–æ–≤–æ—Å—Ç–µ–π –∏ –≤–µ—Ä–Ω–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–µ:
          "news_items": [–º–∞—Å—Å–∏–≤ —Å —Ç–µ–º–∏ –∂–µ –ø–æ–ª—è–º–∏],
          "total_processed": "—á–∏—Å–ª–æ",
          "batch_id": "–∏–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∏–ª–∏ null"

–ú–∞–∫—Å–∏–º—É–º 50 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑."""

    async def extract_entities_async(self, news_text: str) -> ExtractedEntities:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–æ–µ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        messages = [
            {"role": "system", "content": f"{self.base_instructions}\n\n{self.glossary.get_glossary_text()}\n\n–¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏:"},
            {"role": "user", "content": news_text}
        ]
        
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.1,
            "max_tokens": 4000,
            "cache_override": "auto" if self.enable_caching else "disable"
        }
        
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(f"{self.base_url}/chat/completions", json=payload, headers=headers)
        
        response.raise_for_status()
        json_response = response.json()
        
        content = json_response['choices'][0]['message']['content']
        return ExtractedEntities.parse_raw(content)

    def extract_entities_json_batch(self, news_list: List[str], news_metadata: List[Dict[str, Any]] = None, verbose: bool = False) -> Dict[str, Any]:
        """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ JSON –º–∞—Å—Å–∏–≤–∞ –Ω–æ–≤–æ—Å—Ç–µ–π"""
        
        if len(news_list) > 50:
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ —á–∞—Å—Ç—è–º
            chunk_size = 25
            results = []
            for i in range(0, len(news_list), chunk_size):
                chunk = news_list[i:i + chunk_size]
                chunk_meta = news_metadata[i:i + chunk_size] if news_meta else []
                chunk_result = self._process_batch_chunk(chunk, chunk_meta)
                results.extend(chunk_result.get('news_items', []))
            
            return {
                "news_items": results,
                "total_processed": len(results),
                "batch_id": None
            }
        
        return self._process_batch_chunk(news_list, news_metadata or [])
    
    def _process_batch_chunk(self, news_list: List[str], metadata_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç chunk –Ω–æ–≤–æ—Å—Ç–µ–π"""
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º –º–∞—Å—Å–∏–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
        news_array = []
        for i, news_text in enumerate(news_list):
            news_item = {"text": news_text}
            if i < len(metadata_list):
                news_item.update(metadata_list[i])
            news_array.append(news_item)
        
        messages = [
            {"role": "system", "content": f"{self.batch_instructions}\n\n{self.glossary.get_glossary_text()}"},
            {"role": "user", "content": f"–û–±—Ä–∞–±–æ—Ç–∞–π:\n{json.dumps(news_array, ensure_ascii=False, indent=2)}"}
        ]
        
        headers = {"Authorization": f"Bearer {self.api_key}", "Content-Type": "application/json"}
        payload = {
            "model": self.model,
            "messages": messages,
            "temperature": 0.1,
            "max_tokens": 8000,
            "cache_override": "auto" if self.enable_caching else "disable"
        }
        
        response = httpx.post(f"{self.base_url}/chat/completions", json=payload, headers=headers)
        response.raise_for_status()
        
        try:
            batch_result = BatchExtractedEntities.parse_raw(response.json()['choices'][0]['message']['content'])
            return batch_result.dict()
        except Exception as e:
            return {
                "news_items": [],
                "total_processed": 0,
                "batch_id": None,
                "error": str(e)
            }

# ===== –°–û–í–ú–ï–°–¢–ò–ú–û–°–¢–¨ =====
class CompatibilityWrapper:
    """–°–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç—å —Å —Å—Ç–∞—Ä–æ–π —Å–∏—Å—Ç–µ–º–æ–π Natasha"""
    
    def __init__(self, extractor: CachedFinanceNERExtractor):
        self.extractor = extractor
        
    async def extract_entities(self, text: str, news_meta: Dict[str, Any] = None) -> Dict[str, Any]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ–≤–º–µ—Å—Ç–∏–º–æ–º —Å Natasha"""
        try:
            extracted = await self.extractor.extract_entities_async(text)
            
            result = {
                'matches': [],
                'norm': {},
                'fact': {},
                'meta': news_meta or {}
            }
            
            # –ö–æ–º–ø–∞–Ω–∏–∏ –∫–∞–∫ ORG
            for company in extracted.companies:
                result['matches'].append({
                    'fact': company.dict(),
                    'span': [0, len(company.name)],
                    'type': 'ORG'
                })
            
            # –§–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∫–∞–∫ AMOUNT
            for metric in extracted.financial_metrics:
                result['matches'].append({
                    'fact': {'type': metric.metric_type, 'value': metric.value},
                    'span': [0, len(metric.value)],
                    'type': 'AMOUNT'
                })
            
            # –ù–æ–≤—ã–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ
            if extracted.event_types:
                result['meta']['event_types'] = extracted.event_types
            if extracted.sector:
                result['meta']['sector'] = extracted.sector
            if extracted.country:
                result['meta']['country'] = extracted.country
            if extracted.content_types:
                result['meta']['content_types'] = extracted.content_types
            
            result['meta']['is_advertisement'] = extracted.is_advertisement
            result['meta']['requires_market_data'] = extracted.requires_market_data
            result['meta']['urgency_level'] = extracted.urgency_level
            
            return result
            
        except Exception as e:
            return {
                'matches': [],
                'norm': {},
                'fact': {},
                'meta': news_meta or {},
                'error': str(e)
            }

# ===== TE–°–¢ =====
async def main():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ NER"""
    
    API_KEY = os.environ.get('API_KEY_2')
    if not API_KEY:
        print("‚ùå API_KEY –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    extractor = CachedFinanceNERExtractor(api_key=API_KEY, enable_caching=True)
    
    # –¢–µ—Å—Ç 1: –û–¥–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
    news_text = """
    –¶–ë –†–§ —Å–æ—Ö—Ä–∞–Ω–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –Ω–∞ —É—Ä–æ–≤–Ω–µ 7.5%. 
    –°–±–µ—Ä–±–∞–Ω–∫ –ø–æ–∫–∞–∑–∞–ª —Ä–æ—Å—Ç –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ 25% –¥–æ 180 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π.
    """
    
    print("üöÄ –¢–µ—Å—Ç –Ω–æ–≤–æ–π NER —Å–∏—Å—Ç–µ–º—ã")
    print("="*50)
    
    extracted = await extractor.extract_entities_async(news_text)
    print(f"–ö–æ–º–ø–∞–Ω–∏–∏: {[c.name for c in extracted.companies]}")
    print(f"–°–æ–±—ã—Ç–∏—è: {extracted.event_types}")
    print(f"–§–∏–ª—å—Ç—Ä —Ä–µ–∫–ª–∞–º—ã: {'–î–ê' if extracted.is_advertisement else '–ù–ï–¢'}")
    print(f"–ö–æ–Ω—Ç–µ–Ω—Ç: {extracted.content_types}")
    print(f"–ù–ï –Ø–ö–û–†–ù–û–ï: –£–±—Ä–∞–Ω–∞ –ø—Ä–æ–≤–µ—Ä–∫–∞ —è–∫–æ—Ä–Ω–æ—Å—Ç–∏")
    
    # –¢–µ—Å—Ç 2: Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞
    batch_news = [
        "–¶–ë –†–§ –ø–æ–≤—ã—Å–∏–ª —Å—Ç–∞–≤–∫—É –¥–æ 16%",
        "–°–±–µ—Ä–±–∞–Ω–∫ –ø–æ–∫–∞–∑–∞–ª —Ä–æ—Å—Ç –ø—Ä–∏–±—ã–ª–∏ –Ω–∞ 30%", 
        "–ö—É–ø–∏—Ç–µ –∞–∫—Ü–∏–∏! –°–∫–∏–¥–∫–∞ –Ω–∞ –∫–æ–Ω—Å—É–ª—å—Ç–∞—Ü–∏–∏! –ü–æ–¥–ø–∏—Å—ã–≤–∞–π—Ç–µ—Å—å!",
        "–ó–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–µ –≤ –Ø–ø–æ–Ω–∏–∏ –Ω–∞—Ä—É—à–∏–ª–æ –ø–æ—Å—Ç–∞–≤–∫–∏"
    ]
    
    print("\n" + "="*50)
    print("üìä –ë–ê–¢–ß –û–ë–†–ê–ë–û–¢–ö–ê")
    
    batch_result = extractor.extract_entities_json_batch(batch_news, verbose=True)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {batch_result['total_processed']}/{len(batch_news)}")
    
    for i, item in enumerate(batch_result['news_items']):
        if item:
            print(f"–ù–æ–≤–æ—Å—Ç—å {i+1}: —Å–æ–±—ã—Ç–∏—è={item.get('event_types', [])}, "
                  f"—Ä–µ–∫–ª–∞–º–∞={'–î–ê' if item.get('is_advertisement', False) else '–ù–ï–¢'}, "
                  f"–∫–æ–Ω—Ç–µ–Ω—Ç={item['content_types']}")
    
    print("\n‚úÖ –¢–µ—Å—Ç –∑–∞–≤–µ—Ä—à–µ–Ω")

if __name__ == "__main__":
    asyncio.run(main())
