

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from parser.base_parser import BaseNewsParser
import requests
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime
from urllib.parse import urljoin, urlparse
import re

class ForbesParser(BaseNewsParser):
    """–ü–∞—Ä—Å–µ—Ä –¥–ª—è Forbes.ru"""
    
    def __init__(self):
        self.base_url = "https://www.forbes.ru"
        self.name = "forbes"
        super().__init__(source_name="forbes.ru", base_url=self.base_url)
        
        # –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ headers –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã Forbes
        self.session = requests.Session()
        
        # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ user-agent –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
        mobile_agents = [
            'Mozilla/5.0 (iPhone; CPU iPhone OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1',
            'Mozilla/5.0 (Android 13; Mobile; rv:109.0) Gecko/120.0 Firefox/120.0',
            'Mozilla/5.0 (iPad; CPU OS 17_0 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.0 Mobile/15E148 Safari/604.1'
        ]
        
        self.current_agent = mobile_agents[0]
        self.mobile_agents = mobile_agents
        
        self.session.headers.update({
            'User-Agent': self.current_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ru-RU,ru;q=0.9,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate, br',
            'DNT': '1',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0',
        })
        
        # –£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –¥–ª—è Forbes
        self.delay = 2.0
        
        # –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã (–ø–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º –∞–Ω–∞–ª–∏–∑–∞)
        self.sections = {
            'biznes': {
                'url': '/biznes/',
                'name': '–ë–∏–∑–Ω–µ—Å'
            },
            'investicii': {
                'url': '/investicii/', 
                'name': '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'
            }
        }
        
        print(f"üöÄ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ Forbes.ru")
        print(f"   –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–∞–∑–¥–µ–ª—ã: {list(self.sections.keys())}")
    
    def get_section_urls(self, section_name: str, max_articles: int = 50) -> list:
        """
        –ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π –∏–∑ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ä–∞–∑–¥–µ–ª–∞ Forbes
        
        Args:
            section_name: –ù–∞–∑–≤–∞–Ω–∏–µ —Ä–∞–∑–¥–µ–ª–∞ (biznes, investicii)
            max_articles: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π –¥–ª—è —Å–±–æ—Ä–∞
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π
        """
        if section_name not in self.sections:
            print(f"‚ùå –ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ä–∞–∑–¥–µ–ª: {section_name}")
            return []
        
        section_info = self.sections[section_name]
        section_url = self.base_url + section_info['url']
        
        print(f"üîç –°–±–æ—Ä URL –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ {section_info['name']}: {section_url}")
        
        try:
            # –ü–æ–ø—Ä–æ–±—É–µ–º –Ω–µ—Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ —Å —Ä–∞–∑–Ω—ã–º–∏ user-agent
            response = None
            
            for attempt in range(len(self.mobile_agents)):
                # –ú–µ–Ω—è–µ–º user-agent
                self.session.headers['User-Agent'] = self.mobile_agents[attempt]
                
                print(f"   –ü–æ–ø—ã—Ç–∫–∞ {attempt + 1} —Å agent: {self.mobile_agents[attempt][:50]}...")
                
                # –ü–∞—É–∑–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—Ä–æ—Å–æ–º
                time.sleep(2)
                
                response = self.session.get(section_url, timeout=20)
                
                print(f"   –°—Ç–∞—Ç—É—Å: {response.status_code}, —Ä–∞–∑–º–µ—Ä: {len(response.content)} –±–∞–π—Ç")
                
                if response.status_code == 200:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ –∫–∞–ø—á—É
                    content_lower = response.text.lower()
                    if ('captcha' not in content_lower and 
                        'yandex' not in content_lower and 
                        len(response.content) > 20000):
                        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω—ã–π –∑–∞–ø—Ä–æ—Å —Å –ø–æ–ø—ã—Ç–∫–∏ {attempt + 1}")
                        break
                    else:
                        print(f"   ‚ùå –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∫–∞–ø—á–∞ –∏–ª–∏ –±–ª–æ–∫–∏—Ä–æ–≤–∫–∞")
                        response = None
                else:
                    print(f"   ‚ùå –û—à–∏–±–∫–∞ —Å—Ç–∞—Ç—É—Å–∞: {response.status_code}")
                    response = None
            
            if not response or response.status_code != 200:
                print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ {section_url} –ø–æ—Å–ª–µ –≤—Å–µ—Ö –ø–æ–ø—ã—Ç–æ–∫")
                return []
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ —Å—Ç–∞—Ç—å–∏
            article_urls = set()
            
            # –ò—â–µ–º –≤—Å–µ —Å—Å—ã–ª–∫–∏
            all_links = soup.find_all('a', href=True)
            
            for link in all_links:
                href = link.get('href', '').strip()
                text = link.get_text(strip=True)
                
                # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø—É—Å—Ç—ã–µ —Å—Å—ã–ª–∫–∏ –∏ –±–µ–∑ —Ç–µ–∫—Å—Ç–∞
                if not href or not text or len(text) < 10:
                    continue
                
                # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏ –≤ –∞–±—Å–æ–ª—é—Ç–Ω—ã–µ
                if href.startswith('/'):
                    href = self.base_url + href
                elif not href.startswith('http'):
                    continue
                
                # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –ø–∞—Ç—Ç–µ—Ä–Ω–∞–º Forbes —Å—Ç–∞—Ç–µ–π
                if self._is_article_url(href):
                    article_urls.add(href)
                    
                    if len(article_urls) >= max_articles:
                        break
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–º —Å–µ–ª–µ–∫—Ç–æ—Ä–∞–º
            article_selectors = [
                'article a[href*="/"]',
                '.article-item a[href*="/"]',
                '.news-item a[href*="/"]',
                '.story-item a[href*="/"]',
                '[class*="article"] a[href*="/"]',
                '[class*="story"] a[href*="/"]'
            ]
            
            for selector in article_selectors:
                try:
                    links = soup.select(selector)
                    for link in links:
                        href = link.get('href', '').strip()
                        
                        if href.startswith('/'):
                            href = self.base_url + href
                        
                        if self._is_article_url(href):
                            article_urls.add(href)
                            
                            if len(article_urls) >= max_articles:
                                break
                                
                    if len(article_urls) >= max_articles:
                        break
                        
                except Exception as e:
                    continue
            
            urls_list = list(article_urls)[:max_articles]
            print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(urls_list)} URL —Å—Ç–∞—Ç–µ–π –≤ —Ä–∞–∑–¥–µ–ª–µ {section_info['name']}")
            
            return urls_list
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ URL –∏–∑ —Ä–∞–∑–¥–µ–ª–∞ {section_name}: {e}")
            return []
    
    def _is_article_url(self, url: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ URL —Å—Å—ã–ª–∫–æ–π –Ω–∞ —Å—Ç–∞—Ç—å—é Forbes"""
        if not url or 'forbes.ru' not in url:
            return False
            
        # –ò—Å–∫–ª—é—á–∞–µ–º —Å–ª—É–∂–µ–±–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        exclude_patterns = [
            '/forbes-heroes',
            '/sustainability/',
            '/about',
            '/contacts',
            '/advertising',
            '/subscription',
            '/rss',
            '/sitemap',
            '/privacy',
            '/terms'
        ]
        
        for pattern in exclude_patterns:
            if pattern in url.lower():
                return False
        
        # –ò—â–µ–º –ø–∞—Ç—Ç–µ—Ä–Ω—ã —Å—Ç–∞—Ç–µ–π Forbes
        article_patterns = [
            r'/[a-z-]+/\d+',  # /biznes/547083-...
            r'/article/',
            r'/news/',
            r'/story/',
            r'/post/'
        ]
        
        for pattern in article_patterns:
            if re.search(pattern, url):
                return True
        
        # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞: URL –¥–æ–ª–∂–µ–Ω —Å–æ–¥–µ—Ä–∂–∞—Ç—å —Ü–∏—Ñ—Ä—ã (ID —Å—Ç–∞—Ç—å–∏)
        if re.search(r'/\d{4,}', url):
            return True
            
        return False
    
    def parse_article(self, url: str) -> dict:
        """
        –ü–∞—Ä—Å–∏—Ç –æ—Ç–¥–µ–ª—å–Ω—É—é —Å—Ç–∞—Ç—å—é Forbes
        
        Args:
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            dict: –î–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–ª–∏ None –ø—Ä–∏ –æ—à–∏–±–∫–µ
        """
        try:
            # –ü—Ä–æ–±—É–µ–º —Ä–∞–∑–Ω—ã–µ user-agent –¥–ª—è —Å—Ç–∞—Ç—å–∏
            response = None
            
            for attempt in range(len(self.mobile_agents)):
                # –ú–µ–Ω—è–µ–º user-agent
                self.session.headers['User-Agent'] = self.mobile_agents[attempt]
                
                # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                time.sleep(self.delay)
                
                response = self.session.get(url, timeout=20)
                
                if response.status_code == 200:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –∫–∞–ø—á—É
                    content_lower = response.text.lower()
                    if ('captcha' not in content_lower and 
                        'yandex' not in content_lower and 
                        len(response.content) > 10000):
                        break
                    else:
                        response = None
                        
                time.sleep(1)
            
            if not response or response.status_code != 200:
                print(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ —Å—Ç–∞—Ç—å–µ: {url}")
                return None
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            title = self._extract_title(soup)
            content = self._extract_content(soup)
            date = self._extract_date(soup)
            
            if not title or not content:
                return None
            
            source_url = urlparse(url).netloc
            
            return {
                'title': title,
                'content': content,
                'url': url,
                'source': source_url,
                'date': date,
                'parser': self.name
            }
            
        except Exception as e:
            print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏ {url}: {e}")
            return None
    
    def _extract_title(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –∑–∞–≥–æ–ª–æ–≤–æ–∫ —Å—Ç–∞—Ç—å–∏"""
        title_selectors = [
            'h1',
            '.article-title',
            '.headline',
            '.title',
            '[class*="title"]',
            'title'  # fallback
        ]
        
        for selector in title_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    title = element.get_text(strip=True)
                    if title and len(title) > 10:
                        return title
            except:
                continue
        
        return ""
    
    def _extract_content(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–∫—Å—Ç —Å—Ç–∞—Ç—å–∏ Forbes"""
        content_selectors = [
            '.article-content p',
            '.article-body p',
            '.content p',
            '.text p',
            'article p',
            '.article p',
            '[class*="content"] p',
            '[class*="text"] p',
            '[class*="body"] p',
            '.story-content p',
            '.post-content p'
        ]
        
        for selector in content_selectors:
            try:
                paragraphs = soup.select(selector)
                if paragraphs:
                    content_parts = []
                    for p in paragraphs:
                        text = p.get_text(strip=True)
                        if text and len(text) > 20:  # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞
                            content_parts.append(text)
                    
                    if content_parts:
                        full_content = ' '.join(content_parts)
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–æ–ª—É—á–∏–ª–∏ –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
                        if len(full_content) > 100:
                            return full_content
            except:
                continue
        
        try:
            # –£–¥–∞–ª—è–µ–º –Ω–µ–Ω—É–∂–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã
            for unwanted in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                unwanted.decompose()
            
            # –ò—â–µ–º –≤—Å–µ –ø–∞—Ä–∞–≥—Ä–∞—Ñ—ã
            all_paragraphs = soup.find_all('p')
            content_parts = []
            
            for p in all_paragraphs:
                text = p.get_text(strip=True)
                if text and len(text) > 30:
                    content_parts.append(text)
            
            if content_parts:
                return ' '.join(content_parts)
                
        except:
            pass
        
        return ""
    
    def _extract_date(self, soup: BeautifulSoup) -> str:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –¥–∞—Ç—É –ø—É–±–ª–∏–∫–∞—Ü–∏–∏"""
        date_selectors = [
            'time[datetime]',
            '.date',
            '.publish-date',
            '.article-date',
            '[class*="date"]',
            '[class*="time"]'
        ]
        
        for selector in date_selectors:
            try:
                element = soup.select_one(selector)
                if element:
                    # –ü—Ä–æ–±—É–µ–º –∞—Ç—Ä–∏–±—É—Ç datetime
                    datetime_attr = element.get('datetime')
                    if datetime_attr:
                        return datetime_attr
                    
                    # –ü—Ä–æ–±—É–µ–º —Ç–µ–∫—Å—Ç —ç–ª–µ–º–µ–Ω—Ç–∞
                    date_text = element.get_text(strip=True)
                    if date_text and len(date_text) < 50:
                        return date_text
            except:
                continue
        
        # –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É –∫–∞–∫ fallback
        return datetime.now().strftime('%Y-%m-%d')
    
    def get_article_urls(self, max_articles: int = 100) -> list:
        """
        –ü–æ–ª—É—á–∞–µ—Ç URL —Å—Ç–∞—Ç–µ–π –∏–∑ –≤—Å–µ—Ö –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤
        
        Args:
            max_articles: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
            
        Returns:
            list: –°–ø–∏—Å–æ–∫ URL —Å—Ç–∞—Ç–µ–π
        """
        all_urls = []
        articles_per_section = max_articles // len(self.sections)
        
        for section_name in self.sections.keys():
            section_urls = self.get_section_urls(section_name, articles_per_section)
            all_urls.extend(section_urls)
            
            if len(all_urls) >= max_articles:
                break
        
        return all_urls[:max_articles]
    
    def extract_metadata(self, soup: BeautifulSoup, url: str) -> dict:
        """
        –ò–∑–≤–ª–µ–∫–∞–µ—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        
        Args:
            soup: BeautifulSoup –æ–±—ä–µ–∫—Ç —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            url: URL —Å—Ç–∞—Ç—å–∏
            
        Returns:
            dict: –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å—Ç–∞—Ç—å–∏
        """
        metadata = {}
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º meta —Ç–µ–≥–∏
        meta_tags = {
            'description': soup.find('meta', attrs={'name': 'description'}),
            'keywords': soup.find('meta', attrs={'name': 'keywords'}),
            'author': soup.find('meta', attrs={'name': 'author'}),
            'og:title': soup.find('meta', attrs={'property': 'og:title'}),
            'og:description': soup.find('meta', attrs={'property': 'og:description'}),
        }
        
        for key, tag in meta_tags.items():
            if tag:
                content = tag.get('content', '').strip()
                if content:
                    metadata[key] = content
        
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏—é –ø–æ URL
        if '/biznes/' in url:
            metadata['category'] = '–ë–∏–∑–Ω–µ—Å'
        elif '/investicii/' in url:
            metadata['category'] = '–ò–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏'
        else:
            metadata['category'] = '–û–±—â–µ–µ'
        
        return metadata

if __name__ == "__main__":
    # –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–µ—Ä–∞
    parser = ForbesParser()
    
    # –¢–µ—Å—Ç –ø–æ–ª—É—á–µ–Ω–∏—è URL
    urls = parser.get_section_urls('biznes', max_articles=5)
    print(f"\nüìã –ü–æ–ª—É—á–µ–Ω–æ URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {len(urls)}")
    
    # –¢–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å—Ç–∞—Ç—å–∏
    if urls:
        print(f"\nüîç –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø–µ—Ä–≤–æ–π —Å—Ç–∞—Ç—å–∏...")
        article = parser.parse_article(urls[0])
        
        if article:
            print(f"‚úÖ –°—Ç–∞—Ç—å—è —É—Å–ø–µ—à–Ω–æ —Å–ø–∞—Ä—à–µ–Ω–∞:")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {article['title'][:60]}...")
            print(f"   –ö–æ–Ω—Ç–µ–Ω—Ç: {len(article['content'])} —Å–∏–º–≤–æ–ª–æ–≤")
            print(f"   URL: {article['url']}")
        else:
            print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å —Å–ø–∞—Ä—Å–∏—Ç—å —Å—Ç–∞—Ç—å—é")
    else:
        print(f"‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å URL –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è")