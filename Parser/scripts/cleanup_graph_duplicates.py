#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö
–£–¥–∞–ª—è–µ—Ç –¥—É–±–ª–∏—Ä—É—é—â–∏–µ—Å—è —É–∑–ª—ã –∏ –æ–±—ä–µ–¥–∏–Ω—è–µ—Ç —Å–≤—è–∑–∏
"""

import asyncio
import logging
import sys
from typing import Dict, List, Any
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫Parser.src –≤ PYTHONPATH
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from Parser.src.core.config import settings
from Parser.src.graph_models import GraphService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class GraphDuplicateCleaner:
    """–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –±–∞–∑–µ –¥–∞–Ω–Ω—ã—Ö"""
    
    def __init__(self):
        self.graph = None
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–¥–∫–ª—é—á–µ–Ω–∏—è –∫ –≥—Ä–∞—Ñ—É"""
        self.graph = GraphService(
            uri=settings.NEO4J_URI,
            user=settings.NEO4J_USER,
            password=settings.NEO4J_PASSWORD
        )
        logger.info("Connected to Neo4j")
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.graph:
            await self.graph.close()
    
    async def cleanup_duplicate_companies(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π"""
        
        async with self.graph.driver.session() as session:
            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã –∫–æ–º–ø–∞–Ω–∏–π –ø–æ —Ç–∏–∫–µ—Ä—É
            query = """
            MATCH (c:Company)
            WHERE c.ticker IS NOT NULL AND c.ticker <> ""
            WITH c.ticker as ticker, collect(c) as companies
            WHERE size(companies) > 1
            RETURN ticker, companies
            ORDER BY size(companies) DESC
            """
            
            result = await session.run(query)
            duplicates = [record async for record in result]
            
            cleaned_count = 0
            
            for record in duplicates:
                ticker = record["ticker"]
                companies = record["companies"]
                
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∫–æ–º–ø–∞–Ω–∏—é –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
                main_company = companies[0]
                duplicate_companies = companies[1:]
                
                logger.info(f"Cleaning up {len(duplicate_companies)} duplicates for ticker {ticker}")
                
                for dup_company in duplicate_companies:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–∞ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
                    await self._merge_company_relationships(session, dup_company, main_company)
                    
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                    delete_query = """
                    MATCH (c:Company {id: $company_id})
                    DETACH DELETE c
                    """
                    await session.run(delete_query, company_id=dup_company["id"])
                    
                    cleaned_count += 1
            
            return cleaned_count
    
    async def cleanup_duplicate_sectors(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–µ–∫—Ç–æ—Ä–æ–≤"""
        
        async with self.graph.driver.session() as session:
            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Å–µ–∫—Ç–æ—Ä–æ–≤ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            query = """
            MATCH (s:Sector)
            WITH s.name as name, collect(s) as sectors
            WHERE size(sectors) > 1
            RETURN name, sectors
            ORDER BY size(sectors) DESC
            """
            
            result = await session.run(query)
            duplicates = [record async for record in result]
            
            cleaned_count = 0
            
            for record in duplicates:
                name = record["name"]
                sectors = record["sectors"]
                
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π —Å–µ–∫—Ç–æ—Ä –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω–æ–π
                main_sector = sectors[0]
                duplicate_sectors = sectors[1:]
                
                logger.info(f"Cleaning up {len(duplicate_sectors)} duplicate sectors for '{name}'")
                
                for dup_sector in duplicate_sectors:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–∞ –∫ –æ—Å–Ω–æ–≤–Ω–æ–º—É —Å–µ–∫—Ç–æ—Ä—É
                    await self._merge_sector_relationships(session, dup_sector, main_sector)
                    
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                    delete_query = """
                    MATCH (s:Sector {id: $sector_id})
                    DETACH DELETE s
                    """
                    await session.run(delete_query, sector_id=dup_sector["id"])
                    
                    cleaned_count += 1
            
            return cleaned_count
    
    async def cleanup_duplicate_topics(self) -> int:
        """–û—á–∏—Å—Ç–∫–∞ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–º"""
        
        async with self.graph.driver.session() as session:
            # –ù–∞—Ö–æ–¥–∏–º –¥—É–±–ª–∏–∫–∞—Ç—ã —Ç–µ–º –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            query = """
            MATCH (t:Topic)
            WITH t.name as name, collect(t) as topics
            WHERE size(topics) > 1
            RETURN name, topics
            ORDER BY size(topics) DESC
            """
            
            result = await session.run(query)
            duplicates = [record async for record in result]
            
            cleaned_count = 0
            
            for record in duplicates:
                name = record["name"]
                topics = record["topics"]
                
                # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é —Ç–µ–º—É –∫–∞–∫ –æ—Å–Ω–æ–≤–Ω—É—é
                main_topic = topics[0]
                duplicate_topics = topics[1:]
                
                logger.info(f"Cleaning up {len(duplicate_topics)} duplicate topics for '{name}'")
                
                for dup_topic in duplicate_topics:
                    # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –≤—Å–µ —Å–≤—è–∑–∏ –æ—Ç –¥—É–±–ª–∏–∫–∞—Ç–∞ –∫ –æ—Å–Ω–æ–≤–Ω–æ–π —Ç–µ–º–µ
                    await self._merge_topic_relationships(session, dup_topic, main_topic)
                    
                    # –£–¥–∞–ª—è–µ–º –¥—É–±–ª–∏–∫–∞—Ç
                    delete_query = """
                    MATCH (t:Topic {id: $topic_id})
                    DETACH DELETE t
                    """
                    await session.run(delete_query, topic_id=dup_topic["id"])
                    
                    cleaned_count += 1
            
            return cleaned_count
    
    async def _merge_company_relationships(self, session, from_company: Dict, to_company: Dict):
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –æ—Ç –æ–¥–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏ –∫ –¥—Ä—É–≥–æ–π"""
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ ABOUT
        query = """
        MATCH (n:News)-[r:ABOUT]->(c:Company {id: $from_id})
        MERGE (n)-[:ABOUT]->(target:Company {id: $to_id})
        DELETE r
        """
        await session.run(query, from_id=from_company["id"], to_id=to_company["id"])
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ AFFECTS
        query = """
        MATCH (n:News)-[r:AFFECTS]->(c:Company {id: $from_id})
        MERGE (n)-[:AFFECTS {weight: r.weight, window: r.window, dt: r.dt, method: r.method}]->(target:Company {id: $to_id})
        DELETE r
        """
        await session.run(query, from_id=from_company["id"], to_id=to_company["id"])
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ IN_SECTOR
        query = """
        MATCH (c:Company {id: $from_id})-[r:IN_SECTOR]->(s:Sector)
        MERGE (target:Company {id: $to_id})-[:IN_SECTOR]->(s)
        DELETE r
        """
        await session.run(query, from_id=from_company["id"], to_id=to_company["id"])
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ HAS_COMPANY
        query = """
        MATCH (m:Market)-[r:HAS_COMPANY]->(c:Company {id: $from_id})
        MERGE (m)-[:HAS_COMPANY]->(target:Company {id: $to_id})
        DELETE r
        """
        await session.run(query, from_id=from_company["id"], to_id=to_company["id"])
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ COVARIATES_WITH
        query = """
        MATCH (c1:Company {id: $from_id})-[r:COVARIATES_WITH]-(c2:Company)
        MERGE (target:Company {id: $to_id})-[:COVARIATES_WITH {rho: r.rho, window: r.window, updated_at: r.updated_at}]-(c2)
        DELETE r
        """
        await session.run(query, from_id=from_company["id"], to_id=to_company["id"])
    
    async def _merge_sector_relationships(self, session, from_sector: Dict, to_sector: Dict):
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –æ—Ç –æ–¥–Ω–æ–≥–æ —Å–µ–∫—Ç–æ—Ä–∞ –∫ –¥—Ä—É–≥–æ–º—É"""
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ IN_SECTOR
        query = """
        MATCH (c:Company)-[r:IN_SECTOR]->(s:Sector {id: $from_id})
        MERGE (c)-[:IN_SECTOR]->(target:Sector {id: $to_id})
        DELETE r
        """
        await session.run(query, from_id=from_sector["id"], to_id=to_sector["id"])
    
    async def _merge_topic_relationships(self, session, from_topic: Dict, to_topic: Dict):
        """–ü–µ—Ä–µ–º–µ—â–µ–Ω–∏–µ —Å–≤—è–∑–µ–π –æ—Ç –æ–¥–Ω–æ–π —Ç–µ–º—ã –∫ –¥—Ä—É–≥–æ–π"""
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º —Å–≤—è–∑–∏ HAS_TOPIC
        query = """
        MATCH (n:News)-[r:HAS_TOPIC]->(t:Topic {id: $from_id})
        MERGE (n)-[:HAS_TOPIC]->(target:Topic {id: $to_id})
        DELETE r
        """
        await session.run(query, from_id=from_topic["id"], to_id=to_topic["id"])
    
    async def run_cleanup(self, dry_run: bool = False) -> Dict[str, int]:
        """–ó–∞–ø—É—Å–∫ –æ—á–∏—Å—Ç–∫–∏ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤"""
        
        logger.info(f"Starting duplicate cleanup (dry_run={dry_run})...")
        
        results = {
            "companies_cleaned": 0,
            "sectors_cleaned": 0,
            "topics_cleaned": 0
        }
        
        if not dry_run:
            results["companies_cleaned"] = await self.cleanup_duplicate_companies()
            results["sectors_cleaned"] = await self.cleanup_duplicate_sectors()
            results["topics_cleaned"] = await self.cleanup_duplicate_topics()
        else:
            logger.info("Dry run mode - no changes will be made")
            # –í —Ä–µ–∂–∏–º–µ dry run –ø—Ä–æ—Å—Ç–æ —Å—á–∏—Ç–∞–µ–º –¥—É–±–ª–∏–∫–∞—Ç—ã
            results["companies_cleaned"] = await self._count_duplicate_companies()
            results["sectors_cleaned"] = await self._count_duplicate_sectors()
            results["topics_cleaned"] = await self._count_duplicate_topics()
        
        return results
    
    async def _count_duplicate_companies(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ –∫–æ–º–ø–∞–Ω–∏–π"""
        async with self.graph.driver.session() as session:
            query = """
            MATCH (c:Company)
            WHERE c.ticker IS NOT NULL AND c.ticker <> ""
            WITH c.ticker as ticker, collect(c) as companies
            WHERE size(companies) > 1
            RETURN sum(size(companies) - 1) as total_duplicates
            """
            result = await session.run(query)
            record = await result.single()
            return record["total_duplicates"] if record else 0
    
    async def _count_duplicate_sectors(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Å–µ–∫—Ç–æ—Ä–æ–≤"""
        async with self.graph.driver.session() as session:
            query = """
            MATCH (s:Sector)
            WITH s.name as name, collect(s) as sectors
            WHERE size(sectors) > 1
            RETURN sum(size(sectors) - 1) as total_duplicates
            """
            result = await session.run(query)
            record = await result.single()
            return record["total_duplicates"] if record else 0
    
    async def _count_duplicate_topics(self) -> int:
        """–ü–æ–¥—Å—á–µ—Ç –¥—É–±–ª–∏–∫–∞—Ç–æ–≤ —Ç–µ–º"""
        async with self.graph.driver.session() as session:
            query = """
            MATCH (t:Topic)
            WITH t.name as name, collect(t) as topics
            WHERE size(topics) > 1
            RETURN sum(size(topics) - 1) as total_duplicates
            """
            result = await session.run(query)
            record = await result.single()
            return record["total_duplicates"] if record else 0
    
    def print_report(self, results: Dict[str, int], dry_run: bool = False):
        """–í—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –æ –æ—á–∏—Å—Ç–∫–µ"""
        
        print("\n" + "="*60)
        print("GRAPH DUPLICATE CLEANUP REPORT")
        print("="*60)
        
        mode = "DRY RUN" if dry_run else "ACTUAL CLEANUP"
        print(f"\nMode: {mode}")
        
        total_cleaned = sum(results.values())
        
        print(f"\nüìä CLEANUP RESULTS:")
        print(f"  Companies cleaned: {results['companies_cleaned']}")
        print(f"  Sectors cleaned: {results['sectors_cleaned']}")
        print(f"  Topics cleaned: {results['topics_cleaned']}")
        print(f"  Total cleaned: {total_cleaned}")
        
        if dry_run:
            print(f"\nüí° This was a dry run. Use --execute to perform actual cleanup.")
        else:
            print(f"\n‚úÖ Cleanup completed successfully!")
        
        print("\n" + "="*60)


async def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    
    import argparse
    
    parser = argparse.ArgumentParser(description="Clean up duplicates in Neo4j graph")
    parser.add_argument("--execute", action="store_true", 
                       help="Execute actual cleanup (default is dry run)")
    parser.add_argument("--companies", action="store_true",
                       help="Clean up only companies")
    parser.add_argument("--sectors", action="store_true",
                       help="Clean up only sectors")
    parser.add_argument("--topics", action="store_true",
                       help="Clean up only topics")
    
    args = parser.parse_args()
    
    cleaner = GraphDuplicateCleaner()
    
    try:
        await cleaner.initialize()
        
        dry_run = not args.execute
        
        if args.companies:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–æ–ª—å–∫–æ –∫–æ–º–ø–∞–Ω–∏–π
            results = {"companies_cleaned": 0, "sectors_cleaned": 0, "topics_cleaned": 0}
            if dry_run:
                results["companies_cleaned"] = await cleaner._count_duplicate_companies()
            else:
                results["companies_cleaned"] = await cleaner.cleanup_duplicate_companies()
        elif args.sectors:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–æ–ª—å–∫–æ —Å–µ–∫—Ç–æ—Ä–æ–≤
            results = {"companies_cleaned": 0, "sectors_cleaned": 0, "topics_cleaned": 0}
            if dry_run:
                results["sectors_cleaned"] = await cleaner._count_duplicate_sectors()
            else:
                results["sectors_cleaned"] = await cleaner.cleanup_duplicate_sectors()
        elif args.topics:
            # –û—á–∏—Å—Ç–∫–∞ —Ç–æ–ª—å–∫–æ —Ç–µ–º
            results = {"companies_cleaned": 0, "sectors_cleaned": 0, "topics_cleaned": 0}
            if dry_run:
                results["topics_cleaned"] = await cleaner._count_duplicate_topics()
            else:
                results["topics_cleaned"] = await cleaner.cleanup_duplicate_topics()
        else:
            # –ü–æ–ª–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
            results = await cleaner.run_cleanup(dry_run=dry_run)
        
        cleaner.print_report(results, dry_run=dry_run)
        
        return 0
        
    except Exception as e:
        logger.error(f"Error during cleanup: {e}")
        return 1
        
    finally:
        await cleaner.close()


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
