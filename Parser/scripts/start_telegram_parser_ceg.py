# scripts/start_telegram_parser_ceg.py
"""
üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô TELEGRAM PARSER —Å BATCH –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –ø–æ–ª–Ω—ã–º CEG –∞–Ω–∞–ª–∏–∑–æ–º

–ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
- Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
- NER entity recognition –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π
- –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î Neo4j
- CEG —Ä–∞—Å—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
- –î–µ—Ç–∞–ª—å–Ω—ã–π JSON –æ—Ç–≤–µ—Ç —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
- –°–≤—è–∑–∏ –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ —Ç–∏–∫–µ—Ä–∞–º–∏
- –†–∞—Å—á–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ CEG —Å–∏—Å—Ç–µ–º—ã
- –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π

–ê–†–•–ò–¢–ï–ö–¢–£–†–ê:
1. –ß–∏—Ç–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Telegram –≤ batch —Ä–µ–∂–∏–º–µ
2. –§–æ—Ä–º–∏—Ä—É–µ—Ç JSON —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π
3. –ü–µ—Ä–µ–¥–∞–µ—Ç –≤ NER entity_recognition –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
4. –°–æ–∑–¥–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î Neo4j
5. –í—ã–ø–æ–ª–Ω—è–µ—Ç CEG —Ä–∞—Å—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏–∑ —Å–≤—è–∑–µ–π
6. –§–æ—Ä–º–∏—Ä—É–µ—Ç –ø–æ–¥—Ä–æ–±–Ω—ã–π JSON –æ—Ç–≤–µ—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
"""

import asyncio
import logging
import signal
import sys
import os
import json
from pathlib import Path
from typing import Optional, List, Dict, Any
from datetime import datetime, timedelta
from uuid import uuid4

sys.path.insert(0, str(Path(__file__).parent.parent))

from Parser.src.core.config import settings
from Parser.src.core.database import init_db, close_db, get_db_session
from Parser.src.services.telegram_parser.client import TelegramClientManager
from Parser.src.services.telegram_parser.parser import Telegram_Parser
from Parser.src.services.telegram_parser.antispam import AntiSpamFilter
from Parser.src.services.enricher.enrichment_service import EnrichmentService
from Parser.src.services.enricher.moex_linker import MOEXLinker
from Parser.src.utils.logging import setup_logging
from Parser.src.core.models import Source, News, SourceKind
from Parser.src.graph_models import GraphService
from Parser.src.services.html_parser.html_parser_service import HTMLParserService
from Parser.entity_recognition import CachedFinanceNERExtractor
try:
    from Parser.entity_recognition_local import LocalFinanceNERExtractor
except ImportError:
    try:
        from entity_recognition_local import LocalFinanceNERExtractor
    except ImportError:
        LocalFinanceNERExtractor = None
from sqlalchemy import select

logger = logging.getLogger(__name__)


class Telegram_ParserServiceCEG:
    """
    üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô Telegram_Parser —Å BATCH –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ –ø–æ–ª–Ω—ã–º CEG –∞–Ω–∞–ª–∏–∑–æ–º
    üåê + –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø HTML –ü–ê–†–°–ï–†–û–í (Forbes, Interfax)

    –ù–û–í–´–ï –í–û–ó–ú–û–ñ–ù–û–°–¢–ò:
    - Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
    - NER entity recognition –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π  
    - –°–æ–∑–¥–∞–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î Neo4j
    - CEG —Ä–∞—Å—á–µ—Ç—ã –∏ –∞–Ω–∞–ª–∏–∑ –ø—Ä–∏—á–∏–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
    - –î–µ—Ç–∞–ª—å–Ω—ã–π JSON –æ—Ç–≤–µ—Ç —Å –ø–æ–ª–Ω–æ–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–µ–π –æ –Ω–æ–≤–æ—Å—Ç—è—Ö
    - –°–≤—è–∑–∏ –º–µ–∂–¥—É –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ —Ç–∏–∫–µ—Ä–∞–º–∏
    - –†–∞—Å—á–µ—Ç–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏–∑ CEG —Å–∏—Å—Ç–µ–º—ã
    - –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –±—É–¥—É—â–∏—Ö —Å–æ–±—ã—Ç–∏–π
    - üåê HTML –ø–∞—Ä—Å–∏–Ω–≥ Forbes, Interfax, E-disclosure, MOEX –≤ —Ä–∞–º–∫–∞—Ö CEG
    - üîÑ –ï–¥–∏–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ Telegram –∏ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
    """

    def __init__(
        self,
        days: int = 7,
        realtime_mode: bool = False,
        use_local_ai: bool = False,
        batch_size: int = 20,
        max_batches: int = 10,
        batch_delay: int = 5,
        retry_attempts: int = 3,
        network_timeout: int = 30
    ):
        self.client_manager = TelegramClientManager()
        self.client = None
        self.enricher = None
        self.graph_service = None
        self.moex_linker = None
        self.html_parser_service = None
        self.running = False
        self.tasks = []
        self.days = days
        self.realtime_mode = realtime_mode
        self.use_local_ai = use_local_ai
        
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
        self.batch_size = batch_size
        self.max_batches = max_batches
        self.batch_delay = batch_delay
        self.retry_attempts = retry_attempts
        self.network_timeout = network_timeout

        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER —Å–∏—Å—Ç–µ–º—ã
        self._init_ner_system()

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.total_stats = {
            "total_messages": 0,
            "saved_news": 0,
            "ads_filtered": 0,
            "duplicates": 0,
            "errors": 0,
            "batch_processed": 0,
            "entities_extracted": 0,
            "graph_nodes_created": 0,
            "ceg_links_created": 0,
            "predictions_generated": 0,
            "html_sources_processed": 0,
            "html_articles_processed": 0,
            "telegram_sources_processed": 0
        }

    def _init_ner_system(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è NER —Å–∏—Å—Ç–µ–º—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        if self.use_local_ai and LocalFinanceNERExtractor:
            logger.info("üß† Initializing LOCAL AI NER (Qwen3-4B)")
            try:
                self.ner_extractor = LocalFinanceNERExtractor(
                    model_name="unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit",
                    device="cuda",
                    batch_size=self.batch_size
                )
                logger.info("‚úÖ Local AI NER initialized successfully")
            except Exception as e:
                logger.warning(f"‚ùå Failed to load local AI: {e}")
                logger.info("üîÑ Falling back to API mode...")
                self.use_local_ai = False
        
        if not self.use_local_ai:
            logger.info("üß† Initializing API AI NER (OpenAI GPT)")
            api_key = os.getenv("API_KEY_2") or os.getenv("OPENAI_API_KEY")
            if not api_key:
                raise ValueError("‚ùå No API key found for OpenAI GPT. Set API_KEY_2 or OPENAI_API_KEY environment variable")
            
            try:
                self.ner_extractor = CachedFinanceNERExtractor(
                    api_key=api_key,
                    model="gpt-4o-mini",
                    enable_caching=True
                )
                logger.info("‚úÖ API AI NER initialized successfully")
            except Exception as e:
                raise ValueError(f"‚ùå Failed to initialize API NER: {e}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ NER —Å–∏—Å—Ç–µ–º–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞
        if not hasattr(self, 'ner_extractor') or not self.ner_extractor:
            raise RuntimeError("‚ùå Failed to initialize any NER system")

    async def _init_graph_service_with_retry(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Graph Service —Å retry –ª–æ–≥–∏–∫–æ–π –∏ –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Å–µ—Ç–µ–≤—ã—Ö –æ—à–∏–±–æ–∫"""
        for attempt in range(self.retry_attempts):
            try:
                logger.info(f"üîÑ Attempt {attempt + 1}/{self.retry_attempts} to connect to Neo4j...")
                
                graph_service = GraphService(
                    uri=settings.NEO4J_URI,
                    user=settings.NEO4J_USER,
                    password=settings.NEO4J_PASSWORD
                )
                
                # –¢–µ—Å—Ç–∏—Ä—É–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
                await graph_service.create_constraints()
                logger.info("‚úÖ Neo4j Graph Service connected successfully")
                return graph_service
                
            except Exception as e:
                logger.warning(f"‚ùå Neo4j connection attempt {attempt + 1} failed: {e}")
                if attempt < self.retry_attempts - 1:
                    wait_time = (attempt + 1) * 2  # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞
                    logger.info(f"‚è≥ Waiting {wait_time} seconds before retry...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error("‚ùå Failed to connect to Neo4j after all attempts")
                    logger.info("‚ö†Ô∏è Continuing without graph functionality...")
                    return None

    async def _retry_with_backoff(self, func, *args, **kwargs):
        """–í—ã–ø–æ–ª–Ω—è–µ—Ç —Ñ—É–Ω–∫—Ü–∏—é —Å retry –ª–æ–≥–∏–∫–æ–π –∏ —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–π –∑–∞–¥–µ—Ä–∂–∫–æ–π"""
        for attempt in range(self.retry_attempts):
            try:
                return await func(*args, **kwargs)
            except Exception as e:
                if attempt < self.retry_attempts - 1:
                    wait_time = (attempt + 1) * 2
                    logger.warning(f"‚ö†Ô∏è Attempt {attempt + 1} failed: {e}. Retrying in {wait_time}s...")
                    await asyncio.sleep(wait_time)
                else:
                    logger.error(f"‚ùå All {self.retry_attempts} attempts failed for {func.__name__}")
                    raise

    async def start(self):
        """üöÄ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞ —Å batch –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –∏ CEG"""
        try:
            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ë–î
            await init_db()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Telegram –∫–ª–∏–µ–Ω—Ç–∞
            logger.info("üöÄ Initializing Telegram client...")
            self.client = await self.client_manager.initialize()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Graph Service –¥–ª—è Neo4j —Å retry –ª–æ–≥–∏–∫–æ–π
            logger.info("üï∏Ô∏è Initializing Neo4j Graph Service...")
            self.graph_service = await self._init_graph_service_with_retry()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è MOEX Linker –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ç–∏–∫–µ—Ä–æ–≤
            logger.info("üè¢ Initializing MOEX Linker for ticker search...")
            self.moex_linker = MOEXLinker(enable_auto_learning=True)
            await self.moex_linker.initialize()

            # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è EnrichmentService —Å CEG
            async with get_db_session() as session:
                logger.info("üß† Initializing AI Enrichment Service...")
                self.enricher = EnrichmentService(
                    session=session,
                    use_local_ai=self.use_local_ai
                )
                await self.enricher.initialize()

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è CEG service —Å –Ω–∞—à–∏–º graph_service
                if self.graph_service:
                    from Parser.src.services.events.ceg_realtime_service import CEGRealtimeService
                    self.enricher.ceg_service = CEGRealtimeService(
                        session=session,
                        graph_service=self.graph_service,
                        lookback_window=30  # 30 –¥–Ω–µ–π –¥–ª—è —Ä–µ—Ç—Ä–æ—Å–ø–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
                    )
                    logger.info("   ‚úì CEG Real-time Service initialized with graph_service")

                # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è HTML Parser Service
                logger.info("üåê Initializing HTML Parser Service...")
                self.html_parser_service = HTMLParserService(
                    db_session=session,
                    use_local_ai=self.use_local_ai
                )

                logger.info(f"   ‚úì AI Model: {'Local Qwen3-4B' if self.use_local_ai else 'OpenAI GPT'}")
                logger.info(f"   ‚úì Batch size: {self.batch_size}")
                logger.info(f"   ‚úì NER extractor: {type(self.ner_extractor).__name__}")

                if self.enricher.ceg_service:
                    logger.info(f"   ‚úì CEG lookback window: {self.enricher.ceg_service.lookback_window} days")
                else:
                    logger.warning("   ‚ö† CEG Service not available (Neo4j not connected?)")

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ Telegram –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                telegram_result = await session.execute(
                    select(Source).where(
                        Source.kind == SourceKind.TELEGRAM,
                        Source.enabled == True
                    )
                )
                telegram_sources = telegram_result.scalars().all()

                # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –∞–∫—Ç–∏–≤–Ω—ã–µ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∏
                html_result = await session.execute(
                    select(Source).where(
                        Source.kind == SourceKind.HTML,
                        Source.enabled == True
                    )
                )
                html_sources = html_result.scalars().all()

                total_sources = len(telegram_sources) + len(html_sources)
                
                if total_sources == 0:
                    logger.warning("No active sources found")
                    return

                logger.info(f"üì° Found {len(telegram_sources)} active Telegram sources")
                logger.info(f"üåê Found {len(html_sources)} active HTML sources")
                logger.info(f"üìä Total sources: {total_sources}")

                # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Ä–µ–∂–∏–º–µ —Ä–∞–±–æ—Ç—ã
                if self.realtime_mode:
                    logger.info("üîÑ Running in REAL-TIME MONITORING mode")
                    logger.info("   - Batch processing of new messages (Telegram + HTML)")
                    logger.info("   - NER entity extraction")
                    logger.info("   - Graph creation and CEG analysis")
                    logger.info("   - Detailed JSON responses")
                else:
                    logger.info(f"üìö Running in HISTORICAL LOADING mode (last {self.days} days)")
                    logger.info("   - Batch loading historical messages (Telegram + HTML)")
                    logger.info("   - Full NER analysis and graph building")
                    logger.info("   - CEG calculations and predictions")

                # –ó–∞–ø—É—Å–∫–∞–µ–º batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è Telegram –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                self.running = True
                for source in telegram_sources:
                    task = asyncio.create_task(
                        self._monitor_telegram_source_batch(source)
                    )
                    self.tasks.append(task)
                
                # –ó–∞–ø—É—Å–∫–∞–µ–º batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –¥–ª—è HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤
                for source in html_sources:
                    task = asyncio.create_task(
                        self._monitor_html_source_batch(source)
                    )
                    self.tasks.append(task)

                # –ñ–¥–µ–º –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –≤—Å–µ—Ö –∑–∞–¥–∞—á
                await asyncio.gather(*self.tasks)

                # –í—ã–≤–æ–¥–∏–º —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                self._print_final_stats()

        except Exception as e:
            logger.error(f"Failed to start Telegram parser: {e}", exc_info=True)
            raise
        finally:
            await self.stop()

    async def _monitor_telegram_source_batch(self, source: Source):
        """üîÑ Batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –ø–æ–ª–Ω—ã–º –∞–Ω–∞–ª–∏–∑–æ–º"""
        logger.info(f"üìª Starting BATCH monitor for {source.name} ({source.code})")

        # –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º batch backfill –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        async with get_db_session() as session:
            parser = Telegram_Parser(
                client=self.client,
                db_session=session,
                anti_spam=AntiSpamFilter(Path("config/ad_rules.yml")),
                enricher=None  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—Ç–∞—Ä—ã–π enricher
            )

            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –Ω—É–∂–µ–Ω –ª–∏ backfill
            config = source.config or {}
            if not self.realtime_mode and config.get("backfill_enabled", True):
                logger.info(f"üì• Starting BATCH backfill for {source.code} (last {self.days} days)")
                
                try:
                    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Telegram
                    news_batch = await self._collect_news_batch(parser, source, self.days)
                    
                    if news_batch:
                        logger.info(f"üìä Collected {len(news_batch)} news items for batch processing")
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch —á–µ—Ä–µ–∑ NER –∏ CEG
                        detailed_results = await self._process_news_batch(news_batch, source)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        await self._save_batch_results(detailed_results, session)
                        
                        logger.info(f"‚úÖ Batch processing completed for {source.code}")
                        logger.info(f"   - News processed: {len(news_batch)}")
                        logger.info(f"   - Entities extracted: {detailed_results.get('total_entities', 0)}")
                        logger.info(f"   - Graph nodes created: {detailed_results.get('total_nodes', 0)}")
                        logger.info(f"   - CEG links created: {detailed_results.get('total_links', 0)}")

                except Exception as e:
                    logger.error(f"‚ùå Batch backfill failed for {source.code}: {e}", exc_info=True)

            # –í —Ä–µ–∂–∏–º–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤—ã—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ backfill
            if not self.realtime_mode:
                logger.info(f"‚úÖ Historical batch loading completed for {source.code}")
                return

        # –†–µ–∂–∏–º real-time batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
        logger.info(f"üîÑ Starting real-time BATCH monitoring for {source.code}")
        while self.running:
            try:
                async with get_db_session() as session:
                    parser = Telegram_Parser(
                        client=self.client,
                        db_session=session,
                        anti_spam=AntiSpamFilter(Path("config/ad_rules.yml")),
                        enricher=None
                    )

                    # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è batch
                    news_batch = await self._collect_news_batch(parser, source, 1)  # –¢–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ

                    if news_batch:
                        logger.info(f"‚ú® Processing {len(news_batch)} new items from {source.code}")
                        
                        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ NER –∏ CEG
                        detailed_results = await self._process_news_batch(news_batch, source)
                        
                        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                        await self._save_batch_results(detailed_results, session)
                        
                        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                        self._update_batch_stats(detailed_results)

                # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
                await asyncio.sleep(settings.PARSER_POLL_INTERVAL)

            except Exception as e:
                logger.error(f"Error in batch monitoring {source.code}: {e}")
                await asyncio.sleep(60)  # –ñ–¥–µ–º –º–∏–Ω—É—Ç—É –ø—Ä–∏ –æ—à–∏–±–∫–µ

    async def _monitor_html_source_batch(self, source: Source):
        """üåê Batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∞ —Å –ø–æ–ª–Ω—ã–º CEG –∞–Ω–∞–ª–∏–∑–æ–º"""
        logger.info(f"üåê Starting BATCH monitor for HTML source {source.name} ({source.code})")

        try:
            # –°–Ω–∞—á–∞–ª–∞ –≤—ã–ø–æ–ª–Ω—è–µ–º batch backfill –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
            async with get_db_session() as session:
                if not self.realtime_mode:
                    logger.info(f"üì• Starting BATCH backfill for HTML source {source.code}")
                    
                    try:
                        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ HTML –ø–∞—Ä—Å–µ—Ä–∞
                        news_batch = await self._collect_html_news_batch(source, session)
                        
                        if news_batch:
                            logger.info(f"üìä Collected {len(news_batch)} HTML news items for batch processing")
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º batch —á–µ—Ä–µ–∑ NER –∏ CEG
                            detailed_results = await self._process_news_batch(news_batch, source)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                            await self._save_batch_results(detailed_results, session)
                            
                            logger.info(f"‚úÖ HTML batch processing completed for {source.code}")
                            logger.info(f"   - News processed: {len(news_batch)}")
                            logger.info(f"   - Entities extracted: {detailed_results.get('total_entities', 0)}")
                            logger.info(f"   - Graph nodes created: {detailed_results.get('total_nodes', 0)}")
                            logger.info(f"   - CEG links created: {detailed_results.get('total_links', 0)}")
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                            self.total_stats["html_sources_processed"] += 1
                            self.total_stats["html_articles_processed"] += len(news_batch)

                    except Exception as e:
                        logger.error(f"‚ùå HTML batch backfill failed for {source.code}: {e}", exc_info=True)

                # –í —Ä–µ–∂–∏–º–µ –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏ –≤—ã—Ö–æ–¥–∏–º –ø–æ—Å–ª–µ backfill
                if not self.realtime_mode:
                    logger.info(f"‚úÖ Historical batch loading completed for HTML source {source.code}")
                    return

            # –†–µ–∂–∏–º real-time batch –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            logger.info(f"üîÑ Starting real-time BATCH monitoring for HTML source {source.code}")
            while self.running:
                try:
                    async with get_db_session() as session:
                        # –ü–æ–ª—É—á–∞–µ–º –Ω–æ–≤—ã–µ —Å—Ç–∞—Ç—å–∏ –∏–∑ HTML –ø–∞—Ä—Å–µ—Ä–∞
                        news_batch = await self._collect_html_news_batch(source, session)

                        if news_batch:
                            logger.info(f"‚ú® Processing {len(news_batch)} new HTML items from {source.code}")
                            
                            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–µ—Ä–µ–∑ NER –∏ CEG
                            detailed_results = await self._process_news_batch(news_batch, source)
                            
                            # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                            await self._save_batch_results(detailed_results, session)
                            
                            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                            self._update_batch_stats(detailed_results)
                            self.total_stats["html_articles_processed"] += len(news_batch)

                    # –ñ–¥–µ–º –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π (HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∏ –æ–±–Ω–æ–≤–ª—è—é—Ç—Å—è —Ä–µ–∂–µ)
                    poll_interval = source.config.get("poll_interval", 3600) if source.config else 3600
                    await asyncio.sleep(poll_interval)

                except Exception as e:
                    logger.error(f"Error in HTML batch monitoring {source.code}: {e}")
                    await asyncio.sleep(300)  # –ñ–¥–µ–º 5 –º–∏–Ω—É—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ

        except Exception as e:
            logger.error(f"‚ùå Fatal error in HTML batch monitoring {source.code}: {e}", exc_info=True)

    async def _collect_news_batch(self, parser: Telegram_Parser, source: Source, days: int) -> List[Dict[str, Any]]:
        """üìä –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ Telegram –≤ batch —Ñ–æ—Ä–º–∞—Ç–µ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏"""
        try:
            # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–∂–∏–º–∞
            if days == 1:
                # –†–µ–∂–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ - —Ç–æ–ª—å–∫–æ –Ω–æ–≤—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
                limit = min(self.batch_size, source.config.get("realtime_limit", 50))
            else:
                # –†–µ–∂–∏–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
                limit = min(self.max_batches * self.batch_size, source.config.get("fetch_limit", 1000))
            
            logger.info(f"üì• Collecting up to {limit} messages from {source.name} (last {days} days)")
            
            # –ü–æ–ª—É—á–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Telegram —Å retry –ª–æ–≥–∏–∫–æ–π
            messages = await self._retry_with_backoff(
                self._fetch_telegram_messages, parser, source, days, limit
            )
            
            logger.info(f"‚úÖ Collected {len(messages)} valid messages from {source.name}")
            return messages
            
        except Exception as e:
            logger.error(f"‚ùå Error collecting news batch from {source.name}: {e}")
            return []

    async def _collect_html_news_batch(self, source: Source, session) -> List[Dict[str, Any]]:
        """üåê –°–æ–±–∏—Ä–∞–µ—Ç –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –≤ batch —Ñ–æ—Ä–º–∞—Ç–µ"""
        try:
            if not self.html_parser_service:
                logger.warning(f"‚ö†Ô∏è HTML parser service not available for {source.code}")
                return []
            
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å—Ç–∞—Ç–µ–π
            max_articles = source.config.get("max_articles_per_section", 25) if source.config else 25
            
            logger.info(f"üåê Collecting up to {max_articles} articles from {source.name}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º HTML parser service –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π
            parser_stats = await self.html_parser_service.parse_specific_source(
                source.code, 
                max_articles
            )
            
            if 'error' in parser_stats:
                logger.error(f"‚ùå Error parsing HTML source {source.code}: {parser_stats['error']}")
                return []
            
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ –∏–∑ –ë–î –¥–ª—è —ç—Ç–æ–≥–æ –∏—Å—Ç–æ—á–Ω–∏–∫–∞
            # –ü–æ–ª—É—á–∞–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–Ω–∏)
            days_back = self.days if not self.realtime_mode else 1
            
            result = await session.execute(
                select(News).where(
                    News.source_id == source.id,
                    News.published_at >= datetime.now() - timedelta(days=days_back)
                ).order_by(News.published_at.desc()).limit(max_articles)
            )
            news_items = result.scalars().all()
            
            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç batch
            news_batch = []
            for news in news_items:
                news_batch.append({
                    "id": str(news.id),
                    "text": news.text_plain or news.title or "",
                    "title": news.title,
                    "date": news.published_at,
                    "source_id": source.id,
                    "source_name": source.name,
                    "external_id": news.external_id,
                    "url": news.url,
                    "ad_score": 0.0,  # HTML –∏—Å—Ç–æ—á–Ω–∏–∫–∏ —Å—á–∏—Ç–∞–µ–º –Ω–µ —Ä–µ–∫–ª–∞–º–æ–π
                    "ad_reasons": []
                })
            
            logger.info(f"‚úÖ Collected {len(news_batch)} HTML news items from {source.name}")
            return news_batch
            
        except Exception as e:
            logger.error(f"‚ùå Error collecting HTML news batch from {source.name}: {e}")
            return []

    async def _fetch_telegram_messages(self, parser: Telegram_Parser, source: Source, days: int, limit: int) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –∏–∑ Telegram —Å —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–µ–π"""
        messages = []
        collected = 0
        
        try:
            offset_date = datetime.now() - timedelta(days=days) if days > 1 else None
            
            async for message in parser.client.iter_messages(
                entity=source.tg_chat_id,
                limit=limit,
                offset_date=offset_date
            ):
                if collected >= limit:
                    break
                    
                if message.text or message.message:
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ —Ä–µ–∫–ª–∞–º—É
                    is_ad, ad_score, ad_reasons = await parser.anti_spam.check_message(
                        message, source.trust_level
                    )
                    
                    if not is_ad:
                        messages.append({
                            "id": message.id,
                            "text": message.text or message.message or "",
                            "date": message.date,
                            "source_id": source.id,
                            "source_name": source.name,
                            "external_id": f"tg_{source.tg_chat_id}_{message.id}",
                            "ad_score": ad_score,
                            "ad_reasons": ad_reasons
                        })
                        collected += 1
                    else:
                        logger.debug(f"üö´ Message {message.id} filtered as ad (score: {ad_score})")
            
            return messages
            
        except Exception as e:
            logger.error(f"‚ùå Error fetching Telegram messages: {e}")
            raise

    async def _process_news_batch(self, news_batch: List[Dict[str, Any]], source: Source) -> Dict[str, Any]:
        """üß† –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç batch –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NER –∏ CEG —Å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–º–∏ —á–∞–Ω–∫–∞–º–∏"""
        try:
            # –†–∞–∑–±–∏–≤–∞–µ–º –Ω–∞ —á–∞–Ω–∫–∏ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            chunks = []
            for i in range(0, len(news_batch), self.batch_size):
                chunk = news_batch[i:i + self.batch_size]
                chunks.append((i // self.batch_size + 1, chunk))

            total_chunks = len(chunks)
            logger.info(f"üîç Processing {len(news_batch)} news items in {total_chunks} parallel chunks of {self.batch_size}")

            # –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —á–∞–Ω–∫–æ–≤
            chunk_tasks = [
                self._process_single_chunk(chunk_num, chunk, total_chunks, source)
                for chunk_num, chunk in chunks
            ]

            # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—Å–µ —á–∞–Ω–∫–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            chunk_results = await asyncio.gather(*chunk_tasks, return_exceptions=True)

            # –°–æ–±–∏—Ä–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            all_ner_results = []
            all_graph_results = {"nodes_created": 0, "links_created": 0}
            all_ceg_results = []

            for result in chunk_results:
                if isinstance(result, Exception):
                    logger.error(f"‚ùå Chunk processing failed: {result}")
                    continue

                if result and not result.get("error"):
                    all_ner_results.extend(result.get("ner_results", []))
                    all_graph_results["nodes_created"] += result.get("nodes_created", 0)
                    all_graph_results["links_created"] += result.get("links_created", 0)
                    if result.get("ceg_results"):
                        all_ceg_results.append(result["ceg_results"])
            
            logger.info(f"‚úÖ NER extracted {len(all_ner_results)} processed items from {len(news_batch)} news")
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π JSON –æ—Ç–≤–µ—Ç
            combined_ner_results = {"news_items": all_ner_results}
            combined_ceg_results = self._combine_ceg_results(all_ceg_results)
            
            detailed_response = await self._create_detailed_response(
                news_batch, combined_ner_results, all_graph_results, combined_ceg_results
            )
            
            return {
                "total_news": len(news_batch),
                "total_entities": len(all_ner_results),
                "total_nodes": all_graph_results["nodes_created"],
                "total_links": all_graph_results["links_created"],
                "ceg_analysis": combined_ceg_results,
                "detailed_response": detailed_response,
                "batch_id": str(uuid4()),
                "chunks_processed": (len(news_batch) + self.batch_size - 1) // self.batch_size
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error processing news batch: {e}", exc_info=True)
            return {"error": str(e), "total_news": len(news_batch)}

    async def _process_single_chunk(self, chunk_num: int, chunk: List[Dict[str, Any]], total_chunks: int, source: Source) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ–¥–∏–Ω —á–∞–Ω–∫ –Ω–æ–≤–æ—Å—Ç–µ–π (–¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è)"""
        try:
            logger.info(f"üì¶ Processing chunk {chunk_num}/{total_chunks} with {len(chunk)} items")

            # –§–æ—Ä–º–∏—Ä—É–µ–º JSON –¥–ª—è NER –æ–±—Ä–∞–±–æ—Ç–∫–∏ —á–∞–Ω–∫–∞
            news_texts = [item["text"] for item in chunk]
            news_metadata = [
                {
                    "id": item["id"],
                    "date": item["date"].isoformat() if hasattr(item["date"], 'isoformat') else str(item["date"]),
                    "source": item["source_name"]
                }
                for item in chunk
            ]

            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —á–∞–Ω–∫ —á–µ—Ä–µ–∑ NER —Å–∏—Å—Ç–µ–º—É —Å retry
            ner_chunk_results = await self._retry_with_backoff(
                self._process_ner_chunk, news_texts, news_metadata
            )

            if not ner_chunk_results:
                return {"error": "NER processing failed", "ner_results": [], "nodes_created": 0, "links_created": 0}

            # –°–æ–∑–¥–∞–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î –¥–ª—è —á–∞–Ω–∫–∞
            graph_chunk_results = await self._create_graph_entities(ner_chunk_results, chunk)

            # –í—ã–ø–æ–ª–Ω—è–µ–º CEG –∞–Ω–∞–ª–∏–∑ –¥–ª—è —á–∞–Ω–∫–∞
            ceg_chunk_results = await self._perform_ceg_analysis(graph_chunk_results, source)

            logger.info(f"‚úÖ Chunk {chunk_num}/{total_chunks} completed: {graph_chunk_results.get('nodes_created', 0)} nodes, {graph_chunk_results.get('links_created', 0)} links")

            return {
                "ner_results": ner_chunk_results.get('news_items', []),
                "nodes_created": graph_chunk_results.get("nodes_created", 0),
                "links_created": graph_chunk_results.get("links_created", 0),
                "ceg_results": ceg_chunk_results
            }

        except Exception as e:
            logger.error(f"‚ùå Error processing chunk {chunk_num}: {e}", exc_info=True)
            return {"error": str(e), "ner_results": [], "nodes_created": 0, "links_created": 0}

    async def _process_ner_chunk(self, news_texts: List[str], news_metadata: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —á–∞–Ω–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —á–µ—Ä–µ–∑ NER —Å–∏—Å—Ç–µ–º—É"""
        try:
            if self.use_local_ai:
                # –õ–æ–∫–∞–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ (—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è, —Ç.–∫. LocalFinanceNERExtractor –Ω–µ –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π)
                # –ó–∞–ø—É—Å–∫–∞–µ–º –≤ executor —á—Ç–æ–±—ã –Ω–µ –±–ª–æ–∫–∏—Ä–æ–≤–∞—Ç—å event loop
                import concurrent.futures
                loop = asyncio.get_event_loop()
                with concurrent.futures.ThreadPoolExecutor() as executor:
                    entities_list = await loop.run_in_executor(
                        executor,
                        lambda: self.ner_extractor.extract_entities_batch(news_texts, verbose=False)
                    )
            else:
                # API –æ–±—Ä–∞–±–æ—Ç–∫–∞ - –∏—Å–ø–æ–ª—å–∑—É–µ–º –∞—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –Ω–∞–ø—Ä—è–º—É—é
                entities_list = await self.ner_extractor.extract_entities_batch_async(news_texts, verbose=False)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ —Ñ–æ—Ä–º–∞—Ç –¥–ª—è –¥–∞–ª—å–Ω–µ–π—à–µ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
            news_items = []
            for i, entities in enumerate(entities_list):
                if entities:
                    news_items.append({
                        "id": news_metadata[i]["id"],
                        "date": news_metadata[i]["date"],
                        "source": news_metadata[i]["source"],
                        "companies": [{"name": c.name, "ticker": c.ticker, "sector": c.sector} for c in entities.companies],
                        "people": [{"name": p.name, "position": p.position, "company": p.company} for p in entities.people],
                        "markets": [{"name": m.name, "type": m.type, "value": m.value, "change": m.change} for m in entities.markets],
                        "financial_metrics": [{"metric_type": fm.metric_type, "value": fm.value, "company": fm.company} for fm in entities.financial_metrics],
                        "event": "",  # –ú–æ–¥–µ–ª—å –Ω–µ –∏–∑–≤–ª–µ–∫–∞–µ—Ç event –Ω–∞–ø—Ä—è–º—É—é
                        "event_types": [],
                        "sector": entities.companies[0].sector if entities.companies else None,
                        "country": "RU",
                        "is_advertisement": False,
                        "content_types": [],
                        "confidence_score": 0.9 if not self.use_local_ai else 0.8,
                        "urgency_level": "normal"
                    })
                else:
                    # –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è –Ω–µ—É–¥–∞—á–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
                    news_items.append({
                        "id": news_metadata[i]["id"],
                        "date": news_metadata[i]["date"],
                        "source": news_metadata[i]["source"],
                        "companies": [],
                        "people": [],
                        "markets": [],
                        "financial_metrics": [],
                        "event": "",
                        "event_types": [],
                        "sector": None,
                        "country": "RU",
                        "is_advertisement": False,
                        "content_types": [],
                        "confidence_score": 0.0,
                        "urgency_level": "normal"
                    })

            return {"news_items": news_items}
        except Exception as e:
            logger.error(f"‚ùå NER processing failed for chunk: {e}")
            raise

    def _combine_ceg_results(self, ceg_results_list: List[Dict[str, Any]]) -> Dict[str, Any]:
        """–û–±—ä–µ–¥–∏–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã CEG –∞–Ω–∞–ª–∏–∑–∞ –∏–∑ —Ä–∞–∑–Ω—ã—Ö —á–∞–Ω–∫–æ–≤"""
        combined = {
            "causal_links_found": 0,
            "importance_scores": [],
            "market_impacts": [],
            "predictions": [],
            "total_chunks": len(ceg_results_list)
        }
        
        for result in ceg_results_list:
            if not result.get("error"):
                combined["causal_links_found"] += result.get("causal_links_found", 0)
                combined["importance_scores"].extend(result.get("importance_scores", []))
                combined["market_impacts"].extend(result.get("market_impacts", []))
                combined["predictions"].extend(result.get("predictions", []))
        
        return combined

    async def _create_graph_entities(self, ner_results: Dict[str, Any], news_batch: List[Dict[str, Any]]) -> Dict[str, Any]:
        """üï∏Ô∏è –°–æ–∑–¥–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –≤ –≥—Ä–∞—Ñ–æ–≤–æ–π –ë–î Neo4j —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –æ—à–∏–±–æ–∫"""
        try:
            if not self.graph_service:
                logger.warning("‚ö†Ô∏è Graph service not available, skipping graph creation")
                return {"nodes_created": 0, "links_created": 0, "error": "Graph service unavailable"}

            nodes_created = 0
            links_created = 0

            for i, news_item in enumerate(ner_results.get('news_items', [])):
                if not news_item or i >= len(news_batch):
                    continue

                news_data = news_batch[i]
                # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π ID –∏–∑ –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö –≤–º–µ—Å—Ç–æ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ
                news_id = str(news_data.get("id", uuid4()))
                
                try:
                    # –°–æ–∑–¥–∞–µ–º —É–∑–µ–ª –Ω–æ–≤–æ—Å—Ç–∏ —Å retry
                    await self._retry_with_backoff(
                        self._create_news_node_safe,
                        news_id, news_data, news_item
                    )
                    nodes_created += 1
                    logger.debug(f"‚úÖ Created news node: {news_id}")

                    # –ü–æ–∏—Å–∫ —Ç–∏–∫–µ—Ä–æ–≤ —á–µ—Ä–µ–∑ MOEX Linker
                    ticker_matches = await self._find_tickers_for_news(news_data, news_item)

                    # –°–æ–∑–¥–∞–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ —Å –Ω–∞–π–¥–µ–Ω–Ω—ã–º–∏ —Ç–∏–∫–µ—Ä–∞–º–∏
                    companies = news_item.get("companies", [])
                    if companies:
                        logger.debug(f"üìä Found {len(companies)} companies in news")
                        for company in companies:
                            if company.get("name"):
                                # –ò—â–µ–º —Ç–∏–∫–µ—Ä –¥–ª—è —ç—Ç–æ–π –∫–æ–º–ø–∞–Ω–∏–∏
                                ticker_info = self._find_ticker_for_company(company.get("name", ""), ticker_matches)

                                company_id = f"company_{hash(company.get('name', ''))}"
                                await self._retry_with_backoff(
                                    self._create_company_node_with_ticker_safe,
                                    company_id, company, ticker_info
                                )
                                await self._retry_with_backoff(
                                    self._link_news_to_company_safe,
                                    news_id, company_id
                                )
                                links_created += 1
                                logger.debug(f"üîó Linked news {news_id} to company {company.get('name')}")
                    
                    # –°–æ–∑–¥–∞–µ–º –ø–µ—Ä—Å–æ–Ω –∏ —Å–≤—è–∑—ã–≤–∞–µ–º
                    for person in news_item.get("people", []):
                        if person.get("name"):
                            entity_id = f"person_{hash(person.get('name', ''))}"
                            await self._retry_with_backoff(
                                self._create_entity_node_safe,
                                entity_id, person
                            )
                            await self._retry_with_backoff(
                                self._link_news_to_entity_safe,
                                news_id, entity_id
                            )
                            links_created += 1
                    
                    # –°–æ–∑–¥–∞–µ–º —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
                    for metric in news_item.get("financial_metrics", []):
                        if metric.get("value"):
                            metric_id = f"metric_{hash(metric.get('value', ''))}"
                            await self._retry_with_backoff(
                                self._create_metric_node_safe,
                                metric_id, metric
                            )
                            await self._retry_with_backoff(
                                self._link_news_to_metric_safe,
                                news_id, metric_id
                            )
                            links_created += 1
                
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è Failed to create graph entities for news {news_data.get('id', 'unknown')}: {e}")
                    continue
            
            logger.info(f"‚úÖ Created {nodes_created} nodes and {links_created} links in graph")
            return {
                "nodes_created": nodes_created,
                "links_created": links_created
            }
            
        except Exception as e:
            logger.error(f"‚ùå Error creating graph entities: {e}")
            return {"nodes_created": 0, "links_created": 0, "error": str(e)}

    async def _create_news_node_safe(self, news_id: str, news_data: Dict[str, Any], news_item: Dict[str, Any]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–∞ –Ω–æ–≤–æ—Å—Ç–∏"""
        # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –¥–∞—Ç—É –≤ datetime –µ—Å–ª–∏ —ç—Ç–æ —Å—Ç—Ä–æ–∫–∞ –∏–ª–∏ –æ—Å—Ç–∞–≤–ª—è–µ–º –∫–∞–∫ –µ—Å—Ç—å
        published_at = news_data.get("date")
        if isinstance(published_at, str):
            # –ü–∞—Ä—Å–∏–º ISO —Ñ–æ—Ä–º–∞—Ç —Å—Ç—Ä–æ–∫–∏ –æ–±—Ä–∞—Ç–Ω–æ –≤ datetime
            try:
                published_at = datetime.fromisoformat(published_at.replace('Z', '+00:00'))
            except (ValueError, AttributeError):
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å, –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â–µ–µ –≤—Ä–µ–º—è
                published_at = datetime.now()
        elif not published_at:
            published_at = datetime.now()

        # –§–æ—Ä–º–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–æ–∫ –∏–∑ —Å–æ–±—ã—Ç–∏—è –∏–ª–∏ —Ç–µ–∫—Å—Ç–∞
        title = news_item.get("event", "")[:200]
        if not title:
            title = news_data.get("title", news_data.get("text", "")[:200])
        if not title:
            title = "Telegram News"

        await self.graph_service.create_news_node({
            "id": news_id,
            "url": f"telegram://{news_data.get('url') or news_data.get('external_id', '')}",
            "title": title,
            "text": news_data.get("text", "")[:1000],  # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä —Ç–µ–∫—Å—Ç–∞
            "published_at": published_at,
            "source": news_data.get("source_name", ""),
            "lang_orig": "ru",
            "lang_norm": "ru",
            "no_impact": False,
            "news_type": None,
            "news_subtype": None
        })

    async def _create_company_node_safe(self, company_id: str, company: Dict[str, Any]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–∞ –∫–æ–º–ø–∞–Ω–∏–∏"""
        await self.graph_service.create_company_node(
            company_id=company_id,
            name=company.get("name", "")[:100],
            ticker=company.get("ticker", "")[:20],
            is_traded=bool(company.get("ticker"))
        )

    async def _link_news_to_company_safe(self, news_id: str, company_id: str):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –∫–æ–º–ø–∞–Ω–∏–µ–π"""
        await self.graph_service.link_news_to_company(news_id, company_id)

    async def _create_entity_node_safe(self, entity_id: str, person: Dict[str, Any]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–∞ –ø–µ—Ä—Å–æ–Ω—ã"""
        await self.graph_service.create_entity_node(
            entity_id=entity_id,
            text=person.get("name", "")[:100],
            entity_type="PERSON",
            confidence=0.9
        )

    async def _link_news_to_entity_safe(self, news_id: str, entity_id: str):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –ø–µ—Ä—Å–æ–Ω–æ–π"""
        await self.graph_service.link_news_to_entity(news_id, entity_id)

    async def _create_metric_node_safe(self, metric_id: str, metric: Dict[str, Any]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–∞ –º–µ—Ç—Ä–∏–∫–∏"""
        await self.graph_service.create_entity_node(
            entity_id=metric_id,
            text=f"{metric.get('metric_type', '')}: {metric.get('value', '')}",
            entity_type="FINANCIAL_METRIC",
            confidence=0.8
        )

    async def _link_news_to_metric_safe(self, news_id: str, metric_id: str):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–≤—è–∑—ã–≤–∞–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏ —Å –º–µ—Ç—Ä–∏–∫–æ–π"""
        await self.graph_service.link_news_to_entity(news_id, metric_id)

    async def _find_tickers_for_news(self, news_data: Dict[str, Any], news_item: Dict[str, Any]) -> List[Dict[str, Any]]:
        """üîç –ù–∞—Ö–æ–¥–∏—Ç —Ç–∏–∫–µ—Ä—ã –¥–ª—è –Ω–æ–≤–æ—Å—Ç–∏ —á–µ—Ä–µ–∑ MOEX Linker"""
        try:
            if not self.moex_linker:
                logger.warning("‚ö†Ô∏è MOEX Linker not available")
                return []

            # –°–æ–±–∏—Ä–∞–µ–º –≤—Å–µ –≤–æ–∑–º–æ–∂–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã –¥–ª—è –ø–æ–∏—Å–∫–∞
            search_texts = []

            # 1. –¢–µ–∫—Å—Ç –Ω–æ–≤–æ—Å—Ç–∏
            if news_data.get('text'):
                search_texts.append(news_data['text'])

            # 2. –ù–∞–∑–≤–∞–Ω–∏—è –∫–æ–º–ø–∞–Ω–∏–π –∏–∑ NER
            companies_from_ner = news_item.get("companies", [])
            if companies_from_ner:
                for company in companies_from_ner:
                    if company.get("name"):
                        search_texts.append(company["name"])

            # 3. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –µ—Å–ª–∏ –µ—Å—Ç—å
            if news_data.get('title'):
                search_texts.append(news_data['title'])

            if not search_texts:
                return []

            # –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å–µ —Ç–µ–∫—Å—Ç—ã
            full_text = " | ".join(search_texts)

            # –ò—â–µ–º –∫–æ–º–ø–∞–Ω–∏–∏ –≤ —Ç–µ–∫—Å—Ç–µ
            company_matches = await self.moex_linker.link_companies(full_text)

            # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º –≤ —É–¥–æ–±–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç
            ticker_matches = []
            for match in company_matches:
                if match.ticker:
                    ticker_matches.append({
                        "ticker": match.ticker,
                        "company_name": match.company_name,
                        "short_name": match.short_name,
                        "confidence": match.confidence,
                        "match_method": match.match_method,
                        "is_traded": match.is_traded,
                        "isin": match.isin,
                        "board": match.board,
                        "original_text": match.text
                    })

            if ticker_matches:
                logger.info(f"üîç Found {len(ticker_matches)} ticker matches: {[t['ticker'] for t in ticker_matches]}")
            else:
                logger.debug(f"üîç No tickers found in text: {full_text[:100]}...")

            return ticker_matches

        except Exception as e:
            logger.error(f"‚ùå Error finding tickers: {e}", exc_info=True)
            return []

    def _find_ticker_for_company(self, company_name: str, ticker_matches: List[Dict[str, Any]]) -> Optional[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç —Ç–∏–∫–µ—Ä –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –∫–æ–º–ø–∞–Ω–∏–∏"""
        try:
            # –ü—Ä–æ—Å—Ç–æ–π –ø–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—é
            company_name_lower = company_name.lower()
            
            for ticker_match in ticker_matches:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞–∑–ª–∏—á–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã –Ω–∞–∑–≤–∞–Ω–∏–π
                ticker_names = [
                    ticker_match.get("company_name", "").lower(),
                    ticker_match.get("short_name", "").lower(),
                    ticker_match.get("original_text", "").lower()
                ]
                
                # –ò—â–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
                for ticker_name in ticker_names:
                    if ticker_name and self._names_similar(company_name_lower, ticker_name):
                        return ticker_match
            
            return None
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error matching ticker for {company_name}: {e}")
            return None

    def _names_similar(self, name1: str, name2: str, threshold: float = 0.6) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏–π –∫–æ–º–ø–∞–Ω–∏–π"""
        try:
            from difflib import SequenceMatcher
            
            # –£–±–∏—Ä–∞–µ–º –æ–±—â–∏–µ —Å–ª–æ–≤–∞
            stop_words = {"–ø–∞–æ", "–æ–∞–æ", "–æ–æ–æ", "–∞–æ", "–∑–∞–æ", "–∫–æ–º–ø–∞–Ω–∏—è", "–≥—Ä—É–ø–ø–∞", "–±–∞–Ω–∫"}
            
            words1 = set(name1.split()) - stop_words
            words2 = set(name2.split()) - stop_words
            
            if not words1 or not words2:
                return False
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å–ª–æ–≤
            intersection = words1.intersection(words2)
            union = words1.union(words2)
            
            jaccard_similarity = len(intersection) / len(union) if union else 0
            
            # –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —á–µ—Ä–µ–∑ SequenceMatcher
            sequence_similarity = SequenceMatcher(None, name1, name2).ratio()
            
            # –ö–æ–º–±–∏–Ω–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –æ—Ü–µ–Ω–∫–∞
            combined_similarity = (jaccard_similarity * 0.7 + sequence_similarity * 0.3)
            
            return combined_similarity >= threshold
            
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Error comparing names: {e}")
            return False

    async def _create_company_node_with_ticker_safe(self, company_id: str, company: Dict[str, Any], ticker_info: Optional[Dict[str, Any]]):
        """–ë–µ–∑–æ–ø–∞—Å–Ω–æ–µ —Å–æ–∑–¥–∞–Ω–∏–µ —É–∑–ª–∞ –∫–æ–º–ø–∞–Ω–∏–∏ —Å —Ç–∏–∫–µ—Ä–æ–º"""
        try:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –¥–∞–Ω–Ω—ã–µ –∏–∑ ticker_info –µ—Å–ª–∏ –µ—Å—Ç—å
            if ticker_info:
                await self.graph_service.create_company_node(
                    company_id=company_id,
                    name=ticker_info.get("company_name", company.get("name", ""))[:100],
                    ticker=ticker_info.get("ticker", company.get("ticker", ""))[:20],
                    is_traded=ticker_info.get("is_traded", True)
                )
                
                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç–∏–∫–µ—Ä–µ
                await self.graph_service.create_instrument_node(
                    instrument_id=f"instrument_{ticker_info.get('ticker', '')}",
                    symbol=ticker_info.get("ticker", ""),
                    instrument_type="equity",
                    exchange="MOEX",
                    currency="RUB"
                )
                
                # –°–≤—è–∑—ã–≤–∞–µ–º –∫–æ–º–ø–∞–Ω–∏—é —Å –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–º
                await self.graph_service.link_company_to_instrument(
                    company_id=company_id,
                    instrument_id=f"instrument_{ticker_info.get('ticker', '')}"
                )
                
                logger.info(f"‚úÖ Created company {company.get('name', '')} with ticker {ticker_info.get('ticker', '')}")
            else:
                # –°–æ–∑–¥–∞–µ–º –±–µ–∑ —Ç–∏–∫–µ—Ä–∞
                await self.graph_service.create_company_node(
                    company_id=company_id,
                    name=company.get("name", "")[:100],
                    ticker=company.get("ticker", "")[:20],
                    is_traded=False
                )
                
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Failed to create company node with ticker: {e}")
            # Fallback - —Å–æ–∑–¥–∞–µ–º –±–µ–∑ —Ç–∏–∫–µ—Ä–∞
            await self.graph_service.create_company_node(
                company_id=company_id,
                name=company.get("name", "")[:100],
                ticker="",
                is_traded=False
            )

    async def _perform_ceg_analysis(self, graph_results: Dict[str, Any], source: Source) -> Dict[str, Any]:
        """üîó –í—ã–ø–æ–ª–Ω—è–µ—Ç CEG –∞–Ω–∞–ª–∏–∑ –∏ —Ä–∞—Å—á–µ—Ç—ã"""
        try:
            if not self.enricher or not self.enricher.ceg_service:
                logger.warning("‚ö†Ô∏è CEG service not available, using basic analysis")
                return {
                    "causal_links_found": 0,
                    "importance_scores": [],
                    "market_impacts": [],
                    "predictions": [],
                    "analysis_type": "basic"
                }
            
            logger.info("üîó Performing CEG analysis...")
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π CEG –∞–Ω–∞–ª–∏–∑
            ceg_analysis = {
                "causal_links_found": 0,
                "importance_scores": [],
                "market_impacts": [],
                "predictions": [],
                "analysis_type": "advanced"
            }
            
            try:
                # –ê–Ω–∞–ª–∏–∑ –≤–∞–∂–Ω–æ—Å—Ç–∏ —Å–æ–±—ã—Ç–∏–π
                importance_scores = await self._calculate_importance_scores(graph_results)
                ceg_analysis["importance_scores"] = importance_scores
                
                # –ê–Ω–∞–ª–∏–∑ —Ä—ã–Ω–æ—á–Ω–æ–≥–æ –≤–ª–∏—è–Ω–∏—è
                market_impacts = await self._calculate_market_impacts(graph_results)
                ceg_analysis["market_impacts"] = market_impacts
                
                # –ü–æ–∏—Å–∫ –ø—Ä–∏—á–∏–Ω–Ω—ã—Ö —Å–≤—è–∑–µ–π
                causal_links = await self._find_causal_links(graph_results)
                ceg_analysis["causal_links_found"] = len(causal_links)
                
                # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π
                predictions = await self._generate_predictions(graph_results, source)
                ceg_analysis["predictions"] = predictions
                
                logger.info(f"‚úÖ CEG analysis completed: {len(importance_scores)} importance scores, {len(market_impacts)} impacts, {len(causal_links)} causal links, {len(predictions)} predictions")
                
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Advanced CEG analysis failed: {e}")
                ceg_analysis["analysis_type"] = "fallback"
            
            return ceg_analysis
            
        except Exception as e:
            logger.error(f"‚ùå Error in CEG analysis: {e}")
            return {"error": str(e), "analysis_type": "error"}

    async def _calculate_importance_scores(self, graph_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –≤–∞–∂–Ω–æ—Å—Ç—å —Å–æ–±—ã—Ç–∏–π"""
        try:
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –≤–∞–∂–Ω–æ—Å—Ç–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å–≤—è–∑–µ–π
            scores = []
            nodes_created = graph_results.get("nodes_created", 0)
            links_created = graph_results.get("links_created", 0)
            
            if nodes_created > 0:
                # –ë–∞–∑–æ–≤—ã–π score –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–æ—Ç–Ω–æ—Å—Ç–∏ —Å–≤—è–∑–µ–π
                density_score = links_created / nodes_created if nodes_created > 0 else 0
                scores.append({
                    "score": min(density_score * 10, 10.0),  # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –¥–æ 10
                    "metric": "link_density",
                    "description": f"Based on {links_created} links for {nodes_created} nodes"
                })
            
            return scores
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Importance calculation failed: {e}")
            return []

    async def _calculate_market_impacts(self, graph_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ —Ä—ã–Ω–æ—á–Ω–æ–µ –≤–ª–∏—è–Ω–∏–µ"""
        try:
            impacts = []
            
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π
            nodes_created = graph_results.get("nodes_created", 0)
            links_created = graph_results.get("links_created", 0)
            
            if nodes_created > 0:
                # –ë–∞–∑–æ–≤—ã–π impact score
                impact_score = min((nodes_created + links_created) * 0.1, 5.0)
                impacts.append({
                    "impact_score": impact_score,
                    "impact_type": "estimated",
                    "description": f"Estimated impact based on {nodes_created} nodes and {links_created} links",
                    "confidence": 0.3  # –ù–∏–∑–∫–∞—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –¥–ª—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∏
                })
            
            return impacts
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Market impact calculation failed: {e}")
            return []

    async def _find_causal_links(self, graph_results: Dict[str, Any]) -> List[Dict[str, Any]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏—á–∏–Ω–Ω—ã–µ —Å–≤—è–∑–∏"""
        try:
            links = []
            
            # –ü—Ä–æ—Å—Ç–∞—è —ç–≤—Ä–∏—Å—Ç–∏–∫–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            nodes_created = graph_results.get("nodes_created", 0)
            
            if nodes_created > 1:
                # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º, —á—Ç–æ –Ω–æ–≤–æ—Å—Ç–∏ –º–æ–≥—É—Ç –±—ã—Ç—å —Å–≤—è–∑–∞–Ω—ã
                links.append({
                    "link_type": "temporal_correlation",
                    "confidence": 0.2,
                    "description": f"Potential temporal correlation between {nodes_created} news items",
                    "strength": "weak"
                })
            
            return links
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Causal link detection failed: {e}")
            return []

    async def _generate_predictions(self, graph_results: Dict[str, Any], source: Source) -> List[Dict[str, Any]]:
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞"""
        try:
            predictions = []
            
            # –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤
            nodes_created = graph_results.get("nodes_created", 0)
            
            if nodes_created > 5:
                predictions.append({
                    "prediction_type": "follow_up_news",
                    "confidence": 0.4,
                    "timeframe": "1-3 days",
                    "description": f"High activity in {source.name} may lead to follow-up news",
                    "source": source.name
                })
            
            if nodes_created > 10:
                predictions.append({
                    "prediction_type": "market_volatility",
                    "confidence": 0.3,
                    "timeframe": "1-7 days",
                    "description": "High news volume may indicate increased market volatility",
                    "source": source.name
                })
            
            return predictions
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Prediction generation failed: {e}")
            return []

    async def _create_detailed_response(self, news_batch: List[Dict[str, Any]], ner_results: Dict[str, Any], 
                                      graph_results: Dict[str, Any], ceg_results: Dict[str, Any]) -> Dict[str, Any]:
        """üìã –°–æ–∑–¥–∞–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π JSON –æ—Ç–≤–µ—Ç —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏"""
        try:
            detailed_news = []
            
            for i, news_data in enumerate(news_batch):
                ner_item = ner_results.get('news_items', [{}])[i] if i < len(ner_results.get('news_items', [])) else {}
                
                # –ù–∞—Ö–æ–¥–∏–º —Ç–∏–∫–µ—Ä—ã –¥–ª—è —ç—Ç–æ–π –Ω–æ–≤–æ—Å—Ç–∏
                ticker_matches = await self._find_tickers_for_news(news_data, ner_item)
                
                # –§–æ—Ä–º–∏—Ä—É–µ–º –¥–µ—Ç–∞–ª—å–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–æ–≤–æ—Å—Ç–∏
                detailed_item = {
                    "news_id": str(news_data["id"]) if news_data.get("id") else None,
                    "source": {
                        "id": str(news_data["source_id"]) if news_data.get("source_id") else None,
                        "name": news_data["source_name"],
                        "external_id": news_data["external_id"]
                    },
                    "content": {
                        "text": news_data["text"],
                        "date": news_data["date"].isoformat(),
                        "title": ner_item.get("event", "")[:200] or "Telegram News"
                    },
                    "entities": {
                        "companies": ner_item.get("companies", []),
                        "people": ner_item.get("people", []),
                        "markets": ner_item.get("markets", []),
                        "financial_metrics": ner_item.get("financial_metrics", [])
                    },
                    "analysis": {
                        "event_types": ner_item.get("event_types", []),
                        "sector": ner_item.get("sector"),
                        "country": ner_item.get("country"),
                        "is_advertisement": ner_item.get("is_advertisement", False),
                        "content_types": ner_item.get("content_types", []),
                        "confidence_score": ner_item.get("confidence_score", 0.0),
                        "urgency_level": ner_item.get("urgency_level", "normal")
                    },
                    "tickers": [ticker.get("ticker") for ticker in ticker_matches if ticker.get("ticker")],
                    "ticker_details": [
                        {
                            "ticker": ticker.get("ticker"),
                            "company_name": ticker.get("company_name"),
                            "short_name": ticker.get("short_name"),
                            "confidence": ticker.get("confidence"),
                            "match_method": ticker.get("match_method"),
                            "is_traded": ticker.get("is_traded"),
                            "isin": ticker.get("isin"),
                            "board": ticker.get("board"),
                            "original_text": ticker.get("original_text")
                        }
                        for ticker in ticker_matches
                    ],
                    "related_news": [],  # –°–≤—è–∑–∏ —Å –¥—Ä—É–≥–∏–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏
                    "ceg_metrics": {
                        "importance_score": 0.0,
                        "market_impact": 0.0,
                        "causal_links": 0,
                        "predictions": []
                    },
                    "future_events": []  # –í–æ–∑–º–æ–∂–Ω—ã–µ –±—É–¥—É—â–∏–µ —Å–æ–±—ã—Ç–∏—è
                }
                
                detailed_news.append(detailed_item)
            
            return {
                "batch_id": str(uuid4()),
                "timestamp": datetime.now().isoformat(),
                "total_news": len(news_batch),
                "processing_stats": {
                    "entities_extracted": len(ner_results.get('news_items', [])),
                    "graph_nodes_created": graph_results.get("nodes_created", 0),
                    "graph_links_created": graph_results.get("links_created", 0),
                    "ceg_analysis_completed": bool(ceg_results.get("causal_links_found") is not None)
                },
                "news_items": detailed_news,
                "summary": {
                    "total_tickers": len(set([ticker for item in detailed_news for ticker in item["tickers"]])),
                    "total_companies": len(set([company["name"] for item in detailed_news for company in item["entities"]["companies"]])),
                    "high_confidence_news": len([item for item in detailed_news if item["analysis"]["confidence_score"] > 0.8]),
                    "urgent_news": len([item for item in detailed_news if item["analysis"]["urgency_level"] in ["high", "critical"]])
                }
            }
            
        except Exception as e:
            logger.error(f"Error creating detailed response: {e}")
            return {"error": str(e), "batch_id": str(uuid4())}

    async def _save_batch_results(self, results: Dict[str, Any], session):
        """üíæ –°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã batch –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        try:
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π –æ—Ç–≤–µ—Ç –≤ —Ñ–∞–π–ª
            if "detailed_response" in results:
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"batch_results_{timestamp}.json"

                # –°–æ–∑–¥–∞–µ–º –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏—é logs –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
                logs_dir = Path("logs")
                logs_dir.mkdir(exist_ok=True)

                # –ö–∞—Å—Ç–æ–º–Ω—ã–π JSON encoder –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ UUID –∏ datetime
                from uuid import UUID
                class CustomJSONEncoder(json.JSONEncoder):
                    def default(self, obj):
                        if isinstance(obj, UUID):  # UUID
                            return str(obj)
                        if isinstance(obj, datetime):
                            return obj.isoformat()
                        return super().default(obj)

                with open(logs_dir / filename, "w", encoding="utf-8") as f:
                    json.dump(results["detailed_response"], f, ensure_ascii=False, indent=2, cls=CustomJSONEncoder)

                logger.info(f"üíæ Batch results saved to logs/{filename}")

            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–π JSON –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö PostgreSQL
            await self._save_detailed_json_to_db(results.get("detailed_response", {}))

            # –ö–æ–º–º–∏—Ç–∏–º –∏–∑–º–µ–Ω–µ–Ω–∏—è –≤ Neo4j
            await session.commit()

        except Exception as e:
            logger.error(f"Error saving batch results: {e}")

    async def _save_detailed_json_to_db(self, detailed_response: Dict[str, Any]) -> None:
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–µ—Ç–∞–ª—å–Ω—ã–π JSON –≤ –±–∞–∑—É –¥–∞–Ω–Ω—ã—Ö PostgreSQL"""
        try:
            if not detailed_response:
                return
                
            async with get_db_session() as db_session:
                # –ï—Å–ª–∏ —ç—Ç–æ —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π, –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –∫–∞–∂–¥—É—é
                if isinstance(detailed_response, list):
                    for news_data in detailed_response:
                        news_id = news_data.get('news_id')
                        if news_id:
                            await self._update_news_with_detailed_json(db_session, news_id, news_data)
                else:
                    # –ï—Å–ª–∏ —ç—Ç–æ –æ–¥–Ω–∞ –Ω–æ–≤–æ—Å—Ç—å
                    news_id = detailed_response.get('news_id')
                    if news_id:
                        await self._update_news_with_detailed_json(db_session, news_id, detailed_response)
                
                await db_session.commit()
                logger.info(f"‚úÖ Saved detailed JSON to PostgreSQL")
                
        except Exception as e:
            logger.error(f"‚ùå Failed to save detailed JSON to database: {e}")

    async def _update_news_with_detailed_json(self, db_session, news_id: str, detailed_data: Dict[str, Any]) -> None:
        """–û–±–Ω–æ–≤–ª—è–µ—Ç –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –Ω–æ–≤–æ—Å—Ç—å —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º JSON"""
        try:
            from sqlalchemy import update
            
            # –û–±–Ω–æ–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –Ω–æ–≤–æ—Å—Ç–∏, –¥–æ–±–∞–≤–ª—è—è –¥–µ—Ç–∞–ª—å–Ω—ã–π JSON
            update_stmt = update(News).where(
                News.id == news_id
            ).values(
                detailed_json=detailed_data
            )
            
            result = await db_session.execute(update_stmt)
            if result.rowcount > 0:
                logger.debug(f"Updated news {news_id} with detailed JSON")
            else:
                logger.warning(f"News {news_id} not found for detailed JSON update")
                
        except Exception as e:
            logger.error(f"Failed to update news {news_id} with detailed JSON: {e}")

    def _update_batch_stats(self, results: Dict[str, Any]):
        """üìä –û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É batch –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        self.total_stats["batch_processed"] += 1
        self.total_stats["entities_extracted"] += results.get("total_entities", 0)
        self.total_stats["graph_nodes_created"] += results.get("total_nodes", 0)
        self.total_stats["ceg_links_created"] += results.get("total_links", 0)

    def _print_final_stats(self):
        """üìä –í—ã–≤–æ–¥–∏—Ç —É–ª—É—á—à–µ–Ω–Ω—É—é —Ñ–∏–Ω–∞–ª—å–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É"""
        logger.info("\n" + "="*80)
        logger.info("üìä –§–ò–ù–ê–õ–¨–ù–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê BATCH –û–ë–†–ê–ë–û–¢–ö–ò")
        logger.info("="*80)
        logger.info(f"üì® –í—Å–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_stats['total_messages']}")
        logger.info(f"üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {self.total_stats['saved_news']}")
        logger.info(f"üö´ –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ä–µ–∫–ª–∞–º—ã: {self.total_stats['ads_filtered']}")
        logger.info(f"üîÑ –ù–∞–π–¥–µ–Ω–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤: {self.total_stats['duplicates']}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {self.total_stats['errors']}")
        logger.info("-"*80)
        logger.info(" –ò–°–û–ß–ù–ò–ö–ò:")
        logger.info(f"üì° Telegram –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_stats['telegram_sources_processed']}")
        logger.info(f"üåê HTML –∏—Å—Ç–æ—á–Ω–∏–∫–æ–≤ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_stats['html_sources_processed']}")
        logger.info(f"üì∞ HTML —Å—Ç–∞—Ç–µ–π –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_stats['html_articles_processed']}")
        logger.info("-"*80)
        logger.info("üì¶ BATCH –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        logger.info(f"üì¶ Batch –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {self.total_stats['batch_processed']}")
        logger.info(f"üß† –°—É—â–Ω–æ—Å—Ç–µ–π –∏–∑–≤–ª–µ—á–µ–Ω–æ: {self.total_stats['entities_extracted']}")
        logger.info(f"üï∏Ô∏è –£–∑–ª–æ–≤ –≤ –≥—Ä–∞—Ñ–µ —Å–æ–∑–¥–∞–Ω–æ: {self.total_stats['graph_nodes_created']}")
        logger.info(f"üîó –°–≤—è–∑–µ–π –≤ –≥—Ä–∞—Ñ–µ —Å–æ–∑–¥–∞–Ω–æ: {self.total_stats['ceg_links_created']}")
        logger.info(f"üîÆ –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ: {self.total_stats['predictions_generated']}")
        logger.info("-"*80)
        logger.info("‚öôÔ∏è –ù–ê–°–¢–†–û–ô–ö–ò:")
        logger.info(f"üìè –†–∞–∑–º–µ—Ä batch: {self.batch_size}")
        logger.info(f"üîÑ –ú–∞–∫—Å. batch: {self.max_batches}")
        logger.info(f"‚è±Ô∏è –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É batch: {self.batch_delay}s")
        logger.info(f"üîÅ –ü–æ–ø—ã—Ç–∫–∏ retry: {self.retry_attempts}")
        logger.info(f"üß† AI –º–æ–¥–µ–ª—å: {'Local Qwen3-4B' if self.use_local_ai else 'OpenAI GPT'}")
        logger.info("="*80)

    async def stop(self):
        """–û—Å—Ç–∞–Ω–æ–≤–∫–∞ –ø–∞—Ä—Å–µ—Ä–∞"""
        logger.info("Stopping Telegram parser...")
        self.running = False

        # –û—Ç–º–µ–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏
        for task in self.tasks:
            task.cancel()

        # –û—Ç–∫–ª—é—á–∞–µ–º –∫–ª–∏–µ–Ω—Ç
        if self.client_manager:
            await self.client_manager.disconnect()

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º Enricher (–≤–∫–ª—é—á–∞—è –≤—Å–µ GraphService –≤–Ω—É—Ç—Ä–∏)
        if self.enricher:
            await self.enricher.close()

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º MOEX Linker
        if self.moex_linker:
            await self.moex_linker.close()

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º Graph Service
        if self.graph_service:
            await self.graph_service.close()

        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ë–î
        await close_db()

        logger.info("Telegram parser stopped")


def signal_handler(signum, _frame):
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–∏–≥–Ω–∞–ª–æ–≤ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏"""
    logger.info(f"Received signal {signum}")
    asyncio.create_task(parser_service.stop())
    sys.exit(0)


async def main():
    """üöÄ –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
    global parser_service

    setup_logging()

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è –∏–∑ .env —Ñ–∞–π–ª–∞
    from dotenv import load_dotenv
    # –ò—â–µ–º .env –≤ –∫–æ—Ä–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ (–¥–≤–∞ —É—Ä–æ–≤–Ω—è –≤–≤–µ—Ä—Ö –æ—Ç scripts/)
    env_path = Path(__file__).parent.parent.parent / ".env"
    load_dotenv(dotenv_path=env_path)

    # –í—ã–≤–æ–¥–∏–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
    api_key_check = os.getenv("API_KEY_2") or os.getenv("OPENAI_API_KEY")
    if api_key_check:
        logger.info(f"‚úì API –∫–ª—é—á –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {env_path}: {api_key_check[:15]}...")

    try:
        print("\n" + "="*70)
        print("üöÄ –£–õ–£–ß–®–ï–ù–ù–´–ô TELEGRAM PARSER —Å BATCH –æ–±—Ä–∞–±–æ—Ç–∫–æ–π")
        print("üåê + –ò–ù–¢–ï–ì–†–ê–¶–ò–Ø HTML –ü–ê–†–°–ï–†–û–í (Forbes, Interfax)")
        print("="*70)

        # –í—ã–±–æ—Ä AI –º–æ–¥–µ–ª–∏
        print("\nüß† –í—ã–±–µ—Ä–∏—Ç–µ AI –º–æ–¥–µ–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:")
        print("1. OpenAI GPT (API) - —Ç–æ—á–Ω–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç API –∫–ª—é—á")
        print("2. Qwen3-4B (–ª–æ–∫–∞–ª—å–Ω–æ) - –±—ã—Å—Ç—Ä–µ–µ, —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ñ–ª–∞–π–Ω")

        ai_input = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä (1 –∏–ª–∏ 2, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip()
        use_local_ai = (ai_input == '2')

        if not use_local_ai:
            api_key = os.getenv("API_KEY_2") or os.getenv("OPENAI_API_KEY")
            if not api_key:
                print("‚ö†Ô∏è  API –∫–ª—é—á –Ω–µ –Ω–∞–π–¥–µ–Ω, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π Qwen3-4B")
                use_local_ai = True
            else:
                print(f"‚úÖ OpenAI API –∫–ª—é—á –Ω–∞–π–¥–µ–Ω: {api_key[:10]}...")
        else:
            print("‚úÖ –ë—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –ª–æ–∫–∞–ª—å–Ω—ã–π Qwen3-4B")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏
        print("\nüì¶ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ batch –æ–±—Ä–∞–±–æ—Ç–∫–∏:")
        batch_size_input = input("–†–∞–∑–º–µ—Ä batch (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 20): ").strip()
        batch_size = int(batch_size_input) if batch_size_input else 20

        max_batches_input = input("–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ batch (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 10): ").strip()
        max_batches = int(max_batches_input) if max_batches_input else 10

        batch_delay_input = input("–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É batch –≤ —Å–µ–∫—É–Ω–¥–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 5): ").strip()
        batch_delay = int(batch_delay_input) if batch_delay_input else 5

        retry_attempts_input = input("–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö —Å–µ—Ç–∏ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 3): ").strip()
        retry_attempts = int(retry_attempts_input) if retry_attempts_input else 3

        # –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ —Ä–∞–±–æ—Ç—ã
        print("\nüìã –í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º —Ä–∞–±–æ—Ç—ã:")
        print("1. üìö –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ - –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏ –∑–∞ –ø–µ—Ä–∏–æ–¥")
        print("2. üîÑ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ - —Å–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏")

        mode_input = input("\n–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä —Ä–µ–∂–∏–º–∞ (1 –∏–ª–∏ 2, –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 1): ").strip()
        mode = mode_input if mode_input in ['1', '2'] else '1'

        if mode == '1':
            # –†–µ–∂–∏–º –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–æ–π –∑–∞–≥—Ä—É–∑–∫–∏
            print("\nüìö –†–ï–ñ–ò–ú: –ò—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ —Å batch –∞–Ω–∞–ª–∏–∑–æ–º")
            print("-" * 60)

            days_input = input("–ó–∞ —Å–∫–æ–ª—å–∫–æ –¥–Ω–µ–π –∑–∞–≥—Ä—É–∂–∞—Ç—å –Ω–æ–≤–æ—Å—Ç–∏? (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 7): ").strip()
            days = int(days_input) if days_input else 7

            if days <= 0:
                print("‚ùå –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –¥–Ω–µ–π –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å –±–æ–ª—å—à–µ 0")
                return

            print(f"\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è:")
            print(f"   - –ü–µ—Ä–∏–æ–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ {days} –¥–Ω–µ–π")
            print(f"   - –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Telegram + HTML (Forbes, Interfax, E-disclosure, MOEX)")
            print(f"   - AI –º–æ–¥–µ–ª—å: {'Local Qwen3-4B' if use_local_ai else 'OpenAI GPT'}")
            print(f"   - Batch —Ä–∞–∑–º–µ—Ä: {batch_size}")
            print(f"   - –ú–∞–∫—Å. batch: {max_batches}")
            print(f"   - –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É batch: {batch_delay}s")
            print(f"   - –ü–æ–ø—ã—Ç–∫–∏ retry: {retry_attempts}")
            print(f"   - CEG –∞–Ω–∞–ª–∏–∑: –≤–∫–ª—é—á–µ–Ω")
            print(f"   - –ì—Ä–∞—Ñ–æ–≤–∞—è –ë–î: Neo4j")
            print(f"   - JSON –æ—Ç–≤–µ—Ç—ã: –¥–µ—Ç–∞–ª—å–Ω—ã–µ")
            realtime_mode = False

        else:
            # –†–µ–∂–∏–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
            print("\nüîÑ –†–ï–ñ–ò–ú: –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ —Ä–µ–∞–ª—å–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏ —Å batch –æ–±—Ä–∞–±–æ—Ç–∫–æ–π")
            print("-" * 60)
            print("‚úÖ –ü–∞—Ä—Å–µ—Ä –±—É–¥–µ—Ç —Å–ª–µ–¥–∏—Ç—å –∑–∞ –Ω–æ–≤—ã–º–∏ –Ω–æ–≤–æ—Å—Ç—è–º–∏ –∏ –∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∏—Ö –≤ batch")
            print(f"   - –ò—Å—Ç–æ—á–Ω–∏–∫–∏: Telegram + HTML (Forbes, Interfax, E-disclosure, MOEX)")
            print(f"   - AI –º–æ–¥–µ–ª—å: {'Local Qwen3-4B' if use_local_ai else 'OpenAI GPT'}")
            print(f"   - Batch —Ä–∞–∑–º–µ—Ä: {batch_size}")
            print(f"   - –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É batch: {batch_delay}s")
            print(f"   - –ü–æ–ø—ã—Ç–∫–∏ retry: {retry_attempts}")
            print(f"   - CEG –≥—Ä–∞—Ñ: –æ–±–Ω–æ–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏")
            print(f"   - –ì—Ä–∞—Ñ–æ–≤–∞—è –ë–î: Neo4j")
            print(f"   - JSON –æ—Ç–≤–µ—Ç—ã: –¥–µ—Ç–∞–ª—å–Ω—ã–µ")
            days = 0  # –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–µ–∂–∏–º–µ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
            realtime_mode = True

        print("\n‚è≥ –ó–∞–ø—É—Å–∫ —É–ª—É—á—à–µ–Ω–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞...")

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ —Å–∏–≥–Ω–∞–ª–æ–≤
        signal.signal(signal.SIGINT, signal_handler)
        signal.signal(signal.SIGTERM, signal_handler)

        # –°–æ–∑–¥–∞–Ω–∏–µ –∏ –∑–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ —Å –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏
        parser_service = Telegram_ParserServiceCEG(
            days=days,
            realtime_mode=realtime_mode,
            use_local_ai=use_local_ai,
            batch_size=batch_size,
            max_batches=max_batches,
            batch_delay=batch_delay,
            retry_attempts=retry_attempts,
            network_timeout=30
        )

        try:
            await parser_service.start()
        except KeyboardInterrupt:
            logger.info("Received keyboard interrupt")
        except Exception as e:
            logger.error(f"Fatal error: {e}", exc_info=True)
        finally:
            await parser_service.stop()

    except ValueError as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –≤–≤–æ–¥–∞: {e}")
        return
    except KeyboardInterrupt:
        print("\nüëã –ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        return


if __name__ == "__main__":
    parser_service = None
    asyncio.run(main())
