import os
import time
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
import json


# ===== –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• =====
class Person(BaseModel):
    name: str = Field(description="–§–ò–û –∏–ª–∏ –∏–º—è –ø–µ—Ä—Å–æ–Ω—ã")
    position: Optional[str] = Field(None, description="–î–æ–ª–∂–Ω–æ—Å—Ç—å")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")


class Company(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
    ticker: Optional[str] = Field(None, description="–¢–∏–∫–µ—Ä –∞–∫—Ü–∏–π")
    sector: Optional[str] = Field(None, description="–û—Ç—Ä–∞—Å–ª—å")


class Market(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞/–±–∏—Ä–∂–∏/–∏–Ω–¥–µ–∫—Å–∞")
    type: str = Field(description="–¢–∏–ø: '–±–∏—Ä–∂–∞', '–∏–Ω–¥–µ–∫—Å', '–≤–∞–ª—é—Ç–∞', '—Ç–æ–≤–∞—Ä'")
    value: Optional[float] = Field(None, description="–ó–Ω–∞—á–µ–Ω–∏–µ/–∫–æ—Ç–∏—Ä–æ–≤–∫–∞")
    change: Optional[str] = Field(None, description="–ò–∑–º–µ–Ω–µ–Ω–∏–µ")


class FinancialMetric(BaseModel):
    metric_type: str = Field(description="–¢–∏–ø –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è")
    value: str = Field(description="–ó–Ω–∞—á–µ–Ω–∏–µ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")


class ExtractedEntities(BaseModel):
    publication_date: Optional[str] = Field(None, description="–î–∞—Ç–∞ YYYY-MM-DD")
    people: List[Person] = Field(default_factory=list)
    companies: List[Company] = Field(default_factory=list)
    markets: List[Market] = Field(default_factory=list)
    financial_metrics: List[FinancialMetric] = Field(default_factory=list)


# ===== –ì–õ–û–°–°–ê–†–ò–ô =====
class RussianFinanceGlossary:
    def __init__(self):
        self.companies = {
            "–°–ë–ï–†–ë–ê–ù–ö": {"full_name": "–ü–ê–û –°–±–µ—Ä–±–∞–Ω–∫", "ticker": "SBER"},
            "–ì–ê–ó–ü–†–û–ú": {"full_name": "–ü–ê–û –ì–∞–∑–ø—Ä–æ–º", "ticker": "GAZP"},
            "–õ–£–ö–û–ô–õ": {"full_name": "–ü–ê–û –õ–£–ö–û–ô–õ", "ticker": "LKOH"},
            "–†–û–°–ù–ï–§–¢–¨": {"full_name": "–ü–ê–û –†–æ—Å–Ω–µ—Ñ—Ç—å", "ticker": "ROSN"},
            "–Ø–ù–î–ï–ö–°": {"full_name": "–Ø–Ω–¥–µ–∫—Å", "ticker": "YNDX"},
        }

        self.markets = {
            "–ú–ú–í–ë": {"full_name": "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞"},
            "MOEX": {"full_name": "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞"},
            "–†–¢–°": {"full_name": "–ò–Ω–¥–µ–∫—Å –†–¢–°"},
        }

        self.terms = {
            "–¶–ë –†–§": "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –†–æ—Å—Å–∏–π—Å–∫–æ–π –§–µ–¥–µ—Ä–∞—Ü–∏–∏",
            "–ö–ª—é—á–µ–≤–∞—è —Å—Ç–∞–≤–∫–∞": "–ë–∞–∑–æ–≤–∞—è –ø—Ä–æ—Ü–µ–Ω—Ç–Ω–∞—è —Å—Ç–∞–≤–∫–∞ –¶–ë –†–§",
            "–º–ª—Ä–¥": "–º–∏–ª–ª–∏–∞—Ä–¥–æ–≤",
            "—Ç—Ä–ª–Ω": "—Ç—Ä–∏–ª–ª–∏–æ–Ω–æ–≤",
        }

    def get_glossary_text(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ"""
        text = "## –ì–õ–û–°–°–ê–†–ò–ô:\n\n"

        text += "## –ö–æ–º–ø–∞–Ω–∏–∏:\n"
        for abbr, info in self.companies.items():
            text += f"- {abbr}: {info['full_name']} (—Ç–∏–∫–µ—Ä: {info['ticker']})\n"

        text += "\n## –ë–∏—Ä–∂–∏:\n"
        for abbr, info in self.markets.items():
            text += f"- {abbr}: {info['full_name']}\n"

        text += "\n## –¢–µ—Ä–º–∏–Ω—ã:\n"
        for abbr, full in self.terms.items():
            text += f"- {abbr}: {full}\n"

        return text


# ===== –õ–û–ö–ê–õ–¨–ù–´–ô –≠–ö–°–¢–†–ê–ö–¢–û–† =====
class LocalFinanceNERExtractor:
    """
    –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä –Ω–∞ –±–∞–∑–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen3-4B-Instruct —Å 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–µ–π
    –ò—Å–ø–æ–ª—å–∑—É–µ—Ç batch inference –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
    """

    def __init__(
        self,
        model_name: str = "unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit",
        device: str = "cuda",
        batch_size: int = 10
    ):
        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size
        self.glossary = RussianFinanceGlossary()

        print(f"üîÑ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model_name}...")
        start = time.time()

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 4-bit –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏–∏
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.bfloat16,
            bnb_4bit_use_double_quant=True,
        )

        # –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            quantization_config=bnb_config,
            device_map="auto",
            trust_remote_code=True,
        )

        self.tokenizer = AutoTokenizer.from_pretrained(
            model_name,
            trust_remote_code=True,
        )

        # –î–ª—è Qwen –Ω—É–∂–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å pad_token
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
            self.tokenizer.pad_token_id = self.tokenizer.eos_token_id

        print(f"‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {time.time() - start:.2f} —Å–µ–∫")
        print(f"   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
        print(f"   Batch size: {batch_size}")

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_requests": 0,
            "total_time": 0.0,
            "avg_time_per_request": 0.0,
        }

    def _build_prompt(self, news_text: str) -> str:
        """–°—Ç—Ä–æ–∏—Ç –ø—Ä–æ–º–ø—Ç –¥–ª—è Qwen –º–æ–¥–µ–ª–∏"""

        system_prompt = """–ò–∑–≤–ª–µ–∫–∏ –∏–∑ —Ñ–∏–Ω–∞–Ω—Å–æ–≤–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —Å–ª–µ–¥—É—é—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏ –≤–µ—Ä–Ω–∏ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:

1. companies - —É–ø–æ–º—è–Ω—É—Ç—ã–µ –∫–æ–º–ø–∞–Ω–∏–∏
2. people - —É–ø–æ–º—è–Ω—É—Ç—ã–µ –ø–µ—Ä—Å–æ–Ω—ã
3. markets - –±–∏—Ä–∂–∏, –∏–Ω–¥–µ–∫—Å—ã
4. financial_metrics - —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã–µ –ø–æ–∫–∞–∑–∞—Ç–µ–ª–∏ (–ø—Ä–∏–±—ã–ª—å, –≤—ã—Ä—É—á–∫–∞, –¥–∏–≤–∏–¥–µ–Ω–¥—ã –∏ —Ç.–¥.)

–í–µ—Ä–Ω–∏ –¢–û–õ–¨–ö–û JSON, –Ω–∏—á–µ–≥–æ –±–æ–ª—å—à–µ."""

        glossary = self.glossary.get_glossary_text()

        # –ü—Ä–∏–º–µ—Ä –¥–ª—è few-shot –æ–±—É—á–µ–Ω–∏—è
        example_news = """–ê–∫—Ü–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 3%. –ì–ª–∞–≤–∞ –±–∞–Ω–∫–∞ –ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ —Å–æ–æ–±—â–∏–ª –æ –ø—Ä–∏–±—ã–ª–∏ 100 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π."""
        example_json = """{
  "publication_date": null,
  "people": [{"name": "–ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ", "position": "–≥–ª–∞–≤–∞ –±–∞–Ω–∫–∞", "company": "–°–±–µ—Ä–±–∞–Ω–∫"}],
  "companies": [{"name": "–°–±–µ—Ä–±–∞–Ω–∫", "ticker": "SBER", "sector": null}],
  "markets": [],
  "financial_metrics": [
    {"metric_type": "—Ä–æ—Å—Ç –∞–∫—Ü–∏–π", "value": "3%", "company": "–°–±–µ—Ä–±–∞–Ω–∫"},
    {"metric_type": "–ø—Ä–∏–±—ã–ª—å", "value": "100 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π", "company": "–°–±–µ—Ä–±–∞–Ω–∫"}
  ]
}"""

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": example_news},
            {"role": "assistant", "content": example_json},
            {"role": "user", "content": news_text}
        ]

        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –≤ —Ñ–æ—Ä–º–∞—Ç–µ Qwen
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        return prompt

    def extract_entities(self, news_text: str, verbose: bool = False) -> Optional[ExtractedEntities]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –æ–¥–Ω–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""

        prompt = self._build_prompt(news_text)

        inputs = self.tokenizer(
            prompt,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=4096
        ).to(self.device)

        start_time = time.time()

        with torch.no_grad():
            outputs = self.model.generate(
                **inputs,
                max_new_tokens=1024,
                temperature=0.1,
                do_sample=True,
                pad_token_id=self.tokenizer.pad_token_id,
            )

        elapsed = time.time() - start_time
        self.stats["total_requests"] += 1
        self.stats["total_time"] += elapsed

        # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON –∏–∑ –æ—Ç–≤–µ—Ç–∞
        try:
            # –ò—â–µ–º JSON –≤ –æ—Ç–≤–µ—Ç–µ
            if "```json" in generated_text:
                json_start = generated_text.find("```json") + 7
                json_end = generated_text.find("```", json_start)
                json_text = generated_text[json_start:json_end].strip()
            elif "{" in generated_text:
                # –ò—â–µ–º –ø–µ—Ä–≤—ã–π { –∏ –ø—Ä–∞–≤–∏–ª—å–Ω—ã–π –∑–∞–∫—Ä—ã–≤–∞—é—â–∏–π }
                json_start = generated_text.find("{")
                json_end = generated_text.find("}", json_start)

                # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–±–∫–∏
                bracket_count = 0
                for idx in range(json_start, len(generated_text)):
                    if generated_text[idx] == '{':
                        bracket_count += 1
                    elif generated_text[idx] == '}':
                        bracket_count -= 1
                        if bracket_count == 0:
                            json_end = idx + 1
                            break

                json_text = generated_text[json_start:json_end]
            else:
                raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω –≤ –æ—Ç–≤–µ—Ç–µ")

            if verbose:
                print(f"\n{'='*60}")
                print("–°–´–†–û–ô –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:")
                print(json_text)
                print(f"{'='*60}\n")

            entities = ExtractedEntities.model_validate_json(json_text)
            return entities

        except Exception as e:
            if verbose:
                print(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")
                print(f"–û—Ç–≤–µ—Ç –º–æ–¥–µ–ª–∏: {generated_text[:500]}")
            return None

    def extract_entities_batch(self, news_list: List[str], verbose: bool = False) -> List[Optional[ExtractedEntities]]:
        """Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ batch_size —à—Ç—É–∫"""

        results = []
        total_batches = (len(news_list) + self.batch_size - 1) // self.batch_size

        for batch_idx in range(total_batches):
            start_idx = batch_idx * self.batch_size
            end_idx = min(start_idx + self.batch_size, len(news_list))
            batch = news_list[start_idx:end_idx]

            print(f"üì¶ Batch {batch_idx + 1}/{total_batches}: –æ–±—Ä–∞–±–æ—Ç–∫–∞ {len(batch)} –Ω–æ–≤–æ—Å—Ç–µ–π...")

            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –ø—Ä–æ–º–ø—Ç—ã –¥–ª—è –≤—Å–µ–≥–æ batch
            prompts = [self._build_prompt(news) for news in batch]

            # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º —Å padding
            inputs = self.tokenizer(
                prompts,
                return_tensors="pt",
                padding=True,
                truncation=True,
                max_length=4096
            ).to(self.device)

            start_time = time.time()

            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –¥–ª—è –≤—Å–µ–≥–æ batch —Å—Ä–∞–∑—É
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=1024,
                    temperature=0.1,
                    do_sample=True,
                    pad_token_id=self.tokenizer.pad_token_id,
                )

            elapsed = time.time() - start_time
            print(f"   ‚úÖ Batch –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {elapsed:.2f} —Å–µ–∫ ({elapsed/len(batch):.2f} —Å–µ–∫/–Ω–æ–≤–æ—Å—Ç—å)")

            self.stats["total_requests"] += len(batch)
            self.stats["total_time"] += elapsed

            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –∏ –ø–∞—Ä—Å–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for i, output in enumerate(outputs):
                generated_text = self.tokenizer.decode(output, skip_special_tokens=True)

                try:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º JSON
                    if "```json" in generated_text:
                        json_start = generated_text.find("```json") + 7
                        json_end = generated_text.find("```", json_start)
                        json_text = generated_text[json_start:json_end].strip()
                    elif "{" in generated_text:
                        # –ò—â–µ–º –ø–µ—Ä–≤—ã–π { –∏ –ø–æ—Å–ª–µ–¥–Ω–∏–π }
                        json_start = generated_text.find("{")
                        json_end = generated_text.find("}", json_start)

                        # –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —Å–∫–æ–±–∫–∏ –¥–ª—è –Ω–∞—Ö–æ–∂–¥–µ–Ω–∏—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–≥–æ –∑–∞–∫—Ä—ã–≤–∞—é—â–µ–≥–æ }
                        bracket_count = 0
                        for idx in range(json_start, len(generated_text)):
                            if generated_text[idx] == '{':
                                bracket_count += 1
                            elif generated_text[idx] == '}':
                                bracket_count -= 1
                                if bracket_count == 0:
                                    json_end = idx + 1
                                    break

                        json_text = generated_text[json_start:json_end]
                    else:
                        raise ValueError("JSON –Ω–µ –Ω–∞–π–¥–µ–Ω")

                    if verbose:
                        print(f"\n–ù–æ–≤–æ—Å—Ç—å {start_idx + i + 1}:")
                        print(json_text[:200] + "...")

                    entities = ExtractedEntities.model_validate_json(json_text)
                    results.append(entities)

                except Exception as e:
                    if verbose:
                        print(f"–û—à–∏–±–∫–∞ –≤ –Ω–æ–≤–æ—Å—Ç–∏ {start_idx + i + 1}: {e}")
                    results.append(None)

        return results

    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        avg_time = self.stats["total_time"] / self.stats["total_requests"] if self.stats["total_requests"] > 0 else 0

        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò (–õ–û–ö–ê–õ–¨–ù–ê–Ø –ú–û–î–ï–õ–¨)")
        print("="*60)
        print(f"–ú–æ–¥–µ–ª—å: {self.model_name}")
        print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {self.stats['total_requests']}")
        print(f"–û–±—â–µ–µ –≤—Ä–µ–º—è: {self.stats['total_time']:.2f} —Å–µ–∫")
        print(f"–°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è/–∑–∞–ø—Ä–æ—Å: {avg_time:.2f} —Å–µ–∫")
        print(f"–°–∫–æ—Ä–æ—Å—Ç—å: {1/avg_time:.2f} –Ω–æ–≤–æ—Å—Ç–µ–π/—Å–µ–∫" if avg_time > 0 else "N/A")
        print("="*60)


# ===== –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø =====
def main():
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞
    extractor = LocalFinanceNERExtractor(
        model_name="unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit",
        device="cuda",
        batch_size=10  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–æ 10 –Ω–æ–≤–æ—Å—Ç–µ–π –∑–∞ —Ä–∞–∑
    )

    # –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π
    news_samples = [
        """–ê–∫—Ü–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 3.2% –ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏.
        –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 389 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π. –ì–ª–∞–≤–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞ –ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ
        –∑–∞—è–≤–∏–ª –æ –ø–ª–∞–Ω–∞—Ö —É–≤–µ–ª–∏—á–∏—Ç—å –¥–∏–≤–∏–¥–µ–Ω–¥—ã.""",

        """–ò–Ω–¥–µ–∫—Å –ú–ú–í–ë –∑–∞–∫—Ä—ã–ª—Å—è –Ω–∞ –æ—Ç–º–µ—Ç–∫–µ 3245 –ø—É–Ω–∫—Ç–æ–≤ (+1.5%).
        –ù–∞ —Ñ–æ–Ω–µ —Ä–æ—Å—Ç–∞ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å –≤—ã—Ä–æ—Å–ª–∏ –∞–∫—Ü–∏–∏ –õ—É–∫–æ–π–ª–∞ (+2.1%) –∏ –†–æ—Å–Ω–µ—Ñ—Ç–∏ (+1.8%).""",

        """–¶–ë –†–§ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –¥–æ 16%. –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –í–¢–ë –ö–∞–ø–∏—Ç–∞–ª
        –æ–∂–∏–¥–∞—é—Ç –∑–∞–º–µ–¥–ª–µ–Ω–∏—è –∏–Ω—Ñ–ª—è—Ü–∏–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ.""",

        """–Ø–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª –∫–≤–∞—Ä—Ç–∞–ª—å–Ω—É—é –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å. –í—ã—Ä—É—á–∫–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–∞
        150 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π (+18% –≥–æ–¥ –∫ –≥–æ–¥—É). –ê–∫—Ü–∏–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 5%.""",

        """–ì–∞–∑–ø—Ä–æ–º –æ–±—ä—è–≤–∏–ª –æ —Ä–µ–∫–æ—Ä–¥–Ω—ã—Ö –¥–∏–≤–∏–¥–µ–Ω–¥–∞—Ö –≤ —Ä–∞–∑–º–µ—Ä–µ 52 —Ä—É–±–ª–µ–π –Ω–∞ –∞–∫—Ü–∏—é.
        –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ–≤—ã—Å–∏–ª–∏ —Ü–µ–ª–µ–≤—É—é —Ü–µ–Ω—É –¥–æ 250 —Ä—É–±–ª–µ–π."""
    ]

    print("\nüöÄ –ù–∞—á–∏–Ω–∞–µ–º BATCH –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é...\n")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–æ–≤–æ—Å—Ç–µ–π: {len(news_samples)}")
    print(f"Batch size: {extractor.batch_size}\n")

    VERBOSE = True

    start_time = time.time()

    # BATCH –æ–±—Ä–∞–±–æ—Ç–∫–∞
    results = extractor.extract_entities_batch(news_samples, verbose=VERBOSE)

    elapsed = time.time() - start_time
    print(f"\n‚úÖ –û–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {len(news_samples)/elapsed:.2f} –Ω–æ–≤–æ—Å—Ç–µ–π/—Å–µ–∫\n")

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, (news, entities) in enumerate(zip(news_samples, results), 1):
        print(f"\n{'='*70}")
        print(f"üì∞ –ù–û–í–û–°–¢–¨ {i}/{len(news_samples)}")
        print(f"{'='*70}")
        print(f"–¢–ï–ö–°–¢:\n{news}\n")

        if entities is None:
            print(f"   ‚úó –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")
        else:
            print(f"‚úì –ò–ó–í–õ–ï–ß–ï–ù–û:")
            print(f"  ‚Ä¢ –ö–æ–º–ø–∞–Ω–∏–π: {len(entities.companies)}")
            for c in entities.companies:
                print(f"    - {c.name}" + (f" ({c.ticker})" if c.ticker else ""))
            print(f"  ‚Ä¢ –ü–µ—Ä—Å–æ–Ω: {len(entities.people)}")
            for p in entities.people:
                print(f"    - {p.name}" + (f", {p.position}" if p.position else ""))
            print(f"  ‚Ä¢ –†—ã–Ω–∫–æ–≤: {len(entities.markets)}")
            for m in entities.markets:
                print(f"    - {m.name}: {m.value} ({m.change})" if m.value else f"    - {m.name}")
            print(f"  ‚Ä¢ –ú–µ—Ç—Ä–∏–∫: {len(entities.financial_metrics)}")
            for fm in entities.financial_metrics:
                print(f"    - {fm.metric_type}: {fm.value}")

    print(f"\n{'='*70}")

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    extractor.print_stats()

    # –û—Ü–µ–Ω–∫–∞ –¥–ª—è 1000 –Ω–æ–≤–æ—Å—Ç–µ–π
    time_per_1000 = (elapsed / len(news_samples)) * 1000
    print(f"\nüí° –û–¶–ï–ù–ö–ê –î–õ–Ø 1000 –ù–û–í–û–°–¢–ï–ô:")
    print(f"   –ü—Ä–∏–º–µ—Ä–Ω–æ–µ –≤—Ä–µ–º—è: {time_per_1000/60:.1f} –º–∏–Ω—É—Ç")
    print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: $0 (–ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å)")


if __name__ == "__main__":
    main()
