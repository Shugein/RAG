import os
import asyncio
from typing import List, Optional, Dict, Any
from pydantic import BaseModel, Field
import httpx
from dotenv import load_dotenv
import time


# ===== –ú–û–î–ï–õ–ò –î–ê–ù–ù–´–• =====
class Person(BaseModel):
    name: str = Field(description="–§–ò–û –∏–ª–∏ –∏–º—è –ø–µ—Ä—Å–æ–Ω—ã")
    position: Optional[str] = Field(None, description="–î–æ–ª–∂–Ω–æ—Å—Ç—å")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")


class Company(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–º–ø–∞–Ω–∏–∏")
    ticker: Optional[str] = Field(None, description="–¢–∏–∫–µ—Ä –∞–∫—Ü–∏–π")
    sector: Optional[str] = Field(None, description="–û—Ç—Ä–∞—Å–ª—å")


class Market(BaseModel):
    name: str = Field(description="–ù–∞–∑–≤–∞–Ω–∏–µ —Ä—ã–Ω–∫–∞/–±–∏—Ä–∂–∏/–∏–Ω–¥–µ–∫—Å–∞")
    type: str = Field(description="–¢–∏–ø: '–±–∏—Ä–∂–∞', '–∏–Ω–¥–µ–∫—Å', '–≤–∞–ª—é—Ç–∞', '—Ç–æ–≤–∞—Ä'")
    value: Optional[float] = Field(None, description="–ó–Ω–∞—á–µ–Ω–∏–µ/–∫–æ—Ç–∏—Ä–æ–≤–∫–∞")
    change: Optional[str] = Field(None, description="–ò–∑–º–µ–Ω–µ–Ω–∏–µ")


class FinancialMetric(BaseModel):
    metric_type: str = Field(description="–¢–∏–ø –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è")
    value: str = Field(description="–ó–Ω–∞—á–µ–Ω–∏–µ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏")
    company: Optional[str] = Field(None, description="–ö–æ–º–ø–∞–Ω–∏—è")


class ExtractedEntities(BaseModel):
    publication_date: Optional[str] = Field(None, description="–î–∞—Ç–∞ YYYY-MM-DD")
    people: List[Person] = Field(default_factory=list)
    companies: List[Company] = Field(default_factory=list)
    markets: List[Market] = Field(default_factory=list)
    financial_metrics: List[FinancialMetric] = Field(default_factory=list)


# ===== –ì–õ–û–°–°–ê–†–ò–ô =====
class RussianFinanceGlossary:
    def __init__(self):
        self.companies = {
            "–ì–ê–ó–ü–†–û–ú": {"full_name": "–ü–ê–û –ì–∞–∑–ø—Ä–æ–º", "ticker": "GAZP", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–õ–£–ö–û–ô–õ": {"full_name": "–ü–ê–û –õ–£–ö–û–ô–õ", "ticker": "LKOH", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–°–ë–ï–†–ë–ê–ù–ö": {"full_name": "–ü–ê–û –°–±–µ—Ä–±–∞–Ω–∫", "ticker": "SBER", "sector": "–§–∏–Ω–∞–Ω—Å—ã"},
            "–°–ë–ï–†": {"full_name": "–ü–ê–û –°–±–µ—Ä–±–∞–Ω–∫", "ticker": "SBER", "sector": "–§–∏–Ω–∞–Ω—Å—ã"},
            "–í–¢–ë": {"full_name": "–ë–∞–Ω–∫ –í–¢–ë", "ticker": "VTBR", "sector": "–§–∏–Ω–∞–Ω—Å—ã"},
            "–Ø–ù–î–ï–ö–°": {"full_name": "–Ø–Ω–¥–µ–∫—Å", "ticker": "YNDX", "sector": "–¢–µ—Ö–Ω–æ–ª–æ–≥–∏–∏"},
            "–ù–û–†–ù–ò–ö–ï–õ–¨": {"full_name": "–ì–ú–ö –ù–æ—Ä–∏–ª—å—Å–∫–∏–π –Ω–∏–∫–µ–ª—å", "ticker": "GMKN", "sector": "–ú–µ—Ç–∞–ª–ª—É—Ä–≥–∏—è"},
            "–†–û–°–ù–ï–§–¢–¨": {"full_name": "–ü–ê–û –†–æ—Å–Ω–µ—Ñ—Ç—å", "ticker": "ROSN", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–ù–û–í–ê–¢–≠–ö": {"full_name": "–ü–ê–û –ù–û–í–ê–¢–≠–ö", "ticker": "NVTK", "sector": "–ù–µ—Ñ—Ç—å –∏ –≥–∞–∑"},
            "–ú–ê–ì–ù–ò–¢": {"full_name": "–ü–ê–û –ú–∞–≥–Ω–∏—Ç", "ticker": "MGNT", "sector": "–†–∏—Ç–µ–π–ª"},
            "X5": {"full_name": "X5 Retail Group", "ticker": "FIVE", "sector": "–†–∏—Ç–µ–π–ª"},
            "–û–ó–û–ù": {"full_name": "Ozon Holdings", "ticker": "OZON", "sector": "E-commerce"},
            "–ú–¢–°": {"full_name": "–ü–ê–û –ú–¢–°", "ticker": "MTSS", "sector": "–¢–µ–ª–µ–∫–æ–º"},
            "–ú–ï–ì–ê–§–û–ù": {"full_name": "–ü–ê–û –ú–µ–≥–∞–§–æ–Ω", "ticker": "MFON", "sector": "–¢–µ–ª–µ–∫–æ–º"},
        }
        
        self.indices = {
            "–ú–ú–í–ë": {"full_name": "–ò–Ω–¥–µ–∫—Å –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –±–∏—Ä–∂–∏", "type": "–∏–Ω–¥–µ–∫—Å"},
            "IMOEX": {"full_name": "–ò–Ω–¥–µ–∫—Å –ú–æ—Å–∫–æ–≤—Å–∫–æ–π –±–∏—Ä–∂–∏", "type": "–∏–Ω–¥–µ–∫—Å"},
            "–†–¢–°": {"full_name": "–ò–Ω–¥–µ–∫—Å –†–¢–°", "type": "–∏–Ω–¥–µ–∫—Å"},
            "RTSI": {"full_name": "–ò–Ω–¥–µ–∫—Å –†–¢–°", "type": "–∏–Ω–¥–µ–∫—Å"},
        }
        
        self.markets = {
            "MOEX": {"full_name": "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞", "type": "–±–∏—Ä–∂–∞"},
            "–ú–û–°–ë–ò–†–ñ–ê": {"full_name": "–ú–æ—Å–∫–æ–≤—Å–∫–∞—è –±–∏—Ä–∂–∞", "type": "–±–∏—Ä–∂–∞"},
            "–°–ü–ë": {"full_name": "–°–∞–Ω–∫—Ç-–ü–µ—Ç–µ—Ä–±—É—Ä–≥—Å–∫–∞—è –±–∏—Ä–∂–∞", "type": "–±–∏—Ä–∂–∞"},
        }
        
        self.terms = {
            "–¶–ë": "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫",
            "–¶–ë –†–§": "–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–π –±–∞–Ω–∫ –†–§",
            "–§–†–°": "–§–µ–¥–µ—Ä–∞–ª—å–Ω–∞—è —Ä–µ–∑–µ—Ä–≤–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞ –°–®–ê",
            "–í–í–ü": "–í–∞–ª–æ–≤–æ–π –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –ø—Ä–æ–¥—É–∫—Ç",
            "–ò–ü–¶": "–ò–Ω–¥–µ–∫—Å –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—å—Å–∫–∏—Ö —Ü–µ–Ω",
        }
    
    def get_glossary_text(self) -> str:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≥–ª–æ—Å—Å–∞—Ä–∏–π –≤ —Ç–µ–∫—Å—Ç–æ–≤–æ–º —Ñ–æ—Ä–º–∞—Ç–µ –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        text = "# –§–ò–ù–ê–ù–°–û–í–´–ô –ì–õ–û–°–°–ê–†–ò–ô\n\n"
        
        text += "## –ö–æ–º–ø–∞–Ω–∏–∏:\n"
        for abbr, info in self.companies.items():
            text += f"- {abbr}: {info['full_name']} (–¢–∏–∫–µ—Ä: {info['ticker']}, –°–µ–∫—Ç–æ—Ä: {info['sector']})\n"
        
        text += "\n## –ò–Ω–¥–µ–∫—Å—ã:\n"
        for abbr, info in self.indices.items():
            text += f"- {abbr}: {info['full_name']}\n"
        
        text += "\n## –ë–∏—Ä–∂–∏:\n"
        for abbr, info in self.markets.items():
            text += f"- {abbr}: {info['full_name']}\n"
        
        text += "\n## –¢–µ—Ä–º–∏–Ω—ã:\n"
        for abbr, full in self.terms.items():
            text += f"- {abbr}: {full}\n"
        
        return text


# ===== –ö–õ–ò–ï–ù–¢ –° PROMPT CACHING =====
class CachedFinanceNERExtractor:
    """
    –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π prompt caching –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –¥–æ 90% –Ω–∞ –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–∞—Ö
    """
    
    def __init__(
        self,
        api_key: str,
        model: str = "gpt-5-nano-2025-08-07",  # –ò–∑–º–µ–Ω–µ–Ω –¥–µ—Ñ–æ–ª—Ç –Ω–∞ GPT-4o-mini –¥–ª—è OpenAI
        enable_caching: bool = True
    ):
        self.api_key = api_key
        self.model = model
        self.enable_caching = enable_caching
        self.base_url = "https://api.openai.com/v1"
        self.glossary = RussianFinanceGlossary()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —ç–∫–æ–Ω–æ–º–∏–∏
        self.stats = {
            "total_requests": 0,
            "cache_hits": 0,
            "input_tokens": 0,
            "cached_tokens": 0,
            "output_tokens": 0,
            "total_cost": 0.0
        }
    
    def extract_entities(self, news_text: str, news_date: Optional[str] = None, verbose: bool = False) -> ExtractedEntities:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—É—â–Ω–æ—Å—Ç–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º prompt caching

        GPT-5 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã >1024 —Ç–æ–∫–µ–Ω–æ–≤ (—Å–∞–º—ã–π –¥–ª–∏–Ω–Ω—ã–π –ø—Ä–µ—Ñ–∏–∫—Å)
        –ö—ç—à –æ—á–∏—â–∞–µ—Ç—Å—è —á–µ—Ä–µ–∑ 5-10 –º–∏–Ω—É—Ç –Ω–µ–∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        """

        messages = self._build_cached_messages(news_text, news_date)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": messages,
        }

        # GPT-5 –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∫–∞—Å—Ç–æ–º–Ω—ã–π temperature (—Ç–æ–ª—å–∫–æ –¥–µ—Ñ–æ–ª—Ç 1.0)
        if "gpt-4" in self.model:
            payload["temperature"] = 0.1

        # –î–ª—è GPT-5 –∏—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–æ—Å—Ç–æ–π JSON mode (–Ω–µ schema-based)
        # —Ç.–∫. GPT-5 —Ç—Ä–µ–±—É–µ—Ç additionalProperties: false –≤–æ –≤—Å–µ—Ö –æ–±—ä–µ–∫—Ç–∞—Ö —Å—Ö–µ–º—ã
        if "gpt-5" in self.model:
            payload["response_format"] = {"type": "json_object"}
        elif "gpt-4" in self.model:
            # GPT-4 –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç structured outputs
            payload["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "financial_entities",
                    "strict": True,
                    "schema": ExtractedEntities.model_json_schema()
                }
            }

        try:
            response = httpx.post(
                f"{self.base_url}/chat/completions",
                headers=headers,
                json=payload,
                timeout=60.0
            )
            response.raise_for_status()
        except httpx.HTTPStatusError as e:
            # –í—ã–≤–æ–¥–∏–º –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏ –¥–ª—è –æ—Ç–ª–∞–¥–∫–∏
            if verbose:
                print(f"   –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {e.response.text}")
            raise

        result = response.json()

        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
        self._update_stats(result)

        content = result["choices"][0]["message"]["content"]

        # –£–±–∏—Ä–∞–µ–º markdown –æ–±–µ—Ä—Ç–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å (```json ... ```)
        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
            content = content.strip()

        if verbose:
            print(f"\n{'='*60}")
            print("–°–´–†–û–ô –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:")
            print(content)
            print(f"{'='*60}\n")

        entities = ExtractedEntities.model_validate_json(content)

        return entities

    async def extract_entities_async(self, news_text: str, news_date: Optional[str] = None, verbose: bool = False) -> ExtractedEntities:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è extract_entities –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"""

        messages = self._build_cached_messages(news_text, news_date)

        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        payload = {
            "model": self.model,
            "messages": messages,
        }

        if "gpt-4" in self.model:
            payload["temperature"] = 0.1

        if "gpt-5" in self.model:
            payload["response_format"] = {"type": "json_object"}
        elif "gpt-4" in self.model:
            payload["response_format"] = {
                "type": "json_schema",
                "json_schema": {
                    "name": "financial_entities",
                    "strict": True,
                    "schema": ExtractedEntities.model_json_schema()
                }
            }

        async with httpx.AsyncClient(timeout=60.0) as client:
            try:
                response = await client.post(
                    f"{self.base_url}/chat/completions",
                    headers=headers,
                    json=payload
                )
                response.raise_for_status()
            except httpx.HTTPStatusError as e:
                if verbose:
                    print(f"   –î–µ—Ç–∞–ª–∏ –æ—à–∏–±–∫–∏: {e.response.text}")
                raise

        result = response.json()
        self._update_stats(result)

        content = result["choices"][0]["message"]["content"]

        if content.startswith("```"):
            content = content.split("```")[1]
            if content.startswith("json"):
                content = content[4:]
            content = content.strip()

        if verbose:
            print(f"\n{'='*60}")
            print("–°–´–†–û–ô –û–¢–í–ï–¢ –ú–û–î–ï–õ–ò:")
            print(content)
            print(f"{'='*60}\n")

        entities = ExtractedEntities.model_validate_json(content)
        return entities

    async def extract_entities_batch_async(self, news_list: List[str], verbose: bool = False) -> List[Optional[ExtractedEntities]]:
        """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Å–ø–∏—Å–∫–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

        –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –Ω–æ–≤–æ—Å—Ç–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏
        """
        tasks = [self.extract_entities_async(news, verbose=verbose) for news in news_list]
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –æ—à–∏–±–∫–∏
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                if verbose:
                    print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ {i+1}: {result}")
                processed_results.append(None)
            else:
                processed_results.append(result)

        return processed_results

    def extract_entities_batch(self, news_list: List[str], verbose: bool = False, parallel: bool = True) -> List[Optional[ExtractedEntities]]:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π —Å –æ–ø—Ü–∏–µ–π –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏

        Args:
            news_list: –°–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–µ–π –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
            verbose: –î–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥
            parallel: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É (–±—ã—Å—Ç—Ä–µ–µ)
        """
        if parallel:
            return asyncio.run(self.extract_entities_batch_async(news_list, verbose=verbose))
        else:
            # –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            results = []
            for i, news in enumerate(news_list):
                try:
                    entity = self.extract_entities(news, verbose=verbose)
                    results.append(entity)
                except Exception as e:
                    if verbose:
                        print(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –Ω–æ–≤–æ—Å—Ç–∏ {i+1}: {e}")
                    results.append(None)
            return results
    
    def _build_cached_messages(self, news_text: str, news_date: Optional[str]) -> List[Dict[str, Any]]:
        """
        –°—Ç—Ä–æ–∏—Ç messages —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.
        
        –ö–õ–Æ–ß–ï–í–ê–Ø –ò–î–ï–Ø: –ü–æ–º–µ—á–∞–µ–º —Å—Ç–∞—Ç–∏—á–Ω—ã–µ —á–∞—Å—Ç–∏ (–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ + –≥–ª–æ—Å—Å–∞—Ä–∏–π) –¥–ª—è –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è.
        –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —á–∞—Å—Ç—å (–Ω–æ–≤–æ—Å—Ç—å) –∏–¥–µ—Ç –æ—Ç–¥–µ–ª—å–Ω–æ –∏ –Ω–µ –∫—ç—à–∏—Ä—É–µ—Ç—Å—è.
        
        –ü–æ–¥–¥–µ—Ä–∂–∫–∞ —Ä–∞–∑–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π:
        - Claude (Anthropic): explicit cache_control —Å "ephemeral"
        - GPT (OpenAI): –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à (–ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–º–µ—â–∞–µ–º –≤ –Ω–∞—á–∞–ª–µ)
        - Gemini: implicit cache (—Ä–∞–∑–º–µ—â–∞–µ–º –≤ –Ω–∞—á–∞–ª–µ, min 1024 tokens)
        - DeepSeek: –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –∫—ç—à
        """
        
        # –ß–∞—Å—Ç—å 1: –ë–∞–∑–æ–≤—ã–µ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ (–∫—ç—à–∏—Ä—É—é—Ç—Å—è)
        base_instructions = """–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É —Ñ–∏–Ω–∞–Ω—Å–æ–≤—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–¢–≤–æ—è –∑–∞–¥–∞—á–∞: –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ —Å–æ —Å–ª–µ–¥—É—é—â–∏–º–∏ –ø–æ–ª—è–º–∏:
{
  "publication_date": "YYYY-MM-DD –∏–ª–∏ null",
  "people": [{"name": "–§–ò–û", "position": "–¥–æ–ª–∂–Ω–æ—Å—Ç—å –∏–ª–∏ null", "company": "–∫–æ–º–ø–∞–Ω–∏—è –∏–ª–∏ null"}],
  "companies": [{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "ticker": "—Ç–∏–∫–µ—Ä –∏–ª–∏ null", "sector": "–æ—Ç—Ä–∞—Å–ª—å –∏–ª–∏ null"}],
  "markets": [{"name": "–Ω–∞–∑–≤–∞–Ω–∏–µ", "type": "–±–∏—Ä–∂–∞/–∏–Ω–¥–µ–∫—Å/–≤–∞–ª—é—Ç–∞/—Ç–æ–≤–∞—Ä", "value": —á–∏—Å–ª–æ –∏–ª–∏ null, "change": "–∏–∑–º–µ–Ω–µ–Ω–∏–µ –∏–ª–∏ null"}],
  "financial_metrics": [{"metric_type": "—Ç–∏–ø –ø–æ–∫–∞–∑–∞—Ç–µ–ª—è", "value": "–∑–Ω–∞—á–µ–Ω–∏–µ —Å –µ–¥–∏–Ω–∏—Ü–∞–º–∏ (—Å—Ç—Ä–æ–∫–∞)", "company": "–∫–æ–º–ø–∞–Ω–∏—è –∏–ª–∏ null"}]
}

–í–ê–ñ–ù–û:
- –ò—Å–ø–æ–ª—å–∑—É–π –≥–ª–æ—Å—Å–∞—Ä–∏–π –Ω–∏–∂–µ –¥–ª—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –∏–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏–∏ —Å–æ–∫—Ä–∞—â–µ–Ω–∏–π
- –ë—É–¥—å —Ç–æ—á–Ω—ã–º, –Ω–µ –¥–æ–±–∞–≤–ª—è–π –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∫–æ—Ç–æ—Ä–æ–π –Ω–µ—Ç –≤ —Ç–µ–∫—Å—Ç–µ
- –¢–æ—á–Ω–æ —Å–ª–µ–¥—É–π —Å—Ç—Ä—É–∫—Ç—É—Ä–µ JSON —Å—Ö–µ–º—ã –≤—ã—à–µ
- –í—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è metric_type –∏ value –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —Å—Ç—Ä–æ–∫–∞–º–∏"""
        
        # –ß–∞—Å—Ç—å 2: –ì–ª–æ—Å—Å–∞—Ä–∏–π (–∫—ç—à–∏—Ä—É–µ—Ç—Å—è)
        glossary_text = self.glossary.get_glossary_text()
        
        messages = []

        # –î–ª—è OpenAI - –ø—Ä–æ—Å—Ç–æ —Ä–∞–∑–º–µ—â–∞–µ–º –≤ –Ω–∞—á–∞–ª–µ
        # OpenAI –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø—Ä–µ—Ñ–∏–∫—Å—ã –ø—Ä–æ–º–ø—Ç–æ–≤
        messages.append({
            "role": "system",
            "content": base_instructions + "\n\n" + glossary_text
        })
        
        # –ß–∞—Å—Ç—å 3: –ù–æ–≤–æ—Å—Ç—å (–ù–ï –∫—ç—à–∏—Ä—É–µ—Ç—Å—è - –º–µ–Ω—è–µ—Ç—Å—è –∫–∞–∂–¥—ã–π —Ä–∞–∑)
        user_content = f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ñ–∏–Ω–∞–Ω—Å–æ–≤—É—é –Ω–æ–≤–æ—Å—Ç—å:\n\n{news_text}"
        if news_date:
            user_content += f"\n\n–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {news_date}"
        
        messages.append({
            "role": "user",
            "content": user_content
        })
        
        return messages
    
    def _update_stats(self, result: Dict[str, Any]):
        """–û–±–Ω–æ–≤–ª—è–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –∏ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏—è"""
        self.stats["total_requests"] += 1
        
        usage = result.get("usage", {})

        # –î–ª—è OpenAI
        prompt_tokens = usage.get("prompt_tokens", 0)
        # OpenAI –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç cached_tokens –≤ prompt_tokens_details (–¥–ª—è –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π)
        cached_tokens = usage.get("prompt_tokens_details", {}).get("cached_tokens", 0)
        completion_tokens = usage.get("completion_tokens", 0)
        
        self.stats["input_tokens"] += prompt_tokens
        self.stats["cached_tokens"] += cached_tokens
        self.stats["output_tokens"] += completion_tokens
        
        if cached_tokens > 0:
            self.stats["cache_hits"] += 1
        
        # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è OpenAI –º–æ–¥–µ–ª–µ–π
        # –¶–µ–Ω—ã –∞–∫—Ç—É–∞–ª—å–Ω—ã –Ω–∞ –∞–≤–≥—É—Å—Ç 2025
        cost = 0
        regular_input = prompt_tokens - cached_tokens

        if "gpt-5-nano" in self.model:
            cost += (regular_input / 1_000_000) * 0.050  # Input
            cost += (cached_tokens / 1_000_000) * 0.025  # Cached input (50% cheaper)
            cost += (completion_tokens / 1_000_000) * 0.400  # Output
        elif "gpt-4o-mini" in self.model:
            cost += (regular_input / 1_000_000) * 0.150  # Input
            cost += (cached_tokens / 1_000_000) * 0.075  # Cached input (50% cheaper)
            cost += (completion_tokens / 1_000_000) * 0.600  # Output
        elif "gpt-4o" in self.model:
            cost += (regular_input / 1_000_000) * 2.50  # Input
            cost += (cached_tokens / 1_000_000) * 1.25  # Cached input (50% cheaper)
            cost += (completion_tokens / 1_000_000) * 10.0  # Output
        elif "gpt-4-turbo" in self.model:
            cost += (regular_input / 1_000_000) * 10.0  # Input
            cost += (cached_tokens / 1_000_000) * 5.0  # Cached input (50% cheaper)
            cost += (completion_tokens / 1_000_000) * 30.0  # Output
        else:
            # –î–µ—Ñ–æ–ª—Ç–Ω—ã–µ —Ü–µ–Ω—ã –¥–ª—è –Ω–µ–∏–∑–≤–µ—Å—Ç–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π (GPT-5 Nano)
            cost += (regular_input / 1_000_000) * 0.050
            cost += (cached_tokens / 1_000_000) * 0.025
            cost += (completion_tokens / 1_000_000) * 0.400
        
        self.stats["total_cost"] += cost
    
    def get_stats_summary(self) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è"""
        if self.stats["total_requests"] == 0:
            return {
                **self.stats,
                "cache_hit_rate_percent": 0,
                "token_savings_percent": 0,
                "estimated_savings_usd": 0,
                "avg_cost_per_request": 0
            }

        cache_hit_rate = (self.stats["cache_hits"] / self.stats["total_requests"]) * 100
        
        # –†–∞—Å—á–µ—Ç —ç–∫–æ–Ω–æ–º–∏–∏
        total_input = self.stats["input_tokens"]
        if total_input > 0:
            savings_percent = (self.stats["cached_tokens"] / total_input) * 100
        else:
            savings_percent = 0
        
        # –ü—Ä–∏–º–µ—Ä–Ω–∞—è —ç–∫–æ–Ω–æ–º–∏—è –≤ –¥–µ–Ω—å–≥–∞—Ö (–¥–ª—è Claude 3.5 Sonnet)
        cost_without_cache = (self.stats["input_tokens"] / 1_000_000) * 3.0
        actual_cache_cost = (self.stats["cached_tokens"] / 1_000_000) * 0.30
        regular_cost = ((self.stats["input_tokens"] - self.stats["cached_tokens"]) / 1_000_000) * 3.0
        money_saved = cost_without_cache - (actual_cache_cost + regular_cost)
        
        return {
            **self.stats,
            "cache_hit_rate_percent": round(cache_hit_rate, 2),
            "token_savings_percent": round(savings_percent, 2),
            "estimated_savings_usd": round(money_saved, 4),
            "avg_cost_per_request": round(self.stats["total_cost"] / self.stats["total_requests"], 6)
        }
    
    def print_stats(self):
        """–í—ã–≤–æ–¥–∏—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        stats = self.get_stats_summary()

        print("\n" + "="*60)
        print("üìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê –û–ë–†–ê–ë–û–¢–ö–ò")
        print("="*60)
        print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {stats['total_requests']}")
        print(f"Input —Ç–æ–∫–µ–Ω–æ–≤: {stats['input_tokens']:,}")
        print(f"Output —Ç–æ–∫–µ–Ω–æ–≤: {stats['output_tokens']:,}")
        print(f"–û–±—â–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å: ${stats['total_cost']:.4f}")
        print("="*60)


# ===== –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø =====
def main():

# –ó–∞–≥—Ä—É–∂–∞–µ–º –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –æ–∫—Ä—É–∂–µ–Ω–∏—è
    load_dotenv()
    API_KEY = os.environ.get('API_KEY_2')
    print("API_KEY loaded:", bool(API_KEY))

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å –¥–µ—Ñ–æ–ª—Ç–Ω–æ–π –º–æ–¥–µ–ª—å—é (gpt-4o-mini)
    extractor = CachedFinanceNERExtractor(
        api_key=API_KEY,
        enable_caching=True
    )
    
    # –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π
    news_samples = [
        """–ê–∫—Ü–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 3.2% –ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏. 
        –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 389 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π. –ì–ª–∞–≤–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞ –ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ 
        –∑–∞—è–≤–∏–ª –æ –ø–ª–∞–Ω–∞—Ö —É–≤–µ–ª–∏—á–∏—Ç—å –¥–∏–≤–∏–¥–µ–Ω–¥—ã.""",
        
        """–ò–Ω–¥–µ–∫—Å –ú–ú–í–ë –∑–∞–∫—Ä—ã–ª—Å—è –Ω–∞ –æ—Ç–º–µ—Ç–∫–µ 3245 –ø—É–Ω–∫—Ç–æ–≤ (+1.5%). 
        –ù–∞ —Ñ–æ–Ω–µ —Ä–æ—Å—Ç–∞ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å –≤—ã—Ä–æ—Å–ª–∏ –∞–∫—Ü–∏–∏ –õ—É–∫–æ–π–ª–∞ (+2.1%) –∏ –†–æ—Å–Ω–µ—Ñ—Ç–∏ (+1.8%).""",
        
        """–¶–ë –†–§ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –¥–æ 16%. –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –í–¢–ë –ö–∞–ø–∏—Ç–∞–ª 
        –æ–∂–∏–¥–∞—é—Ç –∑–∞–º–µ–¥–ª–µ–Ω–∏—è –∏–Ω—Ñ–ª—è—Ü–∏–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ.""",
        
        """–Ø–Ω–¥–µ–∫—Å –ø—Ä–µ–¥—Å—Ç–∞–≤–∏–ª –∫–≤–∞—Ä—Ç–∞–ª—å–Ω—É—é –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç—å. –í—ã—Ä—É—á–∫–∞ —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 
        150 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π (+18% –≥–æ–¥ –∫ –≥–æ–¥—É). –ê–∫—Ü–∏–∏ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 5%.""",
        
        """–ì–∞–∑–ø—Ä–æ–º –æ–±—ä—è–≤–∏–ª –æ —Ä–µ–∫–æ—Ä–¥–Ω—ã—Ö –¥–∏–≤–∏–¥–µ–Ω–¥–∞—Ö –≤ —Ä–∞–∑–º–µ—Ä–µ 52 —Ä—É–±–ª–µ–π –Ω–∞ –∞–∫—Ü–∏—é. 
        –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –ø–æ–≤—ã—Å–∏–ª–∏ —Ü–µ–ª–µ–≤—É—é —Ü–µ–Ω—É –¥–æ 250 —Ä—É–±–ª–µ–π."""
    ]
    
    print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–£–Æ –æ–±—Ä–∞–±–æ—Ç–∫—É –Ω–æ–≤–æ—Å—Ç–µ–π —Å GPT-5...\n")
    print(f"–ú–æ–¥–µ–ª—å: {extractor.model}")
    print(f"–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: GPT-5 –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –∫—ç—à–∏—Ä—É–µ—Ç –ø—Ä–æ–º–ø—Ç—ã >1024 —Ç–æ–∫–µ–Ω–æ–≤")
    print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞: –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω–∞—è (–≤—Å–µ –∑–∞–ø—Ä–æ—Å—ã –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ)\n")

    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –Ω–æ–≤–æ—Å—Ç–∏ —Å –¥–µ—Ç–∞–ª—å–Ω—ã–º –≤—ã–≤–æ–¥–æ–º
    VERBOSE = False  # –û—Ç–∫–ª—é—á–∏—Ç—å –¥–µ—Ç–∞–ª—å–Ω—ã–π –≤—ã–≤–æ–¥ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏

    print(f"‚è±Ô∏è  –ù–∞—á–∞–ª–æ –æ–±—Ä–∞–±–æ—Ç–∫–∏ {len(news_samples)} –Ω–æ–≤–æ—Å—Ç–µ–π...")
    start_time = time.time()

    # –ü–ê–†–ê–õ–õ–ï–õ–¨–ù–ê–Ø –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö –Ω–æ–≤–æ—Å—Ç–µ–π —Å—Ä–∞–∑—É
    results = extractor.extract_entities_batch(news_samples, verbose=VERBOSE, parallel=True)

    elapsed = time.time() - start_time
    print(f"‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ –∑–∞ {elapsed:.2f} —Å–µ–∫—É–Ω–¥ ({elapsed/len(news_samples):.2f} —Å–µ–∫/–Ω–æ–≤–æ—Å—Ç—å)\n")

    # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    for i, (news, entities) in enumerate(zip(news_samples, results), 1):
        print(f"\n{'='*70}")
        print(f"üì∞ –ù–û–í–û–°–¢–¨ {i}/{len(news_samples)}")
        print(f"{'='*70}")
        print(f"–¢–ï–ö–°–¢:\n{news}\n")

        if entities is None:
            print(f"   ‚úó –û–®–ò–ë–ö–ê: –ù–µ —É–¥–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å")
        else:
            print(f"‚úì –ò–ó–í–õ–ï–ß–ï–ù–û:")
            print(f"  ‚Ä¢ –ö–æ–º–ø–∞–Ω–∏–π: {len(entities.companies)}")
            for c in entities.companies:
                print(f"    - {c.name}" + (f" ({c.ticker})" if c.ticker else ""))
            print(f"  ‚Ä¢ –ü–µ—Ä—Å–æ–Ω: {len(entities.people)}")
            for p in entities.people:
                print(f"    - {p.name}" + (f", {p.position}" if p.position else ""))
            print(f"  ‚Ä¢ –†—ã–Ω–∫–æ–≤: {len(entities.markets)}")
            for m in entities.markets:
                print(f"    - {m.name}: {m.value} ({m.change})" if m.value else f"    - {m.name}")
            print(f"  ‚Ä¢ –ú–µ—Ç—Ä–∏–∫: {len(entities.financial_metrics)}")
            for fm in entities.financial_metrics:
                print(f"    - {fm.metric_type}: {fm.value}")

    print(f"\n{'='*70}")
    
    # –í—ã–≤–æ–¥–∏–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
    extractor.print_stats()

    # –†–∞—Å—á–µ—Ç —Å—Ç–æ–∏–º–æ—Å—Ç–∏ –¥–ª—è 1000 –Ω–æ–≤–æ—Å—Ç–µ–π
    stats = extractor.get_stats_summary()
    if stats['total_requests'] > 0:
        cost_per_news = stats['total_cost'] / stats['total_requests']
        cost_1000 = cost_per_news * 1000
        print(f"\nüí∞ –°–¢–û–ò–ú–û–°–¢–¨ –û–ë–†–ê–ë–û–¢–ö–ò 1000 –ù–û–í–û–°–¢–ï–ô:")
        print(f"   –ú–æ–¥–µ–ª—å: {extractor.model}")
        print(f"   –°—Ç–æ–∏–º–æ—Å—Ç—å: ${cost_1000:.2f}")


if __name__ == "__main__":
    main()