"""
–ü–∞–π–ø–ª–∞–π–Ω –∑–∞–≥—Ä—É–∑–∫–∏, –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π –≤ –≤–µ–∫—Ç–æ—Ä–Ω—É—é –ë–î
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç –ª–æ–∫–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å –¥–ª—è –∏–∑–≤–ª–µ—á–µ–Ω–∏—è —Å—É—â–Ω–æ—Å—Ç–µ–π –∏ Weaviate –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è
"""
import time
import uuid
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
import tqdm

import weaviate
import weaviate.classes.config as wc
from weaviate.classes.config import Configure
from langchain_core.documents import Document
from langchain_text_splitters import RecursiveCharacterTextSplitter

from src.system.entity_recognition_local import LocalFinanceNERExtractor, ExtractedEntities


@dataclass
class NewsDocument:
    """–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –Ω–æ–≤–æ—Å—Ç–Ω–æ–≥–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞"""
    text: str
    title: str
    url: str
    source: str
    timestamp: int
    entities: Optional[ExtractedEntities] = None


class NewsIndexingPipeline:
    """
    –ü–∞–π–ø–ª–∞–π–Ω –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π:
    1. –ü–æ–ª—É—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö —á–µ—Ä–µ–∑ API
    2. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª—å—é
    3. –ß–∞–Ω–∫–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
    4. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ Weaviate
    """

    def __init__(
        self,
        weaviate_host: str = "localhost",
        weaviate_port: int = 8080,
        collection_name: str = "NewsChunks",
        chunk_size: int = 500,
        chunk_overlap: int = 100,
        batch_size: int = 32,
        use_entity_extraction: bool = True,
    ):
        """
        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - weaviate_host: —Ö–æ—Å—Ç Weaviate
        - weaviate_port: –ø–æ—Ä—Ç Weaviate
        - collection_name: –Ω–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
        - chunk_size: —Ä–∞–∑–º–µ—Ä —á–∞–Ω–∫–∞
        - chunk_overlap: –ø–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ —á–∞–Ω–∫–æ–≤
        - batch_size: —Ä–∞–∑–º–µ—Ä –±–∞—Ç—á–∞ –¥–ª—è –≤—Å—Ç–∞–≤–∫–∏
        - use_entity_extraction: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        """
        self.weaviate_host = weaviate_host
        self.weaviate_port = weaviate_port
        self.collection_name = collection_name
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.batch_size = batch_size
        self.use_entity_extraction = use_entity_extraction

        # –ö–ª–∏–µ–Ω—Ç Weaviate
        self.client = None
        self.collection = None

        # –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π
        self.entity_extractor = None

        # –°–ø–ª–∏—Ç—Ç–µ—Ä —Ç–µ–∫—Å—Ç–∞
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap
        )

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        self.stats = {
            "total_documents": 0,
            "total_chunks": 0,
            "total_time": 0.0,
            "entity_extraction_time": 0.0,
            "indexing_time": 0.0,
        }

    def connect_weaviate(self):
        """–ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Weaviate"""
        print(f"üîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Weaviate ({self.weaviate_host}:{self.weaviate_port})...")

        self.client = weaviate.connect_to_local(
            host=self.weaviate_host,
            port=self.weaviate_port
        )

        if not self.client.is_ready():
            raise ConnectionError("Weaviate –Ω–µ –≥–æ—Ç–æ–≤")

        print("‚úÖ –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ Weaviate")

    def initialize_collection(self, recreate: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Weaviate

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - recreate: –µ—Å–ª–∏ True, –ø–µ—Ä–µ—Å–æ–∑–¥–∞–µ—Ç –∫–æ–ª–ª–µ–∫—Ü–∏—é (—É–¥–∞–ª—è–µ—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é)
        """
        if not self.client:
            self.connect_weaviate()

        # –£–¥–∞–ª—è–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
        if recreate and self.client.collections.exists(self.collection_name):
            print(f"üóëÔ∏è  –£–¥–∞–ª–µ–Ω–∏–µ —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}...")
            self.client.collections.delete(self.collection_name)

        # –°–æ–∑–¥–∞–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é –µ—Å–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
        if not self.client.collections.exists(self.collection_name):
            print(f"üì¶ –°–æ–∑–¥–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ {self.collection_name}...")

            properties = [
                # –¢–µ–∫—Å—Ç–æ–≤—ã–µ –ø–æ–ª—è
                wc.Property(name="original_text", data_type=wc.DataType.TEXT, skip_vectorization=False),
                wc.Property(name="text_for_bm25", data_type=wc.DataType.TEXT, skip_vectorization=True),

                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —á–∞–Ω–∫–∞
                wc.Property(name="chunk_index", data_type=wc.DataType.INT),

                # –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç–∞
                wc.Property(name="parent_doc_id", data_type=wc.DataType.TEXT, skip_vectorization=True),
                wc.Property(name="parent_doc_text", data_type=wc.DataType.TEXT, skip_vectorization=True),
                wc.Property(name="title", data_type=wc.DataType.TEXT, skip_vectorization=True),
                wc.Property(name="url", data_type=wc.DataType.TEXT, skip_vectorization=True),
                wc.Property(name="source", data_type=wc.DataType.TEXT, skip_vectorization=True),
                wc.Property(name="timestamp", data_type=wc.DataType.INT, skip_vectorization=True),
            ]

            # –î–æ–±–∞–≤–ª—è–µ–º –ø–æ–ª—è –¥–ª—è —Å—É—â–Ω–æ—Å—Ç–µ–π –µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ
            if self.use_entity_extraction:
                properties.extend([
                    # –°—É—â–Ω–æ—Å—Ç–∏ (JSON —Å—Ç—Ä–æ–∫–∏)
                    wc.Property(name="entities_json", data_type=wc.DataType.TEXT, skip_vectorization=True),

                    # –û—Ç–¥–µ–ª—å–Ω—ã–µ –ø–æ–ª—è –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                    wc.Property(name="companies", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True),
                    wc.Property(name="people", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True),
                    wc.Property(name="markets", data_type=wc.DataType.TEXT_ARRAY, skip_vectorization=True),
                ])

            self.collection = self.client.collections.create(
                name=self.collection_name,
                vectorizer_config=Configure.Vectorizer.text2vec_transformers(
                    vectorize_collection_name=False
                ),
                reranker_config=Configure.Reranker.transformers(),
                properties=properties,
            )

            print(f"‚úÖ –ö–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name} —Å–æ–∑–¥–∞–Ω–∞")
        else:
            self.collection = self.client.collections.get(self.collection_name)
            print(f"‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —Å—É—â–µ—Å—Ç–≤—É—é—â–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {self.collection_name}")

    def initialize_entity_extractor(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π"""
        if not self.use_entity_extraction:
            print("‚ö†Ô∏è  –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –æ—Ç–∫–ª—é—á–µ–Ω–æ")
            return

        print("üîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π...")
        self.entity_extractor = LocalFinanceNERExtractor(
            model_name="unsloth/Qwen3-4B-Instruct-2507-unsloth-bnb-4bit",
            device="cuda",
            batch_size=10
        )
        print("‚úÖ –≠–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")

    def extract_entities_batch(self, news_list: List[NewsDocument]) -> List[NewsDocument]:
        """
        –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ –±–∞—Ç—á–∞ –Ω–æ–≤–æ—Å—Ç–µ–π

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - news_list: —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - —Å–ø–∏—Å–æ–∫ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ —Å –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–º–∏ —Å—É—â–Ω–æ—Å—Ç—è–º–∏
        """
        if not self.use_entity_extraction or not self.entity_extractor:
            return news_list

        print(f"üîç –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π –∏–∑ {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π...")
        start_time = time.time()

        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–µ–∫—Å—Ç—ã
        texts = [news.text for news in news_list]

        # Batch –æ–±—Ä–∞–±–æ—Ç–∫–∞
        entities_list = self.entity_extractor.extract_entities_batch(texts, verbose=False)

        # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –∫ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º
        for i, entities in enumerate(entities_list):
            news_list[i].entities = entities

        elapsed = time.time() - start_time
        self.stats["entity_extraction_time"] += elapsed

        success_count = sum(1 for news in news_list if news.entities is not None)
        print(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω–æ —Å—É—â–Ω–æ—Å—Ç–µ–π: {success_count}/{len(news_list)} ({elapsed:.2f}s)")

        return news_list

    def prepare_chunks(self, news: NewsDocument) -> List[Dict[str, Any]]:
        """
        –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤ –∏–∑ –Ω–æ–≤–æ—Å—Ç–∏

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - news: –Ω–æ–≤–æ—Å—Ç–Ω–æ–π –¥–æ–∫—É–º–µ–Ω—Ç

        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
        - —Å–ø–∏—Å–æ–∫ —á–∞–Ω–∫–æ–≤ —Å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–º–∏
        """
        parent_doc_id = str(uuid.uuid4())
        chunks = self.text_splitter.split_text(news.text)

        prepared_chunks = []

        for chunk_idx, chunk_text in enumerate(chunks):
            chunk_id = str(uuid.uuid4())

            # –ë–∞–∑–æ–≤—ã–µ —Å–≤–æ–π—Å—Ç–≤–∞
            properties = {
                "original_text": chunk_text,
                "text_for_bm25": chunk_text.lower(),  # –î–ª—è BM25 –ø–æ–∏—Å–∫–∞
                "chunk_index": chunk_idx,
                "parent_doc_id": parent_doc_id,
                "parent_doc_text": news.text,
                "title": news.title,
                "url": news.url,
                "source": news.source,
                "timestamp": news.timestamp,
            }

            # –î–æ–±–∞–≤–ª—è–µ–º —Å—É—â–Ω–æ—Å—Ç–∏ –µ—Å–ª–∏ –µ—Å—Ç—å
            if self.use_entity_extraction and news.entities:
                entities = news.entities

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º JSON
                properties["entities_json"] = entities.model_dump_json()

                # –ú–∞—Å—Å–∏–≤—ã –¥–ª—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏
                properties["companies"] = [c.name for c in entities.companies]
                properties["people"] = [p.name for p in entities.people]
                properties["markets"] = [m.name for m in entities.markets]

            prepared_chunks.append({
                "id": chunk_id,
                "properties": properties
            })

        return prepared_chunks

    def index_documents(self, news_list: List[NewsDocument], show_progress: bool = True):
        """
        –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Weaviate

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - news_list: —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        - show_progress: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å-–±–∞—Ä
        """
        if not self.collection:
            self.initialize_collection()

        print(f"\nüì• –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è {len(news_list)} –Ω–æ–≤–æ—Å—Ç–µ–π...")
        start_time = time.time()

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤
        all_chunks = []
        for news in tqdm.tqdm(news_list, desc="–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —á–∞–Ω–∫–æ–≤", disable=not show_progress):
            chunks = self.prepare_chunks(news)
            all_chunks.extend(chunks)

        print(f"üì¶ –ü–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–æ {len(all_chunks)} —á–∞–Ω–∫–æ–≤")

        # Batch –≤—Å—Ç–∞–≤–∫–∞
        with self.collection.batch.fixed_size(batch_size=self.batch_size) as batch:
            for chunk in tqdm.tqdm(all_chunks, desc="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è", disable=not show_progress):
                batch.add_object(
                    properties=chunk["properties"],
                    uuid=chunk["id"]
                )

                if batch.number_errors > 50:
                    print(f"‚ö†Ô∏è  –û—Å—Ç–∞–Ω–æ–≤–∫–∞ –∏–∑-–∑–∞ –±–æ–ª—å—à–æ–≥–æ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –æ—à–∏–±–æ–∫: {batch.number_errors}")
                    break

        elapsed = time.time() - start_time
        self.stats["indexing_time"] += elapsed
        self.stats["total_documents"] += len(news_list)
        self.stats["total_chunks"] += len(all_chunks)

        print(f"‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞ –∑–∞ {elapsed:.2f}s")
        print(f"   –û—à–∏–±–æ–∫: {batch.number_errors}")

    def process_and_index(
        self,
        news_list: List[NewsDocument],
        extract_entities: bool = True,
        show_progress: bool = True
    ):
        """
        –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π + –∏–Ω–¥–µ–∫—Å–∞—Ü–∏—è

        –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
        - news_list: —Å–ø–∏—Å–æ–∫ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        - extract_entities: –∏–∑–≤–ª–µ–∫–∞—Ç—å –ª–∏ —Å—É—â–Ω–æ—Å—Ç–∏
        - show_progress: –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
        """
        print(f"\n{'='*60}")
        print(f"üöÄ –ó–ê–ü–£–°–ö –ü–ê–ô–ü–õ–ê–ô–ù–ê –ò–ù–î–ï–ö–°–ê–¶–ò–ò")
        print(f"{'='*60}")
        print(f"–ù–æ–≤–æ—Å—Ç–µ–π: {len(news_list)}")
        print(f"–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {'–î–∞' if extract_entities and self.use_entity_extraction else '–ù–µ—Ç'}")
        print(f"{'='*60}\n")

        start_time = time.time()

        # –®–∞–≥ 1: –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        if extract_entities and self.use_entity_extraction:
            news_list = self.extract_entities_batch(news_list)

        # –®–∞–≥ 2: –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è
        self.index_documents(news_list, show_progress=show_progress)

        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        total_time = time.time() - start_time
        self.stats["total_time"] += total_time

        print(f"\n{'='*60}")
        print("‚úÖ –ü–ê–ô–ü–õ–ê–ô–ù –ó–ê–í–ï–†–®–ï–ù")
        print(f"{'='*60}")
        print(f"–í—Å–µ–≥–æ –≤—Ä–µ–º–µ–Ω–∏: {total_time:.2f}s")
        print(f"  - –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π: {self.stats['entity_extraction_time']:.2f}s")
        print(f"  - –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è: {self.stats['indexing_time']:.2f}s")
        print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤: {len(news_list)}")
        print(f"–°–æ–∑–¥–∞–Ω–æ —á–∞–Ω–∫–æ–≤: {self.stats['total_chunks']}")
        print(f"{'='*60}\n")

    def get_collection_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∫–æ–ª–ª–µ–∫—Ü–∏–∏"""
        if not self.collection:
            return {}

        total_count = self.collection.aggregate.over_all().total_count

        return {
            "collection_name": self.collection_name,
            "total_objects": total_count,
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
        }

    def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è"""
        if self.client:
            self.client.close()
            print("üîå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å Weaviate –∑–∞–∫—Ä—ã—Ç–æ")


# ===== –ü–†–ò–ú–ï–† –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–Ø =====
def main():
    """–ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–∞–π–ø–ª–∞–π–Ω–∞"""

    # –°–æ–∑–¥–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline = NewsIndexingPipeline(
        collection_name="NewsChunks",
        chunk_size=500,
        chunk_overlap=100,
        use_entity_extraction=True,
    )

    # –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ Weaviate
    pipeline.connect_weaviate()

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–æ–ª–ª–µ–∫—Ü–∏—é (–ø–µ—Ä–µ—Å–æ–∑–¥–∞—Ç—å = False)
    pipeline.initialize_collection(recreate=False)

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —ç–∫—Å—Ç—Ä–∞–∫—Ç–æ—Ä —Å—É—â–Ω–æ—Å—Ç–µ–π
    pipeline.initialize_entity_extractor()

    # –ü—Ä–∏–º–µ—Ä—ã –Ω–æ–≤–æ—Å—Ç–µ–π (–æ–±—ã—á–Ω–æ –∑–∞–≥—Ä—É–∂–∞—é—Ç—Å—è —á–µ—Ä–µ–∑ API)
    sample_news = [
        NewsDocument(
            text="""–ê–∫—Ü–∏–∏ –°–±–µ—Ä–±–∞–Ω–∫–∞ –≤—ã—Ä–æ—Å–ª–∏ –Ω–∞ 3.2% –ø–æ—Å–ª–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ—Ç—á–µ—Ç–Ω–æ—Å—Ç–∏.
            –ß–∏—Å—Ç–∞—è –ø—Ä–∏–±—ã–ª—å —Å–æ—Å—Ç–∞–≤–∏–ª–∞ 389 –º–ª—Ä–¥ —Ä—É–±–ª–µ–π. –ì–ª–∞–≤–∞ –°–±–µ—Ä–±–∞–Ω–∫–∞ –ì–µ—Ä–º–∞–Ω –ì—Ä–µ—Ñ
            –∑–∞—è–≤–∏–ª –æ –ø–ª–∞–Ω–∞—Ö —É–≤–µ–ª–∏—á–∏—Ç—å –¥–∏–≤–∏–¥–µ–Ω–¥—ã.""",
            title="–°–±–µ—Ä–±–∞–Ω–∫ –ø–æ–∫–∞–∑–∞–ª —Ä–æ—Å—Ç –ø—Ä–∏–±—ã–ª–∏",
            url="https://example.com/news/1",
            source="–†–ë–ö",
            timestamp=1704067200  # 2024-01-01
        ),
        NewsDocument(
            text="""–ò–Ω–¥–µ–∫—Å –ú–ú–í–ë –∑–∞–∫—Ä—ã–ª—Å—è –Ω–∞ –æ—Ç–º–µ—Ç–∫–µ 3245 –ø—É–Ω–∫—Ç–æ–≤ (+1.5%).
            –ù–∞ —Ñ–æ–Ω–µ —Ä–æ—Å—Ç–∞ —Ü–µ–Ω –Ω–∞ –Ω–µ—Ñ—Ç—å –≤—ã—Ä–æ—Å–ª–∏ –∞–∫—Ü–∏–∏ –õ—É–∫–æ–π–ª–∞ (+2.1%) –∏ –†–æ—Å–Ω–µ—Ñ—Ç–∏ (+1.8%).""",
            title="–†–æ—Å—Å–∏–π—Å–∫–∏–π —Ä—ã–Ω–æ–∫ –∞–∫—Ü–∏–π –≤—ã—Ä–æ—Å",
            url="https://example.com/news/2",
            source="–ò–Ω—Ç–µ—Ä—Ñ–∞–∫—Å",
            timestamp=1704153600
        ),
        NewsDocument(
            text="""–¶–ë –†–§ –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É –¥–æ 16%. –ê–Ω–∞–ª–∏—Ç–∏–∫–∏ –í–¢–ë –ö–∞–ø–∏—Ç–∞–ª
            –æ–∂–∏–¥–∞—é—Ç –∑–∞–º–µ–¥–ª–µ–Ω–∏—è –∏–Ω—Ñ–ª—è—Ü–∏–∏ –≤ —Å–ª–µ–¥—É—é—â–µ–º –∫–≤–∞—Ä—Ç–∞–ª–µ.""",
            title="–¶–ë –ø–æ–≤—ã—Å–∏–ª –∫–ª—é—á–µ–≤—É—é —Å—Ç–∞–≤–∫—É",
            url="https://example.com/news/3",
            source="–¢–ê–°–°",
            timestamp=1704240000
        ),
    ]

    # –ó–∞–ø—É—Å–∫–∞–µ–º –ø–∞–π–ø–ª–∞–π–Ω
    pipeline.process_and_index(sample_news)

    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏
    stats = pipeline.get_collection_stats()
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∫–æ–ª–ª–µ–∫—Ü–∏–∏:")
    for key, value in stats.items():
        print(f"  {key}: {value}")

    # –ó–∞–∫—Ä—ã–≤–∞–µ–º —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
    pipeline.close()


if __name__ == "__main__":
    main()
