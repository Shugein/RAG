# def semantic_search_with_score(db, query, limit=3):
#     """
#     –í—ã–ø–æ–ª–Ω—è–µ—Ç —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π –ø–æ–∏—Å–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã —Å –∏—Ö –æ—Ü–µ–Ω–∫–∞–º–∏.

#     :param db: –û–±—ä–µ–∫—Ç –±–∞–∑—ã –¥–∞–Ω–Ω—ã—Ö, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—é—â–∏–π similarity_search_with_score.
#     :param query: –°—Ç—Ä–æ–∫–∞ –∑–∞–ø—Ä–æ—Å–∞.
#     :param limit: –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –≤–æ–∑–≤—Ä–∞—Ç–∞.
#     :return: –ö–æ—Ä—Ç–µ–∂ –∏–∑ –¥–≤—É—Ö —Å–ø–∏—Å–∫–æ–≤: –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –∏ –∏—Ö –æ—Ü–µ–Ω–æ–∫.
#     """
#     # –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –±–æ–ª—å—à–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤, —á—Ç–æ–±—ã –∫–æ–º–ø–µ–Ω—Å–∏—Ä–æ–≤–∞—Ç—å –≤–æ–∑–º–æ–∂–Ω—ã–µ –¥—É–±–ª–∏–∫–∞—Ç—ã
#     over_fetch_limit = limit * 2
#     results = db._collection.similarity_search_with_score(query, k=limit)

#     seen = set()
#     unique_docs = []
#     unique_scores = []

#     for doc, score in results:
#         content = doc.page_content.strip()
#         if content not in seen:
#             seen.add(content)
#             unique_docs.append(content)
#             unique_scores.append(score)
#             if len(unique_docs) >= limit:
#                 break

#     return unique_docs, unique_scores


# #===== Reranker =====#
# # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
# from transformers import BertTokenizer, BertForSequenceClassification, AutoModelForSequenceClassification

# # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞ –∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–Ω–∂–∏—Ä–æ–≤–∞–Ω–∏—è
# tokenizer_reranker = BertTokenizer.from_pretrained("nlpaueb/legal-bert-base-uncased")
# model_reranker = BertForSequenceClassification.from_pretrained("nlpaueb/legal-bert-base-uncased").to(device)

# model_reranker.eval()

# def rerank_and_filter(query, documents, top_n=3, threshold=0.3):
#     # –†–µ—Ä–∞–Ω–∫–∏–Ω–≥ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
#     inputs = tokenizer_reranker([query]*len(documents), documents, return_tensors='pt', truncation=True, padding=True)
#     with torch.no_grad():
#         outputs = model_reranker(**inputs)
#     scores = torch.softmax(outputs.logits, dim=1)[:,1].tolist()

#     ranked_docs = sorted(zip(documents, scores), key=lambda x: x[1])

#     # –í—ã–±–æ—Ä –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –Ω–∏–∂–µ –ø–æ—Ä–æ–≥–∞
#     filtered_docs = [doc for doc, score in ranked_docs if score <= threshold]

#     # –ï—Å–ª–∏ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –º–µ–Ω—å—à–µ top_n, –¥–æ–ø–æ–ª–Ω—è–µ–º –∏—Ö —Ç–æ–ø-N
#     if len(filtered_docs) < top_n:
#         filtered_docs = [doc for doc, score in ranked_docs[:top_n]]

#     rearranged_docs_topn = filtered_docs[:top_n]
#     rearranged_docs = [rearranged_docs_topn[1]] + rearranged_docs_topn[2:2+(top_n-2)] + [rearranged_docs_topn[0]]
#     return rearranged_docs, rearranged_docs_topn

import weaviate.classes as wvc
from weaviate.classes.query import HybridFusion, Rerank, Filter

def hybrid_search_with_rerank(
    collection,
    query,
    limit=10,
    rerank_limit=3,
    use_parent_docs=True,
    hotness_weight=0.3,
    alpha=0.6
):
    """
    –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ —Å —Ä–µ—Ä–µ–Ω–∫–∏–Ω–≥–æ–º –∏—Å–ø–æ–ª—å–∑—É—è BAAI/bge-reranker-v2-m3

    Hotness –¥–æ–±–∞–≤–ª—è–µ—Ç—Å—è –∫ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É –∫–∞–∫ –≤–∑–≤–µ—à–µ–Ω–Ω–∞—è –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞.

    :param collection: Weaviate –∫–æ–ª–ª–µ–∫—Ü–∏—è
    :param query: –ü–æ–∏—Å–∫–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å
    :param limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –¥–ª—è –ø–µ—Ä–≤–∏—á–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞
    :param rerank_limit: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–µ—Ä–µ–Ω–∫–∏–Ω–≥–∞
    :param use_parent_docs: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø–æ–ª–Ω—ã–µ —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –≤–º–µ—Å—Ç–æ —á–∞–Ω–∫–æ–≤
    :param hotness_weight: –í–µ—Å hotness –≤ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Å–∫–æ—Ä–µ (0.0-1.0). –§–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä = rerank_score * (1 - weight) + hotness * weight
    :param alpha: –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–Ω—ã–º (alpha) –∏ BM25 (1-alpha) –ø–æ–∏—Å–∫–æ–º
                  0.0 = —Ç–æ–ª—å–∫–æ BM25 (–ª–µ–∫—Å–∏—á–µ—Å–∫–∏–π)
                  0.5 = —Ä–∞–≤–Ω—ã–π –≤–µ—Å (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é)
                  1.0 = —Ç–æ–ª—å–∫–æ –≤–µ–∫—Ç–æ—Ä–Ω—ã–π (—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–∏–π)
    :return: –°–ø–∏—Å–æ–∫ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ—Å–ª–µ —Ä–µ—Ä–µ–Ω–∫–∏–Ω–≥–∞ (—Å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–º–∏ –¥–æ–∫—É–º–µ–Ω—Ç–∞–º–∏ –µ—Å–ª–∏ use_parent_docs=True)
    """
    try:
        # –ï–î–ò–ù–´–ô –≥–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ –° —Ä–µ—Ä–µ–Ω–∫–∏–Ω–≥–æ–º
        print(f"\n{'='*80}")
        print(f"=== HYBRID SEARCH WITH RERANKING ===")
        print(f"=== Alpha (vector/BM25 balance): {alpha} | Hotness weight: {hotness_weight} ===")
        print(f"{'='*80}")

        results = collection.query.hybrid(
            query=query,
            alpha=alpha,  # –ë–∞–ª–∞–Ω—Å –º–µ–∂–¥—É –≤–µ–∫—Ç–æ—Ä–Ω—ã–º –∏ BM25
            query_properties=["original_text", "text_for_bm25"],  # original_text –¥–ª—è –≤–µ–∫—Ç–æ—Ä–Ω–æ–≥–æ, text_for_bm25 –¥–ª—è BM25
            fusion_type=HybridFusion.RELATIVE_SCORE,
            return_metadata=wvc.query.MetadataQuery(score=True),
            limit=limit,
            rerank=Rerank(
                prop="original_text",  # –†–µ—Ä–∞–Ω–∫–µ—Ä –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç
                query=query
            )
        )

        print(f"\nFound {len(results.objects)} results after hybrid search + reranking")

        # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∏ –¥–æ–±–∞–≤–ª—è–µ–º hotness –∫ —Å–∫–æ—Ä—É
        ranking = []
        for i, obj in enumerate(results.objects):
            doc_id = str(obj.uuid)
            rerank_score = obj.metadata.score
            hotness = obj.properties.get('hotness', 0.5)
            title = obj.properties.get('title', 'N/A')

            # –í—ã—á–∏—Å–ª—è–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–∫–æ—Ä —Å —É—á–µ—Ç–æ–º hotness
            final_score = rerank_score * (1 - hotness_weight) + hotness * hotness_weight

            ranking.append({
                'uuid': doc_id,
                'title': title,
                'rerank_score': rerank_score,
                'hotness': hotness,
                'final_score': final_score,
                'rerank_position': i + 1
            })

        # –ü–ï–†–ï–°–û–†–¢–ò–†–û–í–ö–ê –ø–æ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º—É —Å–∫–æ—Ä—É (rerank_score + hotness)
        ranking.sort(key=lambda x: x['final_score'], reverse=True)

        # –û–±–Ω–æ–≤–ª—è–µ–º –ø–æ–∑–∏—Ü–∏–∏ –ø–æ—Å–ª–µ –ø–µ—Ä–µ—Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∏
        for i, item in enumerate(ranking):
            item['final_position'] = i + 1

        print(f"\nAfter hotness adjustment:")
        for item in ranking:
            print(f"{item['final_position']:2d}. Rerank: {item['rerank_score']:.6f} + Hotness: {item['hotness']:.2f} = Final: {item['final_score']:.6f} | {item['title'][:50]}")

        # –§–æ—Ä–º–∏—Ä—É–µ–º —Ñ–∏–Ω–∞–ª—å–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏–µ–π —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
        print(f"\n{'='*80}")
        if use_parent_docs:
            print(f"=== Final Documents (sorted by final score with hotness) ===")
        else:
            print(f"=== Final Chunks (TOP {rerank_limit}) ===")
        print(f"{'='*80}")

        final_results = []
        seen_parent_docs = set()  # –î–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤

        # –°–æ–∑–¥–∞–µ–º –º–∞–ø—É –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ –¥–æ—Å—Ç—É–ø–∞ –∫ –æ–±—ä–µ–∫—Ç–∞–º –ø–æ UUID
        objects_map = {str(obj.uuid): obj for obj in results.objects}

        # –ò—Ç–µ—Ä–∏—Ä—É–µ–º—Å—è –ø–æ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º—É —Å–ø–∏—Å–∫—É (–ø–æ final_score)
        for rank_item in ranking:
            # –ü—Ä–µ–∫—Ä–∞—â–∞–µ–º –µ—Å–ª–∏ –Ω–∞–±—Ä–∞–ª–∏ –Ω—É–∂–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
            if len(final_results) >= rerank_limit:
                break

            doc_id = rank_item['uuid']
            obj = objects_map.get(doc_id)
            if not obj:
                continue

            parent_doc_id = obj.properties.get('parent_doc_id', doc_id)

            # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –∏ —ç—Ç–æ—Ç parent —É–∂–µ –±—ã–ª - –ø—Ä–æ–ø—É—Å–∫–∞–µ–º
            if use_parent_docs and parent_doc_id in seen_parent_docs:
                print(f"   ‚è≠Ô∏è  Skipping duplicate parent doc (chunk from same document)")
                continue

            # –û—Ç–º–µ—á–∞–µ–º parent –¥–æ–∫—É–º–µ–Ω—Ç –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π
            if use_parent_docs:
                seen_parent_docs.add(parent_doc_id)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–æ–π —Ç–µ–∫—Å—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å
            if use_parent_docs:
                text_to_use = obj.properties.get('parent_doc_text', obj.properties.get('original_text', ''))
                text_type = "parent_document"
            else:
                text_to_use = obj.properties.get('original_text', '')
                text_type = "chunk"

            result = {
                'title': obj.properties.get('title', 'N/A'),
                'source': obj.properties.get('source', 'N/A'),
                'text': text_to_use,
                'chunk_text': obj.properties.get('original_text', ''),
                'url': obj.properties.get('url', 'N/A'),
                'timestamp': obj.properties.get('timestamp', 0),
                'rerank_score': rank_item['rerank_score'],
                'hotness': rank_item['hotness'],
                'final_score': rank_item['final_score'],
                'final_position': rank_item['final_position'],
                'chunk_index': obj.properties.get('chunk_index', 0),
                'parent_doc_id': parent_doc_id,
                'text_type': text_type,
                # –î–æ–±–∞–≤–ª—è–µ–º –∏–∑–≤–ª–µ—á–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ –∏–∑ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö
                'companies': obj.properties.get('companies', []),
                'company_tickers': obj.properties.get('company_tickers', []),
                'company_sectors': obj.properties.get('company_sectors', []),
                'people': obj.properties.get('people', []),
                'people_positions': obj.properties.get('people_positions', []),
                'markets': obj.properties.get('markets', []),
                'market_types': obj.properties.get('market_types', []),
                'financial_metric_types': obj.properties.get('financial_metric_types', []),
                'financial_metric_values': obj.properties.get('financial_metric_values', []),
                'entities_json': obj.properties.get('entities_json', ''),
            }
            final_results.append(result)

            print(f"\nüìÑ Result #{len(final_results)}")
            print(f"   Title: {result['title']}")
            print(f"   Type: {'üîπ Full Parent Document' if use_parent_docs else 'üìÑ Chunk'}")
            if use_parent_docs:
                print(f"   Retrieved from chunk #{result['chunk_index']}")
            print(f"   Rerank Score:  {result['rerank_score']:.6f}")
            print(f"   Hotness:       {result['hotness']:.2f}")
            print(f"   Final Score:   {result['final_score']:.6f} = {result['rerank_score']:.4f} √ó {(1-hotness_weight):.2f} + {result['hotness']:.2f} √ó {hotness_weight:.2f}")
            print(f"   Source: {result['source']}")
            if use_parent_docs:
                print(f"   Chunk preview: {result['chunk_text'][:120]}...")
                print(f"   Full doc length: {len(result['text'])} chars")
            else:
                print(f"   Text: {result['text'][:120]}...")

        print(f"\n‚úÖ Final results: {len(final_results)} unique {'parent documents' if use_parent_docs else 'chunks'}")

        return final_results

    except Exception as e:
        print(f"Search with rerank failed: {e}")
        import traceback
        traceback.print_exc()
        return []

# –ü—Ä–∏–º–µ—Ä –ø–æ–∏—Å–∫–∞ —Å GPU –≤–µ–∫—Ç–æ—Ä–∏–∑–∞—Ü–∏–µ–π (—Å—Ç–∞—Ä–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏)
def hybrid_search_test(collection, query):
    try:
        # –ì–∏–±—Ä–∏–¥–Ω—ã–π –ø–æ–∏—Å–∫ (Weaviate –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –≤–µ–∫—Ç–æ—Ä–∏–∑–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å —Å –ø–æ–º–æ—â—å—é GPU)
        results = collection.query.hybrid(
            query=query,
            alpha=0.35,  # 0.5 = —Ä–∞–≤–Ω—ã–π –≤–µ—Å –≤–µ–∫—Ç–æ—Ä–Ω–æ–º—É –∏ BM25
            query_properties=["text_for_bm25", 'title'],
            fusion_type=HybridFusion.RELATIVE_SCORE,
            return_metadata=wvc.query.MetadataQuery(score=True),
            limit=10
        )

        print(f"Found {len(results.objects)} results for query: '{query}'")
        for i, obj in enumerate(results.objects):
            print(f"\n{i+1}. Title: {obj.properties.get('title', 'N/A')}")
            print(f"   Source: {obj.properties.get('source', 'N/A')}")
            print(f"   Text: {obj.properties.get('original_text', '')[:200]}...")
            print(f"   Score: {obj.metadata.score}")

    except Exception as e:
        print(f"Search test failed: {e}")
